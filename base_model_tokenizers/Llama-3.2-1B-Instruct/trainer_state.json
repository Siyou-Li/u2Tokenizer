{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.0,
  "eval_steps": 47538,
  "global_step": 50040,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00407673860911271,
      "grad_norm": 36.04178237915039,
      "learning_rate": 1.8463624903183898e-06,
      "loss": 3.4996,
      "step": 51
    },
    {
      "epoch": 0.00815347721822542,
      "grad_norm": 29.405452728271484,
      "learning_rate": 2.171860382138447e-06,
      "loss": 1.7385,
      "step": 102
    },
    {
      "epoch": 0.01223021582733813,
      "grad_norm": 18.069019317626953,
      "learning_rate": 2.3622644429169727e-06,
      "loss": 1.5622,
      "step": 153
    },
    {
      "epoch": 0.01630695443645084,
      "grad_norm": 25.843109130859375,
      "learning_rate": 2.4973582739585043e-06,
      "loss": 1.545,
      "step": 204
    },
    {
      "epoch": 0.02038369304556355,
      "grad_norm": 18.539148330688477,
      "learning_rate": 2.6021451901619886e-06,
      "loss": 1.4792,
      "step": 255
    },
    {
      "epoch": 0.02446043165467626,
      "grad_norm": 18.40498161315918,
      "learning_rate": 2.68776233473703e-06,
      "loss": 1.4487,
      "step": 306
    },
    {
      "epoch": 0.02853717026378897,
      "grad_norm": 17.429908752441406,
      "learning_rate": 2.7601505990388016e-06,
      "loss": 1.4294,
      "step": 357
    },
    {
      "epoch": 0.03261390887290168,
      "grad_norm": 15.588619232177734,
      "learning_rate": 2.822856165778562e-06,
      "loss": 1.3571,
      "step": 408
    },
    {
      "epoch": 0.03669064748201439,
      "grad_norm": 12.28416919708252,
      "learning_rate": 2.878166395515555e-06,
      "loss": 1.375,
      "step": 459
    },
    {
      "epoch": 0.0407673860911271,
      "grad_norm": 16.199665069580078,
      "learning_rate": 2.927643081982046e-06,
      "loss": 1.4008,
      "step": 510
    },
    {
      "epoch": 0.04484412470023981,
      "grad_norm": 24.783903121948242,
      "learning_rate": 2.9724001890804786e-06,
      "loss": 1.3229,
      "step": 561
    },
    {
      "epoch": 0.04892086330935252,
      "grad_norm": 24.677810668945312,
      "learning_rate": 3.0132602265570876e-06,
      "loss": 1.318,
      "step": 612
    },
    {
      "epoch": 0.052997601918465226,
      "grad_norm": 15.015360832214355,
      "learning_rate": 3.0508478173805226e-06,
      "loss": 1.2822,
      "step": 663
    },
    {
      "epoch": 0.05707434052757794,
      "grad_norm": 10.480027198791504,
      "learning_rate": 3.085648490858859e-06,
      "loss": 1.3237,
      "step": 714
    },
    {
      "epoch": 0.06115107913669065,
      "grad_norm": 19.66171646118164,
      "learning_rate": 3.118047142760571e-06,
      "loss": 1.2904,
      "step": 765
    },
    {
      "epoch": 0.06522781774580336,
      "grad_norm": 25.70284652709961,
      "learning_rate": 3.1483540575986193e-06,
      "loss": 1.3254,
      "step": 816
    },
    {
      "epoch": 0.06930455635491607,
      "grad_norm": 14.021944999694824,
      "learning_rate": 3.1768230280381973e-06,
      "loss": 1.2445,
      "step": 867
    },
    {
      "epoch": 0.07338129496402877,
      "grad_norm": 11.144782066345215,
      "learning_rate": 3.2036642873356123e-06,
      "loss": 1.2577,
      "step": 918
    },
    {
      "epoch": 0.07745803357314149,
      "grad_norm": 11.412226676940918,
      "learning_rate": 3.2290539405486956e-06,
      "loss": 1.3192,
      "step": 969
    },
    {
      "epoch": 0.0815347721822542,
      "grad_norm": 15.75546646118164,
      "learning_rate": 3.2531409738021035e-06,
      "loss": 1.238,
      "step": 1020
    },
    {
      "epoch": 0.0856115107913669,
      "grad_norm": 17.300748825073242,
      "learning_rate": 3.276052551637384e-06,
      "loss": 1.2138,
      "step": 1071
    },
    {
      "epoch": 0.08968824940047962,
      "grad_norm": 12.672006607055664,
      "learning_rate": 3.2978980809005362e-06,
      "loss": 1.2366,
      "step": 1122
    },
    {
      "epoch": 0.09376498800959232,
      "grad_norm": 16.868486404418945,
      "learning_rate": 3.318772370532363e-06,
      "loss": 1.2326,
      "step": 1173
    },
    {
      "epoch": 0.09784172661870504,
      "grad_norm": 11.2781343460083,
      "learning_rate": 3.338758118377145e-06,
      "loss": 1.2427,
      "step": 1224
    },
    {
      "epoch": 0.10191846522781775,
      "grad_norm": 17.341705322265625,
      "learning_rate": 3.357927890005587e-06,
      "loss": 1.263,
      "step": 1275
    },
    {
      "epoch": 0.10599520383693045,
      "grad_norm": 11.51417350769043,
      "learning_rate": 3.3763457092005802e-06,
      "loss": 1.1688,
      "step": 1326
    },
    {
      "epoch": 0.11007194244604317,
      "grad_norm": 14.743123054504395,
      "learning_rate": 3.394068348114137e-06,
      "loss": 1.194,
      "step": 1377
    },
    {
      "epoch": 0.11414868105515588,
      "grad_norm": 11.381552696228027,
      "learning_rate": 3.4111463826789165e-06,
      "loss": 1.1704,
      "step": 1428
    },
    {
      "epoch": 0.11822541966426858,
      "grad_norm": 17.55182456970215,
      "learning_rate": 3.427625062734319e-06,
      "loss": 1.1781,
      "step": 1479
    },
    {
      "epoch": 0.1223021582733813,
      "grad_norm": 16.304595947265625,
      "learning_rate": 3.443545034580628e-06,
      "loss": 1.195,
      "step": 1530
    },
    {
      "epoch": 0.126378896882494,
      "grad_norm": 26.22465705871582,
      "learning_rate": 3.4589429450120248e-06,
      "loss": 1.2063,
      "step": 1581
    },
    {
      "epoch": 0.13045563549160671,
      "grad_norm": 14.869404792785645,
      "learning_rate": 3.4738519494186765e-06,
      "loss": 1.1539,
      "step": 1632
    },
    {
      "epoch": 0.13453237410071942,
      "grad_norm": 10.387263298034668,
      "learning_rate": 3.4883021416790614e-06,
      "loss": 1.2138,
      "step": 1683
    },
    {
      "epoch": 0.13860911270983214,
      "grad_norm": 15.434029579162598,
      "learning_rate": 3.5023209198582545e-06,
      "loss": 1.2246,
      "step": 1734
    },
    {
      "epoch": 0.14268585131894485,
      "grad_norm": 16.775299072265625,
      "learning_rate": 3.5159332988824e-06,
      "loss": 1.2202,
      "step": 1785
    },
    {
      "epoch": 0.14676258992805755,
      "grad_norm": 11.721687316894531,
      "learning_rate": 3.52916217915567e-06,
      "loss": 1.1403,
      "step": 1836
    },
    {
      "epoch": 0.15083932853717028,
      "grad_norm": 15.034772872924805,
      "learning_rate": 3.5420285783655153e-06,
      "loss": 1.161,
      "step": 1887
    },
    {
      "epoch": 0.15491606714628298,
      "grad_norm": 13.635025978088379,
      "learning_rate": 3.554551832368753e-06,
      "loss": 1.074,
      "step": 1938
    },
    {
      "epoch": 0.15899280575539568,
      "grad_norm": 13.507745742797852,
      "learning_rate": 3.5667497699791054e-06,
      "loss": 1.161,
      "step": 1989
    },
    {
      "epoch": 0.1630695443645084,
      "grad_norm": 18.215391159057617,
      "learning_rate": 3.5786388656221607e-06,
      "loss": 1.1009,
      "step": 2040
    },
    {
      "epoch": 0.1671462829736211,
      "grad_norm": 9.80911636352539,
      "learning_rate": 3.5902343731378983e-06,
      "loss": 1.1513,
      "step": 2091
    },
    {
      "epoch": 0.1712230215827338,
      "grad_norm": 16.8636417388916,
      "learning_rate": 3.6015504434574417e-06,
      "loss": 1.0964,
      "step": 2142
    },
    {
      "epoch": 0.1752997601918465,
      "grad_norm": 15.463483810424805,
      "learning_rate": 3.6126002284314035e-06,
      "loss": 1.1184,
      "step": 2193
    },
    {
      "epoch": 0.17937649880095924,
      "grad_norm": 14.994996070861816,
      "learning_rate": 3.6233959727205935e-06,
      "loss": 1.133,
      "step": 2244
    },
    {
      "epoch": 0.18345323741007194,
      "grad_norm": 10.530924797058105,
      "learning_rate": 3.6339490953591538e-06,
      "loss": 1.0784,
      "step": 2295
    },
    {
      "epoch": 0.18752997601918464,
      "grad_norm": 14.322858810424805,
      "learning_rate": 3.64427026235242e-06,
      "loss": 1.0649,
      "step": 2346
    },
    {
      "epoch": 0.19160671462829737,
      "grad_norm": 13.947280883789062,
      "learning_rate": 3.6543694514666545e-06,
      "loss": 1.1768,
      "step": 2397
    },
    {
      "epoch": 0.19568345323741007,
      "grad_norm": 11.337479591369629,
      "learning_rate": 3.664256010197202e-06,
      "loss": 1.1063,
      "step": 2448
    },
    {
      "epoch": 0.19976019184652277,
      "grad_norm": 15.699516296386719,
      "learning_rate": 3.673938707759213e-06,
      "loss": 1.1104,
      "step": 2499
    },
    {
      "epoch": 0.2038369304556355,
      "grad_norm": 11.016111373901367,
      "learning_rate": 3.6834257818256445e-06,
      "loss": 1.14,
      "step": 2550
    },
    {
      "epoch": 0.2079136690647482,
      "grad_norm": 13.105130195617676,
      "learning_rate": 3.6927249806367797e-06,
      "loss": 1.0814,
      "step": 2601
    },
    {
      "epoch": 0.2119904076738609,
      "grad_norm": 17.985530853271484,
      "learning_rate": 3.701843601020638e-06,
      "loss": 1.1187,
      "step": 2652
    },
    {
      "epoch": 0.21606714628297363,
      "grad_norm": 11.441925048828125,
      "learning_rate": 3.710788522791696e-06,
      "loss": 1.1267,
      "step": 2703
    },
    {
      "epoch": 0.22014388489208633,
      "grad_norm": 8.166779518127441,
      "learning_rate": 3.719566239934195e-06,
      "loss": 1.1318,
      "step": 2754
    },
    {
      "epoch": 0.22422062350119903,
      "grad_norm": 12.45948314666748,
      "learning_rate": 3.7281828889240777e-06,
      "loss": 1.1531,
      "step": 2805
    },
    {
      "epoch": 0.22829736211031176,
      "grad_norm": 13.028997421264648,
      "learning_rate": 3.736644274498974e-06,
      "loss": 1.0992,
      "step": 2856
    },
    {
      "epoch": 0.23237410071942446,
      "grad_norm": 12.878449440002441,
      "learning_rate": 3.744955893147278e-06,
      "loss": 1.0465,
      "step": 2907
    },
    {
      "epoch": 0.23645083932853717,
      "grad_norm": 24.73775863647461,
      "learning_rate": 3.7531229545543764e-06,
      "loss": 1.0718,
      "step": 2958
    },
    {
      "epoch": 0.24052757793764987,
      "grad_norm": 10.30970573425293,
      "learning_rate": 3.761150401215583e-06,
      "loss": 1.112,
      "step": 3009
    },
    {
      "epoch": 0.2446043165467626,
      "grad_norm": 15.187329292297363,
      "learning_rate": 3.7690429264006854e-06,
      "loss": 1.0929,
      "step": 3060
    },
    {
      "epoch": 0.2486810551558753,
      "grad_norm": 9.705946922302246,
      "learning_rate": 3.7768049906336095e-06,
      "loss": 1.0713,
      "step": 3111
    },
    {
      "epoch": 0.252757793764988,
      "grad_norm": 15.705211639404297,
      "learning_rate": 3.7844408368320816e-06,
      "loss": 1.0595,
      "step": 3162
    },
    {
      "epoch": 0.2568345323741007,
      "grad_norm": 17.275066375732422,
      "learning_rate": 3.791954504235966e-06,
      "loss": 1.0688,
      "step": 3213
    },
    {
      "epoch": 0.26091127098321343,
      "grad_norm": 12.375412940979004,
      "learning_rate": 3.7993498412387337e-06,
      "loss": 1.0412,
      "step": 3264
    },
    {
      "epoch": 0.26498800959232616,
      "grad_norm": 23.137252807617188,
      "learning_rate": 3.806630517224121e-06,
      "loss": 1.0534,
      "step": 3315
    },
    {
      "epoch": 0.26906474820143883,
      "grad_norm": 9.697213172912598,
      "learning_rate": 3.813800033499119e-06,
      "loss": 1.0224,
      "step": 3366
    },
    {
      "epoch": 0.27314148681055156,
      "grad_norm": 13.453120231628418,
      "learning_rate": 3.820861733404833e-06,
      "loss": 1.0321,
      "step": 3417
    },
    {
      "epoch": 0.2772182254196643,
      "grad_norm": 12.416608810424805,
      "learning_rate": 3.827818811678312e-06,
      "loss": 1.034,
      "step": 3468
    },
    {
      "epoch": 0.28129496402877696,
      "grad_norm": 12.062485694885254,
      "learning_rate": 3.834674323130945e-06,
      "loss": 1.0243,
      "step": 3519
    },
    {
      "epoch": 0.2853717026378897,
      "grad_norm": 8.522455215454102,
      "learning_rate": 3.841431190702458e-06,
      "loss": 1.0238,
      "step": 3570
    },
    {
      "epoch": 0.2894484412470024,
      "grad_norm": 12.17522144317627,
      "learning_rate": 3.848092212943634e-06,
      "loss": 1.043,
      "step": 3621
    },
    {
      "epoch": 0.2935251798561151,
      "grad_norm": 9.673853874206543,
      "learning_rate": 3.854660070975727e-06,
      "loss": 1.0154,
      "step": 3672
    },
    {
      "epoch": 0.2976019184652278,
      "grad_norm": 17.186847686767578,
      "learning_rate": 3.861137334969852e-06,
      "loss": 1.0286,
      "step": 3723
    },
    {
      "epoch": 0.30167865707434055,
      "grad_norm": 16.015684127807617,
      "learning_rate": 3.867526470185573e-06,
      "loss": 1.0528,
      "step": 3774
    },
    {
      "epoch": 0.3057553956834532,
      "grad_norm": 12.994134902954102,
      "learning_rate": 3.87382984260417e-06,
      "loss": 1.0635,
      "step": 3825
    },
    {
      "epoch": 0.30983213429256595,
      "grad_norm": 10.954087257385254,
      "learning_rate": 3.880049724188811e-06,
      "loss": 1.0598,
      "step": 3876
    },
    {
      "epoch": 0.3139088729016787,
      "grad_norm": 14.93641471862793,
      "learning_rate": 3.886188297800891e-06,
      "loss": 1.0612,
      "step": 3927
    },
    {
      "epoch": 0.31798561151079136,
      "grad_norm": 13.162065505981445,
      "learning_rate": 3.892247661799163e-06,
      "loss": 1.056,
      "step": 3978
    },
    {
      "epoch": 0.3220623501199041,
      "grad_norm": 9.27739143371582,
      "learning_rate": 3.898229834345901e-06,
      "loss": 1.122,
      "step": 4029
    },
    {
      "epoch": 0.3261390887290168,
      "grad_norm": 13.898693084716797,
      "learning_rate": 3.904136757442218e-06,
      "loss": 0.9893,
      "step": 4080
    },
    {
      "epoch": 0.3302158273381295,
      "grad_norm": 12.355292320251465,
      "learning_rate": 3.90997030071272e-06,
      "loss": 1.0528,
      "step": 4131
    },
    {
      "epoch": 0.3342925659472422,
      "grad_norm": 10.130159378051758,
      "learning_rate": 3.915732264957956e-06,
      "loss": 0.9904,
      "step": 4182
    },
    {
      "epoch": 0.3383693045563549,
      "grad_norm": 12.157267570495605,
      "learning_rate": 3.921424385491552e-06,
      "loss": 1.0456,
      "step": 4233
    },
    {
      "epoch": 0.3424460431654676,
      "grad_norm": 14.602620124816895,
      "learning_rate": 3.927048335277499e-06,
      "loss": 1.0274,
      "step": 4284
    },
    {
      "epoch": 0.34652278177458035,
      "grad_norm": 26.445528030395508,
      "learning_rate": 3.932605727881796e-06,
      "loss": 1.0092,
      "step": 4335
    },
    {
      "epoch": 0.350599520383693,
      "grad_norm": 18.16699981689453,
      "learning_rate": 3.938098120251461e-06,
      "loss": 1.0502,
      "step": 4386
    },
    {
      "epoch": 0.35467625899280575,
      "grad_norm": 10.46544361114502,
      "learning_rate": 3.943527015332902e-06,
      "loss": 1.0089,
      "step": 4437
    },
    {
      "epoch": 0.3587529976019185,
      "grad_norm": 16.95892333984375,
      "learning_rate": 3.948893864540651e-06,
      "loss": 0.9566,
      "step": 4488
    },
    {
      "epoch": 0.36282973621103115,
      "grad_norm": 17.926116943359375,
      "learning_rate": 3.95420007008662e-06,
      "loss": 1.0066,
      "step": 4539
    },
    {
      "epoch": 0.3669064748201439,
      "grad_norm": 13.28326416015625,
      "learning_rate": 3.959446987179211e-06,
      "loss": 1.0116,
      "step": 4590
    },
    {
      "epoch": 0.3709832134292566,
      "grad_norm": 10.754889488220215,
      "learning_rate": 3.964635926100934e-06,
      "loss": 0.9706,
      "step": 4641
    },
    {
      "epoch": 0.3750599520383693,
      "grad_norm": 12.959267616271973,
      "learning_rate": 3.969768154172478e-06,
      "loss": 0.9808,
      "step": 4692
    },
    {
      "epoch": 0.379136690647482,
      "grad_norm": 12.766610145568848,
      "learning_rate": 3.974844897610608e-06,
      "loss": 0.9759,
      "step": 4743
    },
    {
      "epoch": 0.38321342925659474,
      "grad_norm": 10.310514450073242,
      "learning_rate": 3.979867343286712e-06,
      "loss": 1.015,
      "step": 4794
    },
    {
      "epoch": 0.3872901678657074,
      "grad_norm": 21.56260108947754,
      "learning_rate": 3.9848366403922934e-06,
      "loss": 1.0033,
      "step": 4845
    },
    {
      "epoch": 0.39136690647482014,
      "grad_norm": 14.289692878723145,
      "learning_rate": 3.989753902017259e-06,
      "loss": 0.9852,
      "step": 4896
    },
    {
      "epoch": 0.39544364508393287,
      "grad_norm": 14.369906425476074,
      "learning_rate": 3.994620206646423e-06,
      "loss": 0.9481,
      "step": 4947
    },
    {
      "epoch": 0.39952038369304554,
      "grad_norm": 22.43463706970215,
      "learning_rate": 3.999436599579271e-06,
      "loss": 1.0213,
      "step": 4998
    },
    {
      "epoch": 0.4035971223021583,
      "grad_norm": 8.901679992675781,
      "learning_rate": 4e-06,
      "loss": 0.9358,
      "step": 5049
    },
    {
      "epoch": 0.407673860911271,
      "grad_norm": 11.388010025024414,
      "learning_rate": 4e-06,
      "loss": 0.9905,
      "step": 5100
    },
    {
      "epoch": 0.4117505995203837,
      "grad_norm": 15.2571439743042,
      "learning_rate": 4e-06,
      "loss": 1.0144,
      "step": 5151
    },
    {
      "epoch": 0.4158273381294964,
      "grad_norm": 12.847868919372559,
      "learning_rate": 4e-06,
      "loss": 1.0377,
      "step": 5202
    },
    {
      "epoch": 0.41990407673860913,
      "grad_norm": 10.327792167663574,
      "learning_rate": 4e-06,
      "loss": 0.98,
      "step": 5253
    },
    {
      "epoch": 0.4239808153477218,
      "grad_norm": 11.807661056518555,
      "learning_rate": 4e-06,
      "loss": 0.9422,
      "step": 5304
    },
    {
      "epoch": 0.42805755395683454,
      "grad_norm": 14.101724624633789,
      "learning_rate": 4e-06,
      "loss": 0.9763,
      "step": 5355
    },
    {
      "epoch": 0.43213429256594726,
      "grad_norm": 20.702543258666992,
      "learning_rate": 4e-06,
      "loss": 0.9976,
      "step": 5406
    },
    {
      "epoch": 0.43621103117505994,
      "grad_norm": 9.350689888000488,
      "learning_rate": 4e-06,
      "loss": 0.9857,
      "step": 5457
    },
    {
      "epoch": 0.44028776978417267,
      "grad_norm": 11.459705352783203,
      "learning_rate": 4e-06,
      "loss": 1.0128,
      "step": 5508
    },
    {
      "epoch": 0.4443645083932854,
      "grad_norm": 11.059194564819336,
      "learning_rate": 4e-06,
      "loss": 0.9312,
      "step": 5559
    },
    {
      "epoch": 0.44844124700239807,
      "grad_norm": 12.664650917053223,
      "learning_rate": 4e-06,
      "loss": 0.8891,
      "step": 5610
    },
    {
      "epoch": 0.4525179856115108,
      "grad_norm": 52.87520217895508,
      "learning_rate": 4e-06,
      "loss": 0.929,
      "step": 5661
    },
    {
      "epoch": 0.4565947242206235,
      "grad_norm": 16.383487701416016,
      "learning_rate": 4e-06,
      "loss": 0.96,
      "step": 5712
    },
    {
      "epoch": 0.4606714628297362,
      "grad_norm": 10.563080787658691,
      "learning_rate": 4e-06,
      "loss": 0.9265,
      "step": 5763
    },
    {
      "epoch": 0.46474820143884893,
      "grad_norm": 11.059226036071777,
      "learning_rate": 4e-06,
      "loss": 0.9046,
      "step": 5814
    },
    {
      "epoch": 0.46882494004796166,
      "grad_norm": 13.312170028686523,
      "learning_rate": 4e-06,
      "loss": 0.9555,
      "step": 5865
    },
    {
      "epoch": 0.47290167865707433,
      "grad_norm": 19.166187286376953,
      "learning_rate": 4e-06,
      "loss": 0.9431,
      "step": 5916
    },
    {
      "epoch": 0.47697841726618706,
      "grad_norm": 16.645782470703125,
      "learning_rate": 4e-06,
      "loss": 0.9296,
      "step": 5967
    },
    {
      "epoch": 0.48105515587529973,
      "grad_norm": 10.042229652404785,
      "learning_rate": 4e-06,
      "loss": 0.9691,
      "step": 6018
    },
    {
      "epoch": 0.48513189448441246,
      "grad_norm": 8.530921936035156,
      "learning_rate": 4e-06,
      "loss": 0.8976,
      "step": 6069
    },
    {
      "epoch": 0.4892086330935252,
      "grad_norm": 10.151142120361328,
      "learning_rate": 4e-06,
      "loss": 0.9439,
      "step": 6120
    },
    {
      "epoch": 0.49328537170263786,
      "grad_norm": 17.24022674560547,
      "learning_rate": 4e-06,
      "loss": 0.9197,
      "step": 6171
    },
    {
      "epoch": 0.4973621103117506,
      "grad_norm": 10.012246131896973,
      "learning_rate": 4e-06,
      "loss": 0.9863,
      "step": 6222
    },
    {
      "epoch": 0.5014388489208633,
      "grad_norm": 12.453371047973633,
      "learning_rate": 4e-06,
      "loss": 0.9143,
      "step": 6273
    },
    {
      "epoch": 0.505515587529976,
      "grad_norm": 12.843291282653809,
      "learning_rate": 4e-06,
      "loss": 0.9694,
      "step": 6324
    },
    {
      "epoch": 0.5095923261390888,
      "grad_norm": 13.870990753173828,
      "learning_rate": 4e-06,
      "loss": 0.9109,
      "step": 6375
    },
    {
      "epoch": 0.5136690647482014,
      "grad_norm": 16.6339054107666,
      "learning_rate": 4e-06,
      "loss": 0.9573,
      "step": 6426
    },
    {
      "epoch": 0.5177458033573141,
      "grad_norm": 10.569734573364258,
      "learning_rate": 4e-06,
      "loss": 0.9613,
      "step": 6477
    },
    {
      "epoch": 0.5218225419664269,
      "grad_norm": 10.485825538635254,
      "learning_rate": 4e-06,
      "loss": 0.9965,
      "step": 6528
    },
    {
      "epoch": 0.5258992805755396,
      "grad_norm": 20.74246597290039,
      "learning_rate": 4e-06,
      "loss": 0.9429,
      "step": 6579
    },
    {
      "epoch": 0.5299760191846523,
      "grad_norm": 20.565553665161133,
      "learning_rate": 4e-06,
      "loss": 0.9058,
      "step": 6630
    },
    {
      "epoch": 0.534052757793765,
      "grad_norm": 18.432584762573242,
      "learning_rate": 4e-06,
      "loss": 0.8916,
      "step": 6681
    },
    {
      "epoch": 0.5381294964028777,
      "grad_norm": 16.26030158996582,
      "learning_rate": 4e-06,
      "loss": 0.9429,
      "step": 6732
    },
    {
      "epoch": 0.5422062350119904,
      "grad_norm": 14.551640510559082,
      "learning_rate": 4e-06,
      "loss": 0.9686,
      "step": 6783
    },
    {
      "epoch": 0.5462829736211031,
      "grad_norm": 13.672714233398438,
      "learning_rate": 4e-06,
      "loss": 0.8874,
      "step": 6834
    },
    {
      "epoch": 0.5503597122302158,
      "grad_norm": 22.804607391357422,
      "learning_rate": 4e-06,
      "loss": 0.8885,
      "step": 6885
    },
    {
      "epoch": 0.5544364508393286,
      "grad_norm": 11.495919227600098,
      "learning_rate": 4e-06,
      "loss": 0.886,
      "step": 6936
    },
    {
      "epoch": 0.5585131894484412,
      "grad_norm": 10.646437644958496,
      "learning_rate": 4e-06,
      "loss": 0.9049,
      "step": 6987
    },
    {
      "epoch": 0.5625899280575539,
      "grad_norm": 14.928312301635742,
      "learning_rate": 4e-06,
      "loss": 0.922,
      "step": 7038
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 11.35046100616455,
      "learning_rate": 4e-06,
      "loss": 0.8785,
      "step": 7089
    },
    {
      "epoch": 0.5707434052757794,
      "grad_norm": 11.380122184753418,
      "learning_rate": 4e-06,
      "loss": 0.905,
      "step": 7140
    },
    {
      "epoch": 0.5748201438848921,
      "grad_norm": 15.383838653564453,
      "learning_rate": 4e-06,
      "loss": 0.9103,
      "step": 7191
    },
    {
      "epoch": 0.5788968824940048,
      "grad_norm": 10.827102661132812,
      "learning_rate": 4e-06,
      "loss": 0.9356,
      "step": 7242
    },
    {
      "epoch": 0.5829736211031175,
      "grad_norm": 8.585453987121582,
      "learning_rate": 4e-06,
      "loss": 0.8804,
      "step": 7293
    },
    {
      "epoch": 0.5870503597122302,
      "grad_norm": 19.884126663208008,
      "learning_rate": 4e-06,
      "loss": 0.8921,
      "step": 7344
    },
    {
      "epoch": 0.5911270983213429,
      "grad_norm": 16.010591506958008,
      "learning_rate": 4e-06,
      "loss": 0.8766,
      "step": 7395
    },
    {
      "epoch": 0.5952038369304556,
      "grad_norm": 14.116286277770996,
      "learning_rate": 4e-06,
      "loss": 0.8747,
      "step": 7446
    },
    {
      "epoch": 0.5992805755395684,
      "grad_norm": 14.66557502746582,
      "learning_rate": 4e-06,
      "loss": 0.8975,
      "step": 7497
    },
    {
      "epoch": 0.6033573141486811,
      "grad_norm": 9.436531066894531,
      "learning_rate": 4e-06,
      "loss": 0.863,
      "step": 7548
    },
    {
      "epoch": 0.6074340527577937,
      "grad_norm": 17.711748123168945,
      "learning_rate": 4e-06,
      "loss": 0.9183,
      "step": 7599
    },
    {
      "epoch": 0.6115107913669064,
      "grad_norm": 16.919940948486328,
      "learning_rate": 4e-06,
      "loss": 0.8863,
      "step": 7650
    },
    {
      "epoch": 0.6155875299760192,
      "grad_norm": 10.902548789978027,
      "learning_rate": 4e-06,
      "loss": 0.904,
      "step": 7701
    },
    {
      "epoch": 0.6196642685851319,
      "grad_norm": 13.999035835266113,
      "learning_rate": 4e-06,
      "loss": 0.8548,
      "step": 7752
    },
    {
      "epoch": 0.6237410071942446,
      "grad_norm": 16.011167526245117,
      "learning_rate": 4e-06,
      "loss": 0.8881,
      "step": 7803
    },
    {
      "epoch": 0.6278177458033574,
      "grad_norm": 12.957375526428223,
      "learning_rate": 4e-06,
      "loss": 0.8589,
      "step": 7854
    },
    {
      "epoch": 0.63189448441247,
      "grad_norm": 13.447342872619629,
      "learning_rate": 4e-06,
      "loss": 0.8757,
      "step": 7905
    },
    {
      "epoch": 0.6359712230215827,
      "grad_norm": 14.961343765258789,
      "learning_rate": 4e-06,
      "loss": 0.832,
      "step": 7956
    },
    {
      "epoch": 0.6400479616306954,
      "grad_norm": 12.918347358703613,
      "learning_rate": 4e-06,
      "loss": 0.8161,
      "step": 8007
    },
    {
      "epoch": 0.6441247002398082,
      "grad_norm": 7.281136512756348,
      "learning_rate": 4e-06,
      "loss": 0.8248,
      "step": 8058
    },
    {
      "epoch": 0.6482014388489209,
      "grad_norm": 11.234862327575684,
      "learning_rate": 4e-06,
      "loss": 0.8467,
      "step": 8109
    },
    {
      "epoch": 0.6522781774580336,
      "grad_norm": 11.41833782196045,
      "learning_rate": 4e-06,
      "loss": 0.8493,
      "step": 8160
    },
    {
      "epoch": 0.6563549160671462,
      "grad_norm": 21.067028045654297,
      "learning_rate": 4e-06,
      "loss": 0.8506,
      "step": 8211
    },
    {
      "epoch": 0.660431654676259,
      "grad_norm": 12.869044303894043,
      "learning_rate": 4e-06,
      "loss": 0.8661,
      "step": 8262
    },
    {
      "epoch": 0.6645083932853717,
      "grad_norm": 12.53036880493164,
      "learning_rate": 4e-06,
      "loss": 0.8066,
      "step": 8313
    },
    {
      "epoch": 0.6685851318944844,
      "grad_norm": 13.589787483215332,
      "learning_rate": 4e-06,
      "loss": 0.8693,
      "step": 8364
    },
    {
      "epoch": 0.6726618705035972,
      "grad_norm": 7.943718433380127,
      "learning_rate": 4e-06,
      "loss": 0.9015,
      "step": 8415
    },
    {
      "epoch": 0.6767386091127098,
      "grad_norm": 10.981833457946777,
      "learning_rate": 4e-06,
      "loss": 0.8885,
      "step": 8466
    },
    {
      "epoch": 0.6808153477218225,
      "grad_norm": 12.207497596740723,
      "learning_rate": 4e-06,
      "loss": 0.8921,
      "step": 8517
    },
    {
      "epoch": 0.6848920863309352,
      "grad_norm": 16.019195556640625,
      "learning_rate": 4e-06,
      "loss": 0.8595,
      "step": 8568
    },
    {
      "epoch": 0.688968824940048,
      "grad_norm": 14.443653106689453,
      "learning_rate": 4e-06,
      "loss": 0.8015,
      "step": 8619
    },
    {
      "epoch": 0.6930455635491607,
      "grad_norm": 13.250243186950684,
      "learning_rate": 4e-06,
      "loss": 0.8704,
      "step": 8670
    },
    {
      "epoch": 0.6971223021582734,
      "grad_norm": 9.902998924255371,
      "learning_rate": 4e-06,
      "loss": 0.828,
      "step": 8721
    },
    {
      "epoch": 0.701199040767386,
      "grad_norm": 14.021961212158203,
      "learning_rate": 4e-06,
      "loss": 0.8578,
      "step": 8772
    },
    {
      "epoch": 0.7052757793764988,
      "grad_norm": 13.629913330078125,
      "learning_rate": 4e-06,
      "loss": 0.8754,
      "step": 8823
    },
    {
      "epoch": 0.7093525179856115,
      "grad_norm": 18.996755599975586,
      "learning_rate": 4e-06,
      "loss": 0.8518,
      "step": 8874
    },
    {
      "epoch": 0.7134292565947242,
      "grad_norm": 9.498461723327637,
      "learning_rate": 4e-06,
      "loss": 0.8109,
      "step": 8925
    },
    {
      "epoch": 0.717505995203837,
      "grad_norm": 11.468838691711426,
      "learning_rate": 4e-06,
      "loss": 0.8699,
      "step": 8976
    },
    {
      "epoch": 0.7215827338129497,
      "grad_norm": 13.338132858276367,
      "learning_rate": 4e-06,
      "loss": 0.8511,
      "step": 9027
    },
    {
      "epoch": 0.7256594724220623,
      "grad_norm": 14.742850303649902,
      "learning_rate": 4e-06,
      "loss": 0.8547,
      "step": 9078
    },
    {
      "epoch": 0.729736211031175,
      "grad_norm": 9.556371688842773,
      "learning_rate": 4e-06,
      "loss": 0.7772,
      "step": 9129
    },
    {
      "epoch": 0.7338129496402878,
      "grad_norm": 11.27623176574707,
      "learning_rate": 4e-06,
      "loss": 0.7797,
      "step": 9180
    },
    {
      "epoch": 0.7378896882494005,
      "grad_norm": 12.851790428161621,
      "learning_rate": 4e-06,
      "loss": 0.8167,
      "step": 9231
    },
    {
      "epoch": 0.7419664268585132,
      "grad_norm": 13.483719825744629,
      "learning_rate": 4e-06,
      "loss": 0.8262,
      "step": 9282
    },
    {
      "epoch": 0.746043165467626,
      "grad_norm": 9.68311882019043,
      "learning_rate": 4e-06,
      "loss": 0.8117,
      "step": 9333
    },
    {
      "epoch": 0.7501199040767386,
      "grad_norm": 12.276097297668457,
      "learning_rate": 4e-06,
      "loss": 0.7998,
      "step": 9384
    },
    {
      "epoch": 0.7541966426858513,
      "grad_norm": 13.016493797302246,
      "learning_rate": 4e-06,
      "loss": 0.8475,
      "step": 9435
    },
    {
      "epoch": 0.758273381294964,
      "grad_norm": 20.166629791259766,
      "learning_rate": 4e-06,
      "loss": 0.8323,
      "step": 9486
    },
    {
      "epoch": 0.7623501199040768,
      "grad_norm": 12.73173713684082,
      "learning_rate": 4e-06,
      "loss": 0.8051,
      "step": 9537
    },
    {
      "epoch": 0.7664268585131895,
      "grad_norm": 15.500396728515625,
      "learning_rate": 4e-06,
      "loss": 0.8097,
      "step": 9588
    },
    {
      "epoch": 0.7705035971223022,
      "grad_norm": 10.535673141479492,
      "learning_rate": 4e-06,
      "loss": 0.8194,
      "step": 9639
    },
    {
      "epoch": 0.7745803357314148,
      "grad_norm": 12.298761367797852,
      "learning_rate": 4e-06,
      "loss": 0.8413,
      "step": 9690
    },
    {
      "epoch": 0.7786570743405276,
      "grad_norm": 19.11263656616211,
      "learning_rate": 4e-06,
      "loss": 0.8403,
      "step": 9741
    },
    {
      "epoch": 0.7827338129496403,
      "grad_norm": 21.082683563232422,
      "learning_rate": 4e-06,
      "loss": 0.7974,
      "step": 9792
    },
    {
      "epoch": 0.786810551558753,
      "grad_norm": 8.226676940917969,
      "learning_rate": 4e-06,
      "loss": 0.8297,
      "step": 9843
    },
    {
      "epoch": 0.7908872901678657,
      "grad_norm": 9.943391799926758,
      "learning_rate": 4e-06,
      "loss": 0.8068,
      "step": 9894
    },
    {
      "epoch": 0.7949640287769785,
      "grad_norm": 10.409451484680176,
      "learning_rate": 4e-06,
      "loss": 0.8431,
      "step": 9945
    },
    {
      "epoch": 0.7990407673860911,
      "grad_norm": 12.029556274414062,
      "learning_rate": 4e-06,
      "loss": 0.7536,
      "step": 9996
    },
    {
      "epoch": 0.8031175059952038,
      "grad_norm": 10.037442207336426,
      "learning_rate": 4e-06,
      "loss": 0.832,
      "step": 10047
    },
    {
      "epoch": 0.8071942446043165,
      "grad_norm": 14.487872123718262,
      "learning_rate": 4e-06,
      "loss": 0.8476,
      "step": 10098
    },
    {
      "epoch": 0.8112709832134293,
      "grad_norm": 11.827603340148926,
      "learning_rate": 4e-06,
      "loss": 0.7889,
      "step": 10149
    },
    {
      "epoch": 0.815347721822542,
      "grad_norm": 29.357614517211914,
      "learning_rate": 4e-06,
      "loss": 0.8191,
      "step": 10200
    },
    {
      "epoch": 0.8194244604316546,
      "grad_norm": 10.284354209899902,
      "learning_rate": 4e-06,
      "loss": 0.8355,
      "step": 10251
    },
    {
      "epoch": 0.8235011990407674,
      "grad_norm": 19.447736740112305,
      "learning_rate": 4e-06,
      "loss": 0.8001,
      "step": 10302
    },
    {
      "epoch": 0.8275779376498801,
      "grad_norm": 11.485334396362305,
      "learning_rate": 4e-06,
      "loss": 0.7972,
      "step": 10353
    },
    {
      "epoch": 0.8316546762589928,
      "grad_norm": 17.476430892944336,
      "learning_rate": 4e-06,
      "loss": 0.7637,
      "step": 10404
    },
    {
      "epoch": 0.8357314148681055,
      "grad_norm": 12.081064224243164,
      "learning_rate": 4e-06,
      "loss": 0.7791,
      "step": 10455
    },
    {
      "epoch": 0.8398081534772183,
      "grad_norm": 9.355401039123535,
      "learning_rate": 4e-06,
      "loss": 0.773,
      "step": 10506
    },
    {
      "epoch": 0.8438848920863309,
      "grad_norm": 26.27705955505371,
      "learning_rate": 4e-06,
      "loss": 0.7943,
      "step": 10557
    },
    {
      "epoch": 0.8479616306954436,
      "grad_norm": 21.48892593383789,
      "learning_rate": 4e-06,
      "loss": 0.7993,
      "step": 10608
    },
    {
      "epoch": 0.8520383693045563,
      "grad_norm": 13.609570503234863,
      "learning_rate": 4e-06,
      "loss": 0.8026,
      "step": 10659
    },
    {
      "epoch": 0.8561151079136691,
      "grad_norm": 12.85317611694336,
      "learning_rate": 4e-06,
      "loss": 0.7624,
      "step": 10710
    },
    {
      "epoch": 0.8601918465227818,
      "grad_norm": 10.941635131835938,
      "learning_rate": 4e-06,
      "loss": 0.7592,
      "step": 10761
    },
    {
      "epoch": 0.8642685851318945,
      "grad_norm": 15.540253639221191,
      "learning_rate": 4e-06,
      "loss": 0.7981,
      "step": 10812
    },
    {
      "epoch": 0.8683453237410071,
      "grad_norm": 13.334186553955078,
      "learning_rate": 4e-06,
      "loss": 0.7857,
      "step": 10863
    },
    {
      "epoch": 0.8724220623501199,
      "grad_norm": 10.010978698730469,
      "learning_rate": 4e-06,
      "loss": 0.7713,
      "step": 10914
    },
    {
      "epoch": 0.8764988009592326,
      "grad_norm": 8.540175437927246,
      "learning_rate": 4e-06,
      "loss": 0.7799,
      "step": 10965
    },
    {
      "epoch": 0.8805755395683453,
      "grad_norm": 9.572399139404297,
      "learning_rate": 4e-06,
      "loss": 0.8079,
      "step": 11016
    },
    {
      "epoch": 0.8846522781774581,
      "grad_norm": 9.865501403808594,
      "learning_rate": 4e-06,
      "loss": 0.7952,
      "step": 11067
    },
    {
      "epoch": 0.8887290167865708,
      "grad_norm": 9.886651992797852,
      "learning_rate": 4e-06,
      "loss": 0.7721,
      "step": 11118
    },
    {
      "epoch": 0.8928057553956834,
      "grad_norm": 22.01490020751953,
      "learning_rate": 4e-06,
      "loss": 0.7569,
      "step": 11169
    },
    {
      "epoch": 0.8968824940047961,
      "grad_norm": 11.9666109085083,
      "learning_rate": 4e-06,
      "loss": 0.8083,
      "step": 11220
    },
    {
      "epoch": 0.9009592326139089,
      "grad_norm": 13.040547370910645,
      "learning_rate": 4e-06,
      "loss": 0.789,
      "step": 11271
    },
    {
      "epoch": 0.9050359712230216,
      "grad_norm": 13.846014976501465,
      "learning_rate": 4e-06,
      "loss": 0.7944,
      "step": 11322
    },
    {
      "epoch": 0.9091127098321343,
      "grad_norm": 13.245489120483398,
      "learning_rate": 4e-06,
      "loss": 0.7662,
      "step": 11373
    },
    {
      "epoch": 0.913189448441247,
      "grad_norm": 12.598053932189941,
      "learning_rate": 4e-06,
      "loss": 0.7383,
      "step": 11424
    },
    {
      "epoch": 0.9172661870503597,
      "grad_norm": 12.284782409667969,
      "learning_rate": 4e-06,
      "loss": 0.7724,
      "step": 11475
    },
    {
      "epoch": 0.9213429256594724,
      "grad_norm": 10.752656936645508,
      "learning_rate": 4e-06,
      "loss": 0.7675,
      "step": 11526
    },
    {
      "epoch": 0.9254196642685851,
      "grad_norm": 9.143753051757812,
      "learning_rate": 4e-06,
      "loss": 0.7695,
      "step": 11577
    },
    {
      "epoch": 0.9294964028776979,
      "grad_norm": 10.285872459411621,
      "learning_rate": 4e-06,
      "loss": 0.8053,
      "step": 11628
    },
    {
      "epoch": 0.9335731414868106,
      "grad_norm": 12.787908554077148,
      "learning_rate": 4e-06,
      "loss": 0.7635,
      "step": 11679
    },
    {
      "epoch": 0.9376498800959233,
      "grad_norm": 13.278891563415527,
      "learning_rate": 4e-06,
      "loss": 0.7578,
      "step": 11730
    },
    {
      "epoch": 0.9417266187050359,
      "grad_norm": 10.319464683532715,
      "learning_rate": 4e-06,
      "loss": 0.7711,
      "step": 11781
    },
    {
      "epoch": 0.9458033573141487,
      "grad_norm": 14.974687576293945,
      "learning_rate": 4e-06,
      "loss": 0.7679,
      "step": 11832
    },
    {
      "epoch": 0.9498800959232614,
      "grad_norm": 15.598155975341797,
      "learning_rate": 4e-06,
      "loss": 0.729,
      "step": 11883
    },
    {
      "epoch": 0.9539568345323741,
      "grad_norm": 14.075057029724121,
      "learning_rate": 4e-06,
      "loss": 0.7684,
      "step": 11934
    },
    {
      "epoch": 0.9580335731414868,
      "grad_norm": 9.000067710876465,
      "learning_rate": 4e-06,
      "loss": 0.7743,
      "step": 11985
    },
    {
      "epoch": 0.9621103117505995,
      "grad_norm": 10.701263427734375,
      "learning_rate": 4e-06,
      "loss": 0.7998,
      "step": 12036
    },
    {
      "epoch": 0.9661870503597122,
      "grad_norm": 13.15220832824707,
      "learning_rate": 4e-06,
      "loss": 0.7684,
      "step": 12087
    },
    {
      "epoch": 0.9702637889688249,
      "grad_norm": 9.827987670898438,
      "learning_rate": 4e-06,
      "loss": 0.7473,
      "step": 12138
    },
    {
      "epoch": 0.9743405275779377,
      "grad_norm": 22.329050064086914,
      "learning_rate": 4e-06,
      "loss": 0.7754,
      "step": 12189
    },
    {
      "epoch": 0.9784172661870504,
      "grad_norm": 11.974215507507324,
      "learning_rate": 4e-06,
      "loss": 0.7527,
      "step": 12240
    },
    {
      "epoch": 0.9824940047961631,
      "grad_norm": 11.39158821105957,
      "learning_rate": 4e-06,
      "loss": 0.7227,
      "step": 12291
    },
    {
      "epoch": 0.9865707434052757,
      "grad_norm": 16.695035934448242,
      "learning_rate": 4e-06,
      "loss": 0.7485,
      "step": 12342
    },
    {
      "epoch": 0.9906474820143885,
      "grad_norm": 11.66939640045166,
      "learning_rate": 4e-06,
      "loss": 0.7892,
      "step": 12393
    },
    {
      "epoch": 0.9947242206235012,
      "grad_norm": 10.497869491577148,
      "learning_rate": 4e-06,
      "loss": 0.7483,
      "step": 12444
    },
    {
      "epoch": 0.9988009592326139,
      "grad_norm": 17.223838806152344,
      "learning_rate": 4e-06,
      "loss": 0.73,
      "step": 12495
    },
    {
      "epoch": 1.0028776978417266,
      "grad_norm": 13.55618953704834,
      "learning_rate": 4e-06,
      "loss": 0.712,
      "step": 12546
    },
    {
      "epoch": 1.0069544364508394,
      "grad_norm": 14.871660232543945,
      "learning_rate": 4e-06,
      "loss": 0.6283,
      "step": 12597
    },
    {
      "epoch": 1.011031175059952,
      "grad_norm": 14.342114448547363,
      "learning_rate": 4e-06,
      "loss": 0.6545,
      "step": 12648
    },
    {
      "epoch": 1.0151079136690648,
      "grad_norm": 13.754369735717773,
      "learning_rate": 4e-06,
      "loss": 0.6477,
      "step": 12699
    },
    {
      "epoch": 1.0191846522781776,
      "grad_norm": 10.246557235717773,
      "learning_rate": 4e-06,
      "loss": 0.621,
      "step": 12750
    },
    {
      "epoch": 1.02326139088729,
      "grad_norm": 14.873066902160645,
      "learning_rate": 4e-06,
      "loss": 0.6694,
      "step": 12801
    },
    {
      "epoch": 1.0273381294964028,
      "grad_norm": 10.598921775817871,
      "learning_rate": 4e-06,
      "loss": 0.6104,
      "step": 12852
    },
    {
      "epoch": 1.0314148681055155,
      "grad_norm": 17.1934814453125,
      "learning_rate": 4e-06,
      "loss": 0.629,
      "step": 12903
    },
    {
      "epoch": 1.0354916067146283,
      "grad_norm": 14.199798583984375,
      "learning_rate": 4e-06,
      "loss": 0.6828,
      "step": 12954
    },
    {
      "epoch": 1.039568345323741,
      "grad_norm": 15.648553848266602,
      "learning_rate": 4e-06,
      "loss": 0.6796,
      "step": 13005
    },
    {
      "epoch": 1.0436450839328537,
      "grad_norm": 11.52861213684082,
      "learning_rate": 4e-06,
      "loss": 0.6284,
      "step": 13056
    },
    {
      "epoch": 1.0477218225419664,
      "grad_norm": 9.223957061767578,
      "learning_rate": 4e-06,
      "loss": 0.6357,
      "step": 13107
    },
    {
      "epoch": 1.0517985611510792,
      "grad_norm": 13.58945369720459,
      "learning_rate": 4e-06,
      "loss": 0.6586,
      "step": 13158
    },
    {
      "epoch": 1.055875299760192,
      "grad_norm": 15.256808280944824,
      "learning_rate": 4e-06,
      "loss": 0.6592,
      "step": 13209
    },
    {
      "epoch": 1.0599520383693046,
      "grad_norm": 16.01530647277832,
      "learning_rate": 4e-06,
      "loss": 0.6457,
      "step": 13260
    },
    {
      "epoch": 1.0640287769784174,
      "grad_norm": 12.192428588867188,
      "learning_rate": 4e-06,
      "loss": 0.6662,
      "step": 13311
    },
    {
      "epoch": 1.06810551558753,
      "grad_norm": 9.547392845153809,
      "learning_rate": 4e-06,
      "loss": 0.6369,
      "step": 13362
    },
    {
      "epoch": 1.0721822541966426,
      "grad_norm": 11.454010963439941,
      "learning_rate": 4e-06,
      "loss": 0.6277,
      "step": 13413
    },
    {
      "epoch": 1.0762589928057553,
      "grad_norm": 14.291218757629395,
      "learning_rate": 4e-06,
      "loss": 0.6541,
      "step": 13464
    },
    {
      "epoch": 1.080335731414868,
      "grad_norm": 9.902359962463379,
      "learning_rate": 4e-06,
      "loss": 0.6304,
      "step": 13515
    },
    {
      "epoch": 1.0844124700239808,
      "grad_norm": 14.680432319641113,
      "learning_rate": 4e-06,
      "loss": 0.6394,
      "step": 13566
    },
    {
      "epoch": 1.0884892086330935,
      "grad_norm": 12.332036972045898,
      "learning_rate": 4e-06,
      "loss": 0.6482,
      "step": 13617
    },
    {
      "epoch": 1.0925659472422062,
      "grad_norm": 12.840729713439941,
      "learning_rate": 4e-06,
      "loss": 0.6308,
      "step": 13668
    },
    {
      "epoch": 1.096642685851319,
      "grad_norm": 10.258298873901367,
      "learning_rate": 4e-06,
      "loss": 0.6231,
      "step": 13719
    },
    {
      "epoch": 1.1007194244604317,
      "grad_norm": 23.80570411682129,
      "learning_rate": 4e-06,
      "loss": 0.6462,
      "step": 13770
    },
    {
      "epoch": 1.1047961630695444,
      "grad_norm": 9.304780006408691,
      "learning_rate": 4e-06,
      "loss": 0.637,
      "step": 13821
    },
    {
      "epoch": 1.1088729016786572,
      "grad_norm": 12.443550109863281,
      "learning_rate": 4e-06,
      "loss": 0.615,
      "step": 13872
    },
    {
      "epoch": 1.1129496402877699,
      "grad_norm": 12.204788208007812,
      "learning_rate": 4e-06,
      "loss": 0.6433,
      "step": 13923
    },
    {
      "epoch": 1.1170263788968824,
      "grad_norm": 16.12440299987793,
      "learning_rate": 4e-06,
      "loss": 0.6334,
      "step": 13974
    },
    {
      "epoch": 1.1211031175059951,
      "grad_norm": 10.598883628845215,
      "learning_rate": 4e-06,
      "loss": 0.6149,
      "step": 14025
    },
    {
      "epoch": 1.1251798561151078,
      "grad_norm": 15.690093994140625,
      "learning_rate": 4e-06,
      "loss": 0.6053,
      "step": 14076
    },
    {
      "epoch": 1.1292565947242206,
      "grad_norm": 9.981781005859375,
      "learning_rate": 4e-06,
      "loss": 0.6348,
      "step": 14127
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 14.008990287780762,
      "learning_rate": 4e-06,
      "loss": 0.651,
      "step": 14178
    },
    {
      "epoch": 1.137410071942446,
      "grad_norm": 10.720271110534668,
      "learning_rate": 4e-06,
      "loss": 0.6446,
      "step": 14229
    },
    {
      "epoch": 1.1414868105515588,
      "grad_norm": 9.859781265258789,
      "learning_rate": 4e-06,
      "loss": 0.6472,
      "step": 14280
    },
    {
      "epoch": 1.1455635491606715,
      "grad_norm": 10.708683013916016,
      "learning_rate": 4e-06,
      "loss": 0.6299,
      "step": 14331
    },
    {
      "epoch": 1.1496402877697842,
      "grad_norm": 17.842166900634766,
      "learning_rate": 4e-06,
      "loss": 0.6386,
      "step": 14382
    },
    {
      "epoch": 1.153717026378897,
      "grad_norm": 11.921455383300781,
      "learning_rate": 4e-06,
      "loss": 0.6523,
      "step": 14433
    },
    {
      "epoch": 1.1577937649880097,
      "grad_norm": 8.469239234924316,
      "learning_rate": 4e-06,
      "loss": 0.6295,
      "step": 14484
    },
    {
      "epoch": 1.1618705035971222,
      "grad_norm": 9.445391654968262,
      "learning_rate": 4e-06,
      "loss": 0.6179,
      "step": 14535
    },
    {
      "epoch": 1.1659472422062351,
      "grad_norm": 13.796358108520508,
      "learning_rate": 4e-06,
      "loss": 0.6344,
      "step": 14586
    },
    {
      "epoch": 1.1700239808153476,
      "grad_norm": 10.286890029907227,
      "learning_rate": 4e-06,
      "loss": 0.6347,
      "step": 14637
    },
    {
      "epoch": 1.1741007194244604,
      "grad_norm": 15.482422828674316,
      "learning_rate": 4e-06,
      "loss": 0.6081,
      "step": 14688
    },
    {
      "epoch": 1.178177458033573,
      "grad_norm": 16.478471755981445,
      "learning_rate": 4e-06,
      "loss": 0.6073,
      "step": 14739
    },
    {
      "epoch": 1.1822541966426858,
      "grad_norm": 12.172346115112305,
      "learning_rate": 4e-06,
      "loss": 0.6286,
      "step": 14790
    },
    {
      "epoch": 1.1863309352517986,
      "grad_norm": 10.262702941894531,
      "learning_rate": 4e-06,
      "loss": 0.6448,
      "step": 14841
    },
    {
      "epoch": 1.1904076738609113,
      "grad_norm": 9.533632278442383,
      "learning_rate": 4e-06,
      "loss": 0.6006,
      "step": 14892
    },
    {
      "epoch": 1.194484412470024,
      "grad_norm": 10.540053367614746,
      "learning_rate": 4e-06,
      "loss": 0.6342,
      "step": 14943
    },
    {
      "epoch": 1.1985611510791367,
      "grad_norm": 11.66978931427002,
      "learning_rate": 4e-06,
      "loss": 0.6325,
      "step": 14994
    },
    {
      "epoch": 1.2026378896882495,
      "grad_norm": 9.354032516479492,
      "learning_rate": 4e-06,
      "loss": 0.6448,
      "step": 15045
    },
    {
      "epoch": 1.2067146282973622,
      "grad_norm": 12.874418258666992,
      "learning_rate": 4e-06,
      "loss": 0.6268,
      "step": 15096
    },
    {
      "epoch": 1.210791366906475,
      "grad_norm": 13.874662399291992,
      "learning_rate": 4e-06,
      "loss": 0.593,
      "step": 15147
    },
    {
      "epoch": 1.2148681055155874,
      "grad_norm": 10.280763626098633,
      "learning_rate": 4e-06,
      "loss": 0.6324,
      "step": 15198
    },
    {
      "epoch": 1.2189448441247002,
      "grad_norm": 14.202062606811523,
      "learning_rate": 4e-06,
      "loss": 0.633,
      "step": 15249
    },
    {
      "epoch": 1.223021582733813,
      "grad_norm": 12.310157775878906,
      "learning_rate": 4e-06,
      "loss": 0.6276,
      "step": 15300
    },
    {
      "epoch": 1.2270983213429256,
      "grad_norm": 9.84839153289795,
      "learning_rate": 4e-06,
      "loss": 0.6189,
      "step": 15351
    },
    {
      "epoch": 1.2311750599520384,
      "grad_norm": 14.100603103637695,
      "learning_rate": 4e-06,
      "loss": 0.5986,
      "step": 15402
    },
    {
      "epoch": 1.235251798561151,
      "grad_norm": 9.786356925964355,
      "learning_rate": 4e-06,
      "loss": 0.6288,
      "step": 15453
    },
    {
      "epoch": 1.2393285371702638,
      "grad_norm": 32.717079162597656,
      "learning_rate": 4e-06,
      "loss": 0.6171,
      "step": 15504
    },
    {
      "epoch": 1.2434052757793765,
      "grad_norm": 20.77557373046875,
      "learning_rate": 4e-06,
      "loss": 0.6398,
      "step": 15555
    },
    {
      "epoch": 1.2474820143884893,
      "grad_norm": 55.0799446105957,
      "learning_rate": 4e-06,
      "loss": 0.6624,
      "step": 15606
    },
    {
      "epoch": 1.251558752997602,
      "grad_norm": 16.119102478027344,
      "learning_rate": 4e-06,
      "loss": 0.5971,
      "step": 15657
    },
    {
      "epoch": 1.2556354916067147,
      "grad_norm": 16.805065155029297,
      "learning_rate": 4e-06,
      "loss": 0.6579,
      "step": 15708
    },
    {
      "epoch": 1.2597122302158272,
      "grad_norm": 12.102901458740234,
      "learning_rate": 4e-06,
      "loss": 0.6389,
      "step": 15759
    },
    {
      "epoch": 1.2637889688249402,
      "grad_norm": 9.73510456085205,
      "learning_rate": 4e-06,
      "loss": 0.6312,
      "step": 15810
    },
    {
      "epoch": 1.2678657074340527,
      "grad_norm": 16.27564239501953,
      "learning_rate": 4e-06,
      "loss": 0.5859,
      "step": 15861
    },
    {
      "epoch": 1.2719424460431654,
      "grad_norm": 11.277665138244629,
      "learning_rate": 4e-06,
      "loss": 0.627,
      "step": 15912
    },
    {
      "epoch": 1.2760191846522781,
      "grad_norm": 17.935928344726562,
      "learning_rate": 4e-06,
      "loss": 0.6172,
      "step": 15963
    },
    {
      "epoch": 1.2800959232613909,
      "grad_norm": 12.354071617126465,
      "learning_rate": 4e-06,
      "loss": 0.6217,
      "step": 16014
    },
    {
      "epoch": 1.2841726618705036,
      "grad_norm": 12.68001937866211,
      "learning_rate": 4e-06,
      "loss": 0.5979,
      "step": 16065
    },
    {
      "epoch": 1.2882494004796163,
      "grad_norm": 19.063947677612305,
      "learning_rate": 4e-06,
      "loss": 0.6246,
      "step": 16116
    },
    {
      "epoch": 1.292326139088729,
      "grad_norm": 9.300283432006836,
      "learning_rate": 4e-06,
      "loss": 0.6149,
      "step": 16167
    },
    {
      "epoch": 1.2964028776978418,
      "grad_norm": 9.397138595581055,
      "learning_rate": 4e-06,
      "loss": 0.6377,
      "step": 16218
    },
    {
      "epoch": 1.3004796163069545,
      "grad_norm": 13.179905891418457,
      "learning_rate": 4e-06,
      "loss": 0.6097,
      "step": 16269
    },
    {
      "epoch": 1.304556354916067,
      "grad_norm": 9.413117408752441,
      "learning_rate": 4e-06,
      "loss": 0.6084,
      "step": 16320
    },
    {
      "epoch": 1.30863309352518,
      "grad_norm": 13.201662063598633,
      "learning_rate": 4e-06,
      "loss": 0.605,
      "step": 16371
    },
    {
      "epoch": 1.3127098321342925,
      "grad_norm": 12.346199989318848,
      "learning_rate": 4e-06,
      "loss": 0.6193,
      "step": 16422
    },
    {
      "epoch": 1.3167865707434052,
      "grad_norm": 9.08266830444336,
      "learning_rate": 4e-06,
      "loss": 0.6381,
      "step": 16473
    },
    {
      "epoch": 1.320863309352518,
      "grad_norm": 13.999785423278809,
      "learning_rate": 4e-06,
      "loss": 0.5925,
      "step": 16524
    },
    {
      "epoch": 1.3249400479616307,
      "grad_norm": 18.534244537353516,
      "learning_rate": 4e-06,
      "loss": 0.6116,
      "step": 16575
    },
    {
      "epoch": 1.3290167865707434,
      "grad_norm": 12.14611530303955,
      "learning_rate": 4e-06,
      "loss": 0.6169,
      "step": 16626
    },
    {
      "epoch": 1.3330935251798561,
      "grad_norm": 11.302271842956543,
      "learning_rate": 4e-06,
      "loss": 0.6163,
      "step": 16677
    },
    {
      "epoch": 1.3371702637889689,
      "grad_norm": 9.6621675491333,
      "learning_rate": 4e-06,
      "loss": 0.598,
      "step": 16728
    },
    {
      "epoch": 1.3412470023980816,
      "grad_norm": 10.255205154418945,
      "learning_rate": 4e-06,
      "loss": 0.6529,
      "step": 16779
    },
    {
      "epoch": 1.3453237410071943,
      "grad_norm": 11.915422439575195,
      "learning_rate": 4e-06,
      "loss": 0.6445,
      "step": 16830
    },
    {
      "epoch": 1.3494004796163068,
      "grad_norm": 11.743886947631836,
      "learning_rate": 4e-06,
      "loss": 0.6001,
      "step": 16881
    },
    {
      "epoch": 1.3534772182254198,
      "grad_norm": 13.21473217010498,
      "learning_rate": 4e-06,
      "loss": 0.603,
      "step": 16932
    },
    {
      "epoch": 1.3575539568345323,
      "grad_norm": 13.298608779907227,
      "learning_rate": 4e-06,
      "loss": 0.651,
      "step": 16983
    },
    {
      "epoch": 1.361630695443645,
      "grad_norm": 15.497859001159668,
      "learning_rate": 4e-06,
      "loss": 0.6011,
      "step": 17034
    },
    {
      "epoch": 1.3657074340527577,
      "grad_norm": 10.094143867492676,
      "learning_rate": 4e-06,
      "loss": 0.6265,
      "step": 17085
    },
    {
      "epoch": 1.3697841726618705,
      "grad_norm": 10.850165367126465,
      "learning_rate": 4e-06,
      "loss": 0.6333,
      "step": 17136
    },
    {
      "epoch": 1.3738609112709832,
      "grad_norm": 14.918022155761719,
      "learning_rate": 4e-06,
      "loss": 0.6249,
      "step": 17187
    },
    {
      "epoch": 1.377937649880096,
      "grad_norm": 13.82544231414795,
      "learning_rate": 4e-06,
      "loss": 0.6209,
      "step": 17238
    },
    {
      "epoch": 1.3820143884892087,
      "grad_norm": 13.127535820007324,
      "learning_rate": 4e-06,
      "loss": 0.6249,
      "step": 17289
    },
    {
      "epoch": 1.3860911270983214,
      "grad_norm": 16.29973602294922,
      "learning_rate": 4e-06,
      "loss": 0.6152,
      "step": 17340
    },
    {
      "epoch": 1.3901678657074341,
      "grad_norm": 10.20478343963623,
      "learning_rate": 4e-06,
      "loss": 0.6223,
      "step": 17391
    },
    {
      "epoch": 1.3942446043165468,
      "grad_norm": 11.061800956726074,
      "learning_rate": 4e-06,
      "loss": 0.5995,
      "step": 17442
    },
    {
      "epoch": 1.3983213429256596,
      "grad_norm": 13.203932762145996,
      "learning_rate": 4e-06,
      "loss": 0.6263,
      "step": 17493
    },
    {
      "epoch": 1.402398081534772,
      "grad_norm": 9.814327239990234,
      "learning_rate": 4e-06,
      "loss": 0.6188,
      "step": 17544
    },
    {
      "epoch": 1.406474820143885,
      "grad_norm": 10.484615325927734,
      "learning_rate": 4e-06,
      "loss": 0.6128,
      "step": 17595
    },
    {
      "epoch": 1.4105515587529975,
      "grad_norm": 12.051752090454102,
      "learning_rate": 4e-06,
      "loss": 0.5822,
      "step": 17646
    },
    {
      "epoch": 1.4146282973621103,
      "grad_norm": 10.48294734954834,
      "learning_rate": 4e-06,
      "loss": 0.6155,
      "step": 17697
    },
    {
      "epoch": 1.418705035971223,
      "grad_norm": 14.729616165161133,
      "learning_rate": 4e-06,
      "loss": 0.6092,
      "step": 17748
    },
    {
      "epoch": 1.4227817745803357,
      "grad_norm": 20.98146629333496,
      "learning_rate": 4e-06,
      "loss": 0.6256,
      "step": 17799
    },
    {
      "epoch": 1.4268585131894485,
      "grad_norm": 11.102290153503418,
      "learning_rate": 4e-06,
      "loss": 0.6079,
      "step": 17850
    },
    {
      "epoch": 1.4309352517985612,
      "grad_norm": 13.500692367553711,
      "learning_rate": 4e-06,
      "loss": 0.6158,
      "step": 17901
    },
    {
      "epoch": 1.435011990407674,
      "grad_norm": 12.424516677856445,
      "learning_rate": 4e-06,
      "loss": 0.6001,
      "step": 17952
    },
    {
      "epoch": 1.4390887290167866,
      "grad_norm": 15.996665000915527,
      "learning_rate": 4e-06,
      "loss": 0.6075,
      "step": 18003
    },
    {
      "epoch": 1.4431654676258994,
      "grad_norm": 9.632010459899902,
      "learning_rate": 4e-06,
      "loss": 0.635,
      "step": 18054
    },
    {
      "epoch": 1.4472422062350119,
      "grad_norm": 10.449573516845703,
      "learning_rate": 4e-06,
      "loss": 0.631,
      "step": 18105
    },
    {
      "epoch": 1.4513189448441248,
      "grad_norm": 10.533821105957031,
      "learning_rate": 4e-06,
      "loss": 0.6223,
      "step": 18156
    },
    {
      "epoch": 1.4553956834532373,
      "grad_norm": 12.578534126281738,
      "learning_rate": 4e-06,
      "loss": 0.6343,
      "step": 18207
    },
    {
      "epoch": 1.45947242206235,
      "grad_norm": 13.005437850952148,
      "learning_rate": 4e-06,
      "loss": 0.5885,
      "step": 18258
    },
    {
      "epoch": 1.4635491606714628,
      "grad_norm": 13.544271469116211,
      "learning_rate": 4e-06,
      "loss": 0.6115,
      "step": 18309
    },
    {
      "epoch": 1.4676258992805755,
      "grad_norm": 12.877237319946289,
      "learning_rate": 4e-06,
      "loss": 0.5987,
      "step": 18360
    },
    {
      "epoch": 1.4717026378896882,
      "grad_norm": 11.676237106323242,
      "learning_rate": 4e-06,
      "loss": 0.6592,
      "step": 18411
    },
    {
      "epoch": 1.475779376498801,
      "grad_norm": 11.255859375,
      "learning_rate": 4e-06,
      "loss": 0.6363,
      "step": 18462
    },
    {
      "epoch": 1.4798561151079137,
      "grad_norm": 8.410277366638184,
      "learning_rate": 4e-06,
      "loss": 0.5915,
      "step": 18513
    },
    {
      "epoch": 1.4839328537170264,
      "grad_norm": 9.25663948059082,
      "learning_rate": 4e-06,
      "loss": 0.5981,
      "step": 18564
    },
    {
      "epoch": 1.4880095923261392,
      "grad_norm": 14.194123268127441,
      "learning_rate": 4e-06,
      "loss": 0.6011,
      "step": 18615
    },
    {
      "epoch": 1.4920863309352517,
      "grad_norm": 11.938138961791992,
      "learning_rate": 4e-06,
      "loss": 0.5935,
      "step": 18666
    },
    {
      "epoch": 1.4961630695443646,
      "grad_norm": 12.044941902160645,
      "learning_rate": 4e-06,
      "loss": 0.6036,
      "step": 18717
    },
    {
      "epoch": 1.5002398081534771,
      "grad_norm": 9.250407218933105,
      "learning_rate": 4e-06,
      "loss": 0.5876,
      "step": 18768
    },
    {
      "epoch": 1.50431654676259,
      "grad_norm": 15.646172523498535,
      "learning_rate": 4e-06,
      "loss": 0.6123,
      "step": 18819
    },
    {
      "epoch": 1.5083932853717026,
      "grad_norm": 14.72516918182373,
      "learning_rate": 4e-06,
      "loss": 0.6115,
      "step": 18870
    },
    {
      "epoch": 1.5124700239808153,
      "grad_norm": 9.683553695678711,
      "learning_rate": 4e-06,
      "loss": 0.6131,
      "step": 18921
    },
    {
      "epoch": 1.516546762589928,
      "grad_norm": 10.628843307495117,
      "learning_rate": 4e-06,
      "loss": 0.6261,
      "step": 18972
    },
    {
      "epoch": 1.5206235011990408,
      "grad_norm": 12.379210472106934,
      "learning_rate": 4e-06,
      "loss": 0.5617,
      "step": 19023
    },
    {
      "epoch": 1.5247002398081535,
      "grad_norm": 19.257949829101562,
      "learning_rate": 4e-06,
      "loss": 0.6215,
      "step": 19074
    },
    {
      "epoch": 1.5287769784172662,
      "grad_norm": 12.156522750854492,
      "learning_rate": 4e-06,
      "loss": 0.5841,
      "step": 19125
    },
    {
      "epoch": 1.532853717026379,
      "grad_norm": 12.93503475189209,
      "learning_rate": 4e-06,
      "loss": 0.6103,
      "step": 19176
    },
    {
      "epoch": 1.5369304556354915,
      "grad_norm": 9.786437034606934,
      "learning_rate": 4e-06,
      "loss": 0.6163,
      "step": 19227
    },
    {
      "epoch": 1.5410071942446044,
      "grad_norm": 11.665353775024414,
      "learning_rate": 4e-06,
      "loss": 0.5972,
      "step": 19278
    },
    {
      "epoch": 1.545083932853717,
      "grad_norm": 16.958820343017578,
      "learning_rate": 4e-06,
      "loss": 0.6126,
      "step": 19329
    },
    {
      "epoch": 1.5491606714628299,
      "grad_norm": 11.81750202178955,
      "learning_rate": 4e-06,
      "loss": 0.5911,
      "step": 19380
    },
    {
      "epoch": 1.5532374100719424,
      "grad_norm": 7.661946773529053,
      "learning_rate": 4e-06,
      "loss": 0.5802,
      "step": 19431
    },
    {
      "epoch": 1.557314148681055,
      "grad_norm": 12.106283187866211,
      "learning_rate": 4e-06,
      "loss": 0.6273,
      "step": 19482
    },
    {
      "epoch": 1.5613908872901678,
      "grad_norm": 22.994096755981445,
      "learning_rate": 4e-06,
      "loss": 0.6211,
      "step": 19533
    },
    {
      "epoch": 1.5654676258992806,
      "grad_norm": 13.419297218322754,
      "learning_rate": 4e-06,
      "loss": 0.6429,
      "step": 19584
    },
    {
      "epoch": 1.5695443645083933,
      "grad_norm": 18.543142318725586,
      "learning_rate": 4e-06,
      "loss": 0.5999,
      "step": 19635
    },
    {
      "epoch": 1.573621103117506,
      "grad_norm": 9.90273380279541,
      "learning_rate": 4e-06,
      "loss": 0.5853,
      "step": 19686
    },
    {
      "epoch": 1.5776978417266188,
      "grad_norm": 13.030680656433105,
      "learning_rate": 4e-06,
      "loss": 0.5775,
      "step": 19737
    },
    {
      "epoch": 1.5817745803357313,
      "grad_norm": 10.32126235961914,
      "learning_rate": 4e-06,
      "loss": 0.5805,
      "step": 19788
    },
    {
      "epoch": 1.5858513189448442,
      "grad_norm": 14.030089378356934,
      "learning_rate": 4e-06,
      "loss": 0.6189,
      "step": 19839
    },
    {
      "epoch": 1.5899280575539567,
      "grad_norm": 12.940481185913086,
      "learning_rate": 4e-06,
      "loss": 0.5932,
      "step": 19890
    },
    {
      "epoch": 1.5940047961630697,
      "grad_norm": 10.846888542175293,
      "learning_rate": 4e-06,
      "loss": 0.6289,
      "step": 19941
    },
    {
      "epoch": 1.5980815347721822,
      "grad_norm": 16.07390785217285,
      "learning_rate": 4e-06,
      "loss": 0.606,
      "step": 19992
    },
    {
      "epoch": 1.6021582733812951,
      "grad_norm": 12.921998977661133,
      "learning_rate": 4e-06,
      "loss": 0.6348,
      "step": 20043
    },
    {
      "epoch": 1.6062350119904076,
      "grad_norm": 22.18972396850586,
      "learning_rate": 4e-06,
      "loss": 0.6195,
      "step": 20094
    },
    {
      "epoch": 1.6103117505995204,
      "grad_norm": 17.51222801208496,
      "learning_rate": 4e-06,
      "loss": 0.6274,
      "step": 20145
    },
    {
      "epoch": 1.614388489208633,
      "grad_norm": 14.936586380004883,
      "learning_rate": 4e-06,
      "loss": 0.5827,
      "step": 20196
    },
    {
      "epoch": 1.6184652278177458,
      "grad_norm": 10.241886138916016,
      "learning_rate": 4e-06,
      "loss": 0.6147,
      "step": 20247
    },
    {
      "epoch": 1.6225419664268586,
      "grad_norm": 16.38634490966797,
      "learning_rate": 4e-06,
      "loss": 0.5874,
      "step": 20298
    },
    {
      "epoch": 1.626618705035971,
      "grad_norm": 11.271408081054688,
      "learning_rate": 4e-06,
      "loss": 0.6182,
      "step": 20349
    },
    {
      "epoch": 1.630695443645084,
      "grad_norm": 15.623200416564941,
      "learning_rate": 4e-06,
      "loss": 0.6002,
      "step": 20400
    },
    {
      "epoch": 1.6347721822541965,
      "grad_norm": 12.039374351501465,
      "learning_rate": 4e-06,
      "loss": 0.5901,
      "step": 20451
    },
    {
      "epoch": 1.6388489208633095,
      "grad_norm": 10.85869026184082,
      "learning_rate": 4e-06,
      "loss": 0.5757,
      "step": 20502
    },
    {
      "epoch": 1.642925659472422,
      "grad_norm": 22.742368698120117,
      "learning_rate": 4e-06,
      "loss": 0.6091,
      "step": 20553
    },
    {
      "epoch": 1.647002398081535,
      "grad_norm": 14.31108283996582,
      "learning_rate": 4e-06,
      "loss": 0.5877,
      "step": 20604
    },
    {
      "epoch": 1.6510791366906474,
      "grad_norm": 18.32335090637207,
      "learning_rate": 4e-06,
      "loss": 0.6079,
      "step": 20655
    },
    {
      "epoch": 1.6551558752997602,
      "grad_norm": 11.915358543395996,
      "learning_rate": 4e-06,
      "loss": 0.5762,
      "step": 20706
    },
    {
      "epoch": 1.6592326139088729,
      "grad_norm": 12.319726943969727,
      "learning_rate": 4e-06,
      "loss": 0.6018,
      "step": 20757
    },
    {
      "epoch": 1.6633093525179856,
      "grad_norm": 14.848578453063965,
      "learning_rate": 4e-06,
      "loss": 0.5905,
      "step": 20808
    },
    {
      "epoch": 1.6673860911270983,
      "grad_norm": 17.781530380249023,
      "learning_rate": 4e-06,
      "loss": 0.619,
      "step": 20859
    },
    {
      "epoch": 1.671462829736211,
      "grad_norm": 12.02390193939209,
      "learning_rate": 4e-06,
      "loss": 0.5967,
      "step": 20910
    },
    {
      "epoch": 1.6755395683453238,
      "grad_norm": 12.806224822998047,
      "learning_rate": 4e-06,
      "loss": 0.5955,
      "step": 20961
    },
    {
      "epoch": 1.6796163069544363,
      "grad_norm": 12.873128890991211,
      "learning_rate": 4e-06,
      "loss": 0.5936,
      "step": 21012
    },
    {
      "epoch": 1.6836930455635493,
      "grad_norm": 9.47560977935791,
      "learning_rate": 4e-06,
      "loss": 0.5844,
      "step": 21063
    },
    {
      "epoch": 1.6877697841726618,
      "grad_norm": 10.010106086730957,
      "learning_rate": 4e-06,
      "loss": 0.5935,
      "step": 21114
    },
    {
      "epoch": 1.6918465227817747,
      "grad_norm": 15.164613723754883,
      "learning_rate": 4e-06,
      "loss": 0.616,
      "step": 21165
    },
    {
      "epoch": 1.6959232613908872,
      "grad_norm": 11.419065475463867,
      "learning_rate": 4e-06,
      "loss": 0.5541,
      "step": 21216
    },
    {
      "epoch": 1.7,
      "grad_norm": 8.661081314086914,
      "learning_rate": 4e-06,
      "loss": 0.5872,
      "step": 21267
    },
    {
      "epoch": 1.7040767386091127,
      "grad_norm": 24.847139358520508,
      "learning_rate": 4e-06,
      "loss": 0.6056,
      "step": 21318
    },
    {
      "epoch": 1.7081534772182254,
      "grad_norm": 6.945821285247803,
      "learning_rate": 4e-06,
      "loss": 0.5919,
      "step": 21369
    },
    {
      "epoch": 1.7122302158273381,
      "grad_norm": 17.141542434692383,
      "learning_rate": 4e-06,
      "loss": 0.5869,
      "step": 21420
    },
    {
      "epoch": 1.7163069544364509,
      "grad_norm": 9.604776382446289,
      "learning_rate": 4e-06,
      "loss": 0.5975,
      "step": 21471
    },
    {
      "epoch": 1.7203836930455636,
      "grad_norm": 12.08397388458252,
      "learning_rate": 4e-06,
      "loss": 0.6289,
      "step": 21522
    },
    {
      "epoch": 1.724460431654676,
      "grad_norm": 11.738258361816406,
      "learning_rate": 4e-06,
      "loss": 0.6309,
      "step": 21573
    },
    {
      "epoch": 1.728537170263789,
      "grad_norm": 9.105463027954102,
      "learning_rate": 4e-06,
      "loss": 0.5588,
      "step": 21624
    },
    {
      "epoch": 1.7326139088729016,
      "grad_norm": 13.473869323730469,
      "learning_rate": 4e-06,
      "loss": 0.5655,
      "step": 21675
    },
    {
      "epoch": 1.7366906474820145,
      "grad_norm": 16.97240447998047,
      "learning_rate": 4e-06,
      "loss": 0.5704,
      "step": 21726
    },
    {
      "epoch": 1.740767386091127,
      "grad_norm": 11.526656150817871,
      "learning_rate": 4e-06,
      "loss": 0.598,
      "step": 21777
    },
    {
      "epoch": 1.74484412470024,
      "grad_norm": 16.830228805541992,
      "learning_rate": 4e-06,
      "loss": 0.5754,
      "step": 21828
    },
    {
      "epoch": 1.7489208633093525,
      "grad_norm": 14.634958267211914,
      "learning_rate": 4e-06,
      "loss": 0.5587,
      "step": 21879
    },
    {
      "epoch": 1.7529976019184652,
      "grad_norm": 13.176641464233398,
      "learning_rate": 4e-06,
      "loss": 0.6056,
      "step": 21930
    },
    {
      "epoch": 1.757074340527578,
      "grad_norm": 10.850558280944824,
      "learning_rate": 4e-06,
      "loss": 0.5749,
      "step": 21981
    },
    {
      "epoch": 1.7611510791366907,
      "grad_norm": 12.636214256286621,
      "learning_rate": 4e-06,
      "loss": 0.6069,
      "step": 22032
    },
    {
      "epoch": 1.7652278177458034,
      "grad_norm": 16.436723709106445,
      "learning_rate": 4e-06,
      "loss": 0.5978,
      "step": 22083
    },
    {
      "epoch": 1.769304556354916,
      "grad_norm": 10.360323905944824,
      "learning_rate": 4e-06,
      "loss": 0.561,
      "step": 22134
    },
    {
      "epoch": 1.7733812949640289,
      "grad_norm": 9.114195823669434,
      "learning_rate": 4e-06,
      "loss": 0.5654,
      "step": 22185
    },
    {
      "epoch": 1.7774580335731414,
      "grad_norm": 22.40851593017578,
      "learning_rate": 4e-06,
      "loss": 0.6098,
      "step": 22236
    },
    {
      "epoch": 1.7815347721822543,
      "grad_norm": 11.027215957641602,
      "learning_rate": 4e-06,
      "loss": 0.5942,
      "step": 22287
    },
    {
      "epoch": 1.7856115107913668,
      "grad_norm": 12.282816886901855,
      "learning_rate": 4e-06,
      "loss": 0.5675,
      "step": 22338
    },
    {
      "epoch": 1.7896882494004798,
      "grad_norm": 10.265154838562012,
      "learning_rate": 4e-06,
      "loss": 0.5957,
      "step": 22389
    },
    {
      "epoch": 1.7937649880095923,
      "grad_norm": 11.424843788146973,
      "learning_rate": 4e-06,
      "loss": 0.5827,
      "step": 22440
    },
    {
      "epoch": 1.797841726618705,
      "grad_norm": 9.647868156433105,
      "learning_rate": 4e-06,
      "loss": 0.5835,
      "step": 22491
    },
    {
      "epoch": 1.8019184652278177,
      "grad_norm": 15.158039093017578,
      "learning_rate": 4e-06,
      "loss": 0.5763,
      "step": 22542
    },
    {
      "epoch": 1.8059952038369305,
      "grad_norm": 14.650430679321289,
      "learning_rate": 4e-06,
      "loss": 0.5839,
      "step": 22593
    },
    {
      "epoch": 1.8100719424460432,
      "grad_norm": 11.754793167114258,
      "learning_rate": 4e-06,
      "loss": 0.6124,
      "step": 22644
    },
    {
      "epoch": 1.814148681055156,
      "grad_norm": 13.092693328857422,
      "learning_rate": 4e-06,
      "loss": 0.6034,
      "step": 22695
    },
    {
      "epoch": 1.8182254196642686,
      "grad_norm": 28.620927810668945,
      "learning_rate": 4e-06,
      "loss": 0.6053,
      "step": 22746
    },
    {
      "epoch": 1.8223021582733812,
      "grad_norm": 13.870695114135742,
      "learning_rate": 4e-06,
      "loss": 0.5875,
      "step": 22797
    },
    {
      "epoch": 1.826378896882494,
      "grad_norm": 8.752069473266602,
      "learning_rate": 4e-06,
      "loss": 0.5943,
      "step": 22848
    },
    {
      "epoch": 1.8304556354916066,
      "grad_norm": 10.07255744934082,
      "learning_rate": 4e-06,
      "loss": 0.5672,
      "step": 22899
    },
    {
      "epoch": 1.8345323741007196,
      "grad_norm": 14.99223804473877,
      "learning_rate": 4e-06,
      "loss": 0.5917,
      "step": 22950
    },
    {
      "epoch": 1.838609112709832,
      "grad_norm": 13.934625625610352,
      "learning_rate": 4e-06,
      "loss": 0.6265,
      "step": 23001
    },
    {
      "epoch": 1.8426858513189448,
      "grad_norm": 9.440780639648438,
      "learning_rate": 4e-06,
      "loss": 0.5636,
      "step": 23052
    },
    {
      "epoch": 1.8467625899280575,
      "grad_norm": 12.92880630493164,
      "learning_rate": 4e-06,
      "loss": 0.5828,
      "step": 23103
    },
    {
      "epoch": 1.8508393285371703,
      "grad_norm": 12.664255142211914,
      "learning_rate": 4e-06,
      "loss": 0.5888,
      "step": 23154
    },
    {
      "epoch": 1.854916067146283,
      "grad_norm": 11.93727970123291,
      "learning_rate": 4e-06,
      "loss": 0.5918,
      "step": 23205
    },
    {
      "epoch": 1.8589928057553957,
      "grad_norm": 13.895913124084473,
      "learning_rate": 4e-06,
      "loss": 0.5629,
      "step": 23256
    },
    {
      "epoch": 1.8630695443645084,
      "grad_norm": 12.741511344909668,
      "learning_rate": 4e-06,
      "loss": 0.583,
      "step": 23307
    },
    {
      "epoch": 1.867146282973621,
      "grad_norm": 8.630165100097656,
      "learning_rate": 4e-06,
      "loss": 0.5671,
      "step": 23358
    },
    {
      "epoch": 1.871223021582734,
      "grad_norm": 10.262612342834473,
      "learning_rate": 4e-06,
      "loss": 0.5918,
      "step": 23409
    },
    {
      "epoch": 1.8752997601918464,
      "grad_norm": 16.437620162963867,
      "learning_rate": 4e-06,
      "loss": 0.5458,
      "step": 23460
    },
    {
      "epoch": 1.8793764988009594,
      "grad_norm": 11.48611068725586,
      "learning_rate": 4e-06,
      "loss": 0.5661,
      "step": 23511
    },
    {
      "epoch": 1.8834532374100719,
      "grad_norm": 10.690231323242188,
      "learning_rate": 4e-06,
      "loss": 0.5495,
      "step": 23562
    },
    {
      "epoch": 1.8875299760191848,
      "grad_norm": 12.141891479492188,
      "learning_rate": 4e-06,
      "loss": 0.5771,
      "step": 23613
    },
    {
      "epoch": 1.8916067146282973,
      "grad_norm": 10.812232971191406,
      "learning_rate": 4e-06,
      "loss": 0.5748,
      "step": 23664
    },
    {
      "epoch": 1.89568345323741,
      "grad_norm": 9.086119651794434,
      "learning_rate": 4e-06,
      "loss": 0.6079,
      "step": 23715
    },
    {
      "epoch": 1.8997601918465228,
      "grad_norm": 12.804695129394531,
      "learning_rate": 4e-06,
      "loss": 0.559,
      "step": 23766
    },
    {
      "epoch": 1.9038369304556355,
      "grad_norm": 12.176484107971191,
      "learning_rate": 4e-06,
      "loss": 0.5814,
      "step": 23817
    },
    {
      "epoch": 1.9079136690647482,
      "grad_norm": 12.307960510253906,
      "learning_rate": 4e-06,
      "loss": 0.5767,
      "step": 23868
    },
    {
      "epoch": 1.9119904076738607,
      "grad_norm": 12.399637222290039,
      "learning_rate": 4e-06,
      "loss": 0.5805,
      "step": 23919
    },
    {
      "epoch": 1.9160671462829737,
      "grad_norm": 16.76950454711914,
      "learning_rate": 4e-06,
      "loss": 0.5907,
      "step": 23970
    },
    {
      "epoch": 1.9201438848920862,
      "grad_norm": 9.499781608581543,
      "learning_rate": 4e-06,
      "loss": 0.5943,
      "step": 24021
    },
    {
      "epoch": 1.9242206235011992,
      "grad_norm": 8.533117294311523,
      "learning_rate": 4e-06,
      "loss": 0.553,
      "step": 24072
    },
    {
      "epoch": 1.9282973621103117,
      "grad_norm": 11.960416793823242,
      "learning_rate": 4e-06,
      "loss": 0.586,
      "step": 24123
    },
    {
      "epoch": 1.9323741007194246,
      "grad_norm": 12.43326187133789,
      "learning_rate": 4e-06,
      "loss": 0.5801,
      "step": 24174
    },
    {
      "epoch": 1.9364508393285371,
      "grad_norm": 17.215476989746094,
      "learning_rate": 4e-06,
      "loss": 0.6004,
      "step": 24225
    },
    {
      "epoch": 1.9405275779376499,
      "grad_norm": 15.882363319396973,
      "learning_rate": 4e-06,
      "loss": 0.5531,
      "step": 24276
    },
    {
      "epoch": 1.9446043165467626,
      "grad_norm": 17.718008041381836,
      "learning_rate": 4e-06,
      "loss": 0.5904,
      "step": 24327
    },
    {
      "epoch": 1.9486810551558753,
      "grad_norm": 12.895169258117676,
      "learning_rate": 4e-06,
      "loss": 0.6071,
      "step": 24378
    },
    {
      "epoch": 1.952757793764988,
      "grad_norm": 12.70910930633545,
      "learning_rate": 4e-06,
      "loss": 0.5509,
      "step": 24429
    },
    {
      "epoch": 1.9568345323741008,
      "grad_norm": 9.80748176574707,
      "learning_rate": 4e-06,
      "loss": 0.5901,
      "step": 24480
    },
    {
      "epoch": 1.9609112709832135,
      "grad_norm": 8.481432914733887,
      "learning_rate": 4e-06,
      "loss": 0.5707,
      "step": 24531
    },
    {
      "epoch": 1.964988009592326,
      "grad_norm": 8.48207950592041,
      "learning_rate": 4e-06,
      "loss": 0.5318,
      "step": 24582
    },
    {
      "epoch": 1.969064748201439,
      "grad_norm": 9.77503490447998,
      "learning_rate": 4e-06,
      "loss": 0.5818,
      "step": 24633
    },
    {
      "epoch": 1.9731414868105515,
      "grad_norm": 13.138458251953125,
      "learning_rate": 4e-06,
      "loss": 0.5953,
      "step": 24684
    },
    {
      "epoch": 1.9772182254196644,
      "grad_norm": 13.943453788757324,
      "learning_rate": 4e-06,
      "loss": 0.5596,
      "step": 24735
    },
    {
      "epoch": 1.981294964028777,
      "grad_norm": 13.21434211730957,
      "learning_rate": 4e-06,
      "loss": 0.544,
      "step": 24786
    },
    {
      "epoch": 1.9853717026378896,
      "grad_norm": 12.74588394165039,
      "learning_rate": 4e-06,
      "loss": 0.5528,
      "step": 24837
    },
    {
      "epoch": 1.9894484412470024,
      "grad_norm": 10.731670379638672,
      "learning_rate": 4e-06,
      "loss": 0.5801,
      "step": 24888
    },
    {
      "epoch": 1.993525179856115,
      "grad_norm": 9.64101791381836,
      "learning_rate": 4e-06,
      "loss": 0.5835,
      "step": 24939
    },
    {
      "epoch": 1.9976019184652278,
      "grad_norm": 12.159348487854004,
      "learning_rate": 4e-06,
      "loss": 0.585,
      "step": 24990
    },
    {
      "epoch": 2.0016786570743403,
      "grad_norm": 8.424814224243164,
      "learning_rate": 4e-06,
      "loss": 0.5011,
      "step": 25041
    },
    {
      "epoch": 2.0057553956834533,
      "grad_norm": 10.85832405090332,
      "learning_rate": 4e-06,
      "loss": 0.4467,
      "step": 25092
    },
    {
      "epoch": 2.009832134292566,
      "grad_norm": 11.879873275756836,
      "learning_rate": 4e-06,
      "loss": 0.4424,
      "step": 25143
    },
    {
      "epoch": 2.0139088729016787,
      "grad_norm": 22.04907989501953,
      "learning_rate": 4e-06,
      "loss": 0.5308,
      "step": 25194
    },
    {
      "epoch": 2.0179856115107913,
      "grad_norm": 10.198373794555664,
      "learning_rate": 4e-06,
      "loss": 0.4574,
      "step": 25245
    },
    {
      "epoch": 2.022062350119904,
      "grad_norm": 14.756986618041992,
      "learning_rate": 4e-06,
      "loss": 0.452,
      "step": 25296
    },
    {
      "epoch": 2.0261390887290167,
      "grad_norm": 12.169490814208984,
      "learning_rate": 4e-06,
      "loss": 0.4549,
      "step": 25347
    },
    {
      "epoch": 2.0302158273381297,
      "grad_norm": 12.534502983093262,
      "learning_rate": 4e-06,
      "loss": 0.4623,
      "step": 25398
    },
    {
      "epoch": 2.034292565947242,
      "grad_norm": 14.03164291381836,
      "learning_rate": 4e-06,
      "loss": 0.4535,
      "step": 25449
    },
    {
      "epoch": 2.038369304556355,
      "grad_norm": 10.771193504333496,
      "learning_rate": 4e-06,
      "loss": 0.458,
      "step": 25500
    },
    {
      "epoch": 2.0424460431654676,
      "grad_norm": 11.14108657836914,
      "learning_rate": 4e-06,
      "loss": 0.4546,
      "step": 25551
    },
    {
      "epoch": 2.04652278177458,
      "grad_norm": 11.133054733276367,
      "learning_rate": 4e-06,
      "loss": 0.4477,
      "step": 25602
    },
    {
      "epoch": 2.050599520383693,
      "grad_norm": 9.624749183654785,
      "learning_rate": 4e-06,
      "loss": 0.4477,
      "step": 25653
    },
    {
      "epoch": 2.0546762589928056,
      "grad_norm": 9.972084999084473,
      "learning_rate": 4e-06,
      "loss": 0.4878,
      "step": 25704
    },
    {
      "epoch": 2.0587529976019185,
      "grad_norm": 17.873870849609375,
      "learning_rate": 4e-06,
      "loss": 0.4546,
      "step": 25755
    },
    {
      "epoch": 2.062829736211031,
      "grad_norm": 8.686954498291016,
      "learning_rate": 4e-06,
      "loss": 0.462,
      "step": 25806
    },
    {
      "epoch": 2.066906474820144,
      "grad_norm": 11.533903121948242,
      "learning_rate": 4e-06,
      "loss": 0.4415,
      "step": 25857
    },
    {
      "epoch": 2.0709832134292565,
      "grad_norm": 11.030670166015625,
      "learning_rate": 4e-06,
      "loss": 0.4547,
      "step": 25908
    },
    {
      "epoch": 2.0750599520383695,
      "grad_norm": 15.264395713806152,
      "learning_rate": 4e-06,
      "loss": 0.4498,
      "step": 25959
    },
    {
      "epoch": 2.079136690647482,
      "grad_norm": 11.015848159790039,
      "learning_rate": 4e-06,
      "loss": 0.4698,
      "step": 26010
    },
    {
      "epoch": 2.083213429256595,
      "grad_norm": 16.333967208862305,
      "learning_rate": 4e-06,
      "loss": 0.4606,
      "step": 26061
    },
    {
      "epoch": 2.0872901678657074,
      "grad_norm": 13.910479545593262,
      "learning_rate": 4e-06,
      "loss": 0.4782,
      "step": 26112
    },
    {
      "epoch": 2.09136690647482,
      "grad_norm": 9.210867881774902,
      "learning_rate": 4e-06,
      "loss": 0.4428,
      "step": 26163
    },
    {
      "epoch": 2.095443645083933,
      "grad_norm": 18.061975479125977,
      "learning_rate": 4e-06,
      "loss": 0.4526,
      "step": 26214
    },
    {
      "epoch": 2.0995203836930454,
      "grad_norm": 9.9329195022583,
      "learning_rate": 4e-06,
      "loss": 0.4452,
      "step": 26265
    },
    {
      "epoch": 2.1035971223021583,
      "grad_norm": 14.708541870117188,
      "learning_rate": 4e-06,
      "loss": 0.4708,
      "step": 26316
    },
    {
      "epoch": 2.107673860911271,
      "grad_norm": 10.317123413085938,
      "learning_rate": 4e-06,
      "loss": 0.4719,
      "step": 26367
    },
    {
      "epoch": 2.111750599520384,
      "grad_norm": 14.525203704833984,
      "learning_rate": 4e-06,
      "loss": 0.4581,
      "step": 26418
    },
    {
      "epoch": 2.1158273381294963,
      "grad_norm": 14.736098289489746,
      "learning_rate": 4e-06,
      "loss": 0.4482,
      "step": 26469
    },
    {
      "epoch": 2.1199040767386093,
      "grad_norm": 13.985912322998047,
      "learning_rate": 4e-06,
      "loss": 0.4529,
      "step": 26520
    },
    {
      "epoch": 2.1239808153477218,
      "grad_norm": 12.47414779663086,
      "learning_rate": 4e-06,
      "loss": 0.4449,
      "step": 26571
    },
    {
      "epoch": 2.1280575539568347,
      "grad_norm": 14.198772430419922,
      "learning_rate": 4e-06,
      "loss": 0.4435,
      "step": 26622
    },
    {
      "epoch": 2.132134292565947,
      "grad_norm": 14.823195457458496,
      "learning_rate": 4e-06,
      "loss": 0.4391,
      "step": 26673
    },
    {
      "epoch": 2.13621103117506,
      "grad_norm": 21.91176414489746,
      "learning_rate": 4e-06,
      "loss": 0.4647,
      "step": 26724
    },
    {
      "epoch": 2.1402877697841727,
      "grad_norm": 13.382187843322754,
      "learning_rate": 4e-06,
      "loss": 0.4599,
      "step": 26775
    },
    {
      "epoch": 2.144364508393285,
      "grad_norm": 11.034791946411133,
      "learning_rate": 4e-06,
      "loss": 0.4579,
      "step": 26826
    },
    {
      "epoch": 2.148441247002398,
      "grad_norm": 12.068931579589844,
      "learning_rate": 4e-06,
      "loss": 0.461,
      "step": 26877
    },
    {
      "epoch": 2.1525179856115106,
      "grad_norm": 8.893102645874023,
      "learning_rate": 4e-06,
      "loss": 0.454,
      "step": 26928
    },
    {
      "epoch": 2.1565947242206236,
      "grad_norm": 9.408421516418457,
      "learning_rate": 4e-06,
      "loss": 0.4477,
      "step": 26979
    },
    {
      "epoch": 2.160671462829736,
      "grad_norm": 23.24079132080078,
      "learning_rate": 4e-06,
      "loss": 0.4664,
      "step": 27030
    },
    {
      "epoch": 2.164748201438849,
      "grad_norm": 10.264227867126465,
      "learning_rate": 4e-06,
      "loss": 0.4472,
      "step": 27081
    },
    {
      "epoch": 2.1688249400479616,
      "grad_norm": 16.25213050842285,
      "learning_rate": 4e-06,
      "loss": 0.4648,
      "step": 27132
    },
    {
      "epoch": 2.1729016786570745,
      "grad_norm": 13.195564270019531,
      "learning_rate": 4e-06,
      "loss": 0.4821,
      "step": 27183
    },
    {
      "epoch": 2.176978417266187,
      "grad_norm": 13.052867889404297,
      "learning_rate": 4e-06,
      "loss": 0.4689,
      "step": 27234
    },
    {
      "epoch": 2.1810551558753,
      "grad_norm": 13.988957405090332,
      "learning_rate": 4e-06,
      "loss": 0.4393,
      "step": 27285
    },
    {
      "epoch": 2.1851318944844125,
      "grad_norm": 10.96356201171875,
      "learning_rate": 4e-06,
      "loss": 0.4353,
      "step": 27336
    },
    {
      "epoch": 2.189208633093525,
      "grad_norm": 8.7963285446167,
      "learning_rate": 4e-06,
      "loss": 0.448,
      "step": 27387
    },
    {
      "epoch": 2.193285371702638,
      "grad_norm": 13.102046966552734,
      "learning_rate": 4e-06,
      "loss": 0.4675,
      "step": 27438
    },
    {
      "epoch": 2.1973621103117504,
      "grad_norm": 15.037910461425781,
      "learning_rate": 4e-06,
      "loss": 0.4606,
      "step": 27489
    },
    {
      "epoch": 2.2014388489208634,
      "grad_norm": 14.596921920776367,
      "learning_rate": 4e-06,
      "loss": 0.4741,
      "step": 27540
    },
    {
      "epoch": 2.205515587529976,
      "grad_norm": 13.13980484008789,
      "learning_rate": 4e-06,
      "loss": 0.4554,
      "step": 27591
    },
    {
      "epoch": 2.209592326139089,
      "grad_norm": 22.32353401184082,
      "learning_rate": 4e-06,
      "loss": 0.4616,
      "step": 27642
    },
    {
      "epoch": 2.2136690647482014,
      "grad_norm": 10.891512870788574,
      "learning_rate": 4e-06,
      "loss": 0.4512,
      "step": 27693
    },
    {
      "epoch": 2.2177458033573143,
      "grad_norm": 8.737810134887695,
      "learning_rate": 4e-06,
      "loss": 0.4632,
      "step": 27744
    },
    {
      "epoch": 2.221822541966427,
      "grad_norm": 13.635896682739258,
      "learning_rate": 4e-06,
      "loss": 0.4778,
      "step": 27795
    },
    {
      "epoch": 2.2258992805755398,
      "grad_norm": 22.792301177978516,
      "learning_rate": 4e-06,
      "loss": 0.4709,
      "step": 27846
    },
    {
      "epoch": 2.2299760191846523,
      "grad_norm": 14.883259773254395,
      "learning_rate": 4e-06,
      "loss": 0.4715,
      "step": 27897
    },
    {
      "epoch": 2.2340527577937648,
      "grad_norm": 12.378639221191406,
      "learning_rate": 4e-06,
      "loss": 0.456,
      "step": 27948
    },
    {
      "epoch": 2.2381294964028777,
      "grad_norm": 8.521824836730957,
      "learning_rate": 4e-06,
      "loss": 0.4629,
      "step": 27999
    },
    {
      "epoch": 2.2422062350119902,
      "grad_norm": 9.912333488464355,
      "learning_rate": 4e-06,
      "loss": 0.4746,
      "step": 28050
    },
    {
      "epoch": 2.246282973621103,
      "grad_norm": 10.698668479919434,
      "learning_rate": 4e-06,
      "loss": 0.4477,
      "step": 28101
    },
    {
      "epoch": 2.2503597122302157,
      "grad_norm": 17.685829162597656,
      "learning_rate": 4e-06,
      "loss": 0.4506,
      "step": 28152
    },
    {
      "epoch": 2.2544364508393286,
      "grad_norm": 12.194108009338379,
      "learning_rate": 4e-06,
      "loss": 0.4729,
      "step": 28203
    },
    {
      "epoch": 2.258513189448441,
      "grad_norm": 12.304038047790527,
      "learning_rate": 4e-06,
      "loss": 0.4762,
      "step": 28254
    },
    {
      "epoch": 2.262589928057554,
      "grad_norm": 13.71133804321289,
      "learning_rate": 4e-06,
      "loss": 0.4659,
      "step": 28305
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 9.620288848876953,
      "learning_rate": 4e-06,
      "loss": 0.4444,
      "step": 28356
    },
    {
      "epoch": 2.2707434052757796,
      "grad_norm": 11.439412117004395,
      "learning_rate": 4e-06,
      "loss": 0.4701,
      "step": 28407
    },
    {
      "epoch": 2.274820143884892,
      "grad_norm": 13.923293113708496,
      "learning_rate": 4e-06,
      "loss": 0.447,
      "step": 28458
    },
    {
      "epoch": 2.2788968824940046,
      "grad_norm": 12.300345420837402,
      "learning_rate": 4e-06,
      "loss": 0.4763,
      "step": 28509
    },
    {
      "epoch": 2.2829736211031175,
      "grad_norm": 11.21446418762207,
      "learning_rate": 4e-06,
      "loss": 0.4627,
      "step": 28560
    },
    {
      "epoch": 2.28705035971223,
      "grad_norm": 11.700380325317383,
      "learning_rate": 4e-06,
      "loss": 0.4433,
      "step": 28611
    },
    {
      "epoch": 2.291127098321343,
      "grad_norm": 10.343286514282227,
      "learning_rate": 4e-06,
      "loss": 0.4705,
      "step": 28662
    },
    {
      "epoch": 2.2952038369304555,
      "grad_norm": 10.525687217712402,
      "learning_rate": 4e-06,
      "loss": 0.4753,
      "step": 28713
    },
    {
      "epoch": 2.2992805755395684,
      "grad_norm": 9.505599975585938,
      "learning_rate": 4e-06,
      "loss": 0.4732,
      "step": 28764
    },
    {
      "epoch": 2.303357314148681,
      "grad_norm": 12.202049255371094,
      "learning_rate": 4e-06,
      "loss": 0.4717,
      "step": 28815
    },
    {
      "epoch": 2.307434052757794,
      "grad_norm": 8.376129150390625,
      "learning_rate": 4e-06,
      "loss": 0.4527,
      "step": 28866
    },
    {
      "epoch": 2.3115107913669064,
      "grad_norm": 11.057503700256348,
      "learning_rate": 4e-06,
      "loss": 0.4743,
      "step": 28917
    },
    {
      "epoch": 2.3155875299760194,
      "grad_norm": 9.12792682647705,
      "learning_rate": 4e-06,
      "loss": 0.478,
      "step": 28968
    },
    {
      "epoch": 2.319664268585132,
      "grad_norm": 26.0190372467041,
      "learning_rate": 4e-06,
      "loss": 0.4745,
      "step": 29019
    },
    {
      "epoch": 2.3237410071942444,
      "grad_norm": 9.50634479522705,
      "learning_rate": 4e-06,
      "loss": 0.4738,
      "step": 29070
    },
    {
      "epoch": 2.3278177458033573,
      "grad_norm": 12.048861503601074,
      "learning_rate": 4e-06,
      "loss": 0.4467,
      "step": 29121
    },
    {
      "epoch": 2.3318944844124703,
      "grad_norm": 10.667549133300781,
      "learning_rate": 4e-06,
      "loss": 0.4685,
      "step": 29172
    },
    {
      "epoch": 2.3359712230215828,
      "grad_norm": 10.96118450164795,
      "learning_rate": 4e-06,
      "loss": 0.4567,
      "step": 29223
    },
    {
      "epoch": 2.3400479616306953,
      "grad_norm": 12.190624237060547,
      "learning_rate": 4e-06,
      "loss": 0.4631,
      "step": 29274
    },
    {
      "epoch": 2.3441247002398082,
      "grad_norm": 9.092680931091309,
      "learning_rate": 4e-06,
      "loss": 0.4676,
      "step": 29325
    },
    {
      "epoch": 2.3482014388489207,
      "grad_norm": 8.926836013793945,
      "learning_rate": 4e-06,
      "loss": 0.4842,
      "step": 29376
    },
    {
      "epoch": 2.3522781774580337,
      "grad_norm": 15.439827919006348,
      "learning_rate": 4e-06,
      "loss": 0.4665,
      "step": 29427
    },
    {
      "epoch": 2.356354916067146,
      "grad_norm": 14.458600997924805,
      "learning_rate": 4e-06,
      "loss": 0.4671,
      "step": 29478
    },
    {
      "epoch": 2.360431654676259,
      "grad_norm": 7.673156261444092,
      "learning_rate": 4e-06,
      "loss": 0.4784,
      "step": 29529
    },
    {
      "epoch": 2.3645083932853717,
      "grad_norm": 12.269221305847168,
      "learning_rate": 4e-06,
      "loss": 0.4605,
      "step": 29580
    },
    {
      "epoch": 2.368585131894484,
      "grad_norm": 10.393404006958008,
      "learning_rate": 4e-06,
      "loss": 0.4493,
      "step": 29631
    },
    {
      "epoch": 2.372661870503597,
      "grad_norm": 12.109273910522461,
      "learning_rate": 4e-06,
      "loss": 0.4822,
      "step": 29682
    },
    {
      "epoch": 2.37673860911271,
      "grad_norm": 13.180609703063965,
      "learning_rate": 4e-06,
      "loss": 0.4622,
      "step": 29733
    },
    {
      "epoch": 2.3808153477218226,
      "grad_norm": 10.510076522827148,
      "learning_rate": 4e-06,
      "loss": 0.4543,
      "step": 29784
    },
    {
      "epoch": 2.384892086330935,
      "grad_norm": 11.404170989990234,
      "learning_rate": 4e-06,
      "loss": 0.4606,
      "step": 29835
    },
    {
      "epoch": 2.388968824940048,
      "grad_norm": 9.548449516296387,
      "learning_rate": 4e-06,
      "loss": 0.4812,
      "step": 29886
    },
    {
      "epoch": 2.3930455635491605,
      "grad_norm": 7.8412885665893555,
      "learning_rate": 4e-06,
      "loss": 0.4593,
      "step": 29937
    },
    {
      "epoch": 2.3971223021582735,
      "grad_norm": 8.529440879821777,
      "learning_rate": 4e-06,
      "loss": 0.4892,
      "step": 29988
    },
    {
      "epoch": 2.401199040767386,
      "grad_norm": 9.321039199829102,
      "learning_rate": 4e-06,
      "loss": 0.4653,
      "step": 30039
    },
    {
      "epoch": 2.405275779376499,
      "grad_norm": 10.242959976196289,
      "learning_rate": 4e-06,
      "loss": 0.4746,
      "step": 30090
    },
    {
      "epoch": 2.4093525179856115,
      "grad_norm": 11.728069305419922,
      "learning_rate": 4e-06,
      "loss": 0.4672,
      "step": 30141
    },
    {
      "epoch": 2.4134292565947244,
      "grad_norm": 10.197078704833984,
      "learning_rate": 4e-06,
      "loss": 0.4848,
      "step": 30192
    },
    {
      "epoch": 2.417505995203837,
      "grad_norm": 12.632417678833008,
      "learning_rate": 4e-06,
      "loss": 0.4763,
      "step": 30243
    },
    {
      "epoch": 2.42158273381295,
      "grad_norm": 11.686958312988281,
      "learning_rate": 4e-06,
      "loss": 0.4715,
      "step": 30294
    },
    {
      "epoch": 2.4256594724220624,
      "grad_norm": 6.103004455566406,
      "learning_rate": 4e-06,
      "loss": 0.4597,
      "step": 30345
    },
    {
      "epoch": 2.429736211031175,
      "grad_norm": 12.037240028381348,
      "learning_rate": 4e-06,
      "loss": 0.4719,
      "step": 30396
    },
    {
      "epoch": 2.433812949640288,
      "grad_norm": 11.227663040161133,
      "learning_rate": 4e-06,
      "loss": 0.4433,
      "step": 30447
    },
    {
      "epoch": 2.4378896882494003,
      "grad_norm": 11.460360527038574,
      "learning_rate": 4e-06,
      "loss": 0.4549,
      "step": 30498
    },
    {
      "epoch": 2.4419664268585133,
      "grad_norm": 15.712830543518066,
      "learning_rate": 4e-06,
      "loss": 0.4602,
      "step": 30549
    },
    {
      "epoch": 2.446043165467626,
      "grad_norm": 9.378046035766602,
      "learning_rate": 4e-06,
      "loss": 0.4706,
      "step": 30600
    },
    {
      "epoch": 2.4501199040767387,
      "grad_norm": 18.471338272094727,
      "learning_rate": 4e-06,
      "loss": 0.4548,
      "step": 30651
    },
    {
      "epoch": 2.4541966426858512,
      "grad_norm": 12.491426467895508,
      "learning_rate": 4e-06,
      "loss": 0.4651,
      "step": 30702
    },
    {
      "epoch": 2.458273381294964,
      "grad_norm": 14.841476440429688,
      "learning_rate": 4e-06,
      "loss": 0.4806,
      "step": 30753
    },
    {
      "epoch": 2.4623501199040767,
      "grad_norm": 16.399370193481445,
      "learning_rate": 4e-06,
      "loss": 0.4565,
      "step": 30804
    },
    {
      "epoch": 2.4664268585131897,
      "grad_norm": 8.741857528686523,
      "learning_rate": 4e-06,
      "loss": 0.4653,
      "step": 30855
    },
    {
      "epoch": 2.470503597122302,
      "grad_norm": 10.050724029541016,
      "learning_rate": 4e-06,
      "loss": 0.4753,
      "step": 30906
    },
    {
      "epoch": 2.4745803357314147,
      "grad_norm": 12.965930938720703,
      "learning_rate": 4e-06,
      "loss": 0.4617,
      "step": 30957
    },
    {
      "epoch": 2.4786570743405276,
      "grad_norm": 10.470799446105957,
      "learning_rate": 4e-06,
      "loss": 0.479,
      "step": 31008
    },
    {
      "epoch": 2.48273381294964,
      "grad_norm": 10.020484924316406,
      "learning_rate": 4e-06,
      "loss": 0.4524,
      "step": 31059
    },
    {
      "epoch": 2.486810551558753,
      "grad_norm": 16.008569717407227,
      "learning_rate": 4e-06,
      "loss": 0.4655,
      "step": 31110
    },
    {
      "epoch": 2.4908872901678656,
      "grad_norm": 9.86565113067627,
      "learning_rate": 4e-06,
      "loss": 0.4633,
      "step": 31161
    },
    {
      "epoch": 2.4949640287769785,
      "grad_norm": 7.930871486663818,
      "learning_rate": 4e-06,
      "loss": 0.4721,
      "step": 31212
    },
    {
      "epoch": 2.499040767386091,
      "grad_norm": 8.779703140258789,
      "learning_rate": 4e-06,
      "loss": 0.4567,
      "step": 31263
    },
    {
      "epoch": 2.503117505995204,
      "grad_norm": 15.015779495239258,
      "learning_rate": 4e-06,
      "loss": 0.4835,
      "step": 31314
    },
    {
      "epoch": 2.5071942446043165,
      "grad_norm": 10.901209831237793,
      "learning_rate": 4e-06,
      "loss": 0.4623,
      "step": 31365
    },
    {
      "epoch": 2.5112709832134295,
      "grad_norm": 9.622361183166504,
      "learning_rate": 4e-06,
      "loss": 0.4844,
      "step": 31416
    },
    {
      "epoch": 2.515347721822542,
      "grad_norm": 10.299530029296875,
      "learning_rate": 4e-06,
      "loss": 0.467,
      "step": 31467
    },
    {
      "epoch": 2.5194244604316545,
      "grad_norm": 8.35836124420166,
      "learning_rate": 4e-06,
      "loss": 0.4611,
      "step": 31518
    },
    {
      "epoch": 2.5235011990407674,
      "grad_norm": 11.007613182067871,
      "learning_rate": 4e-06,
      "loss": 0.4756,
      "step": 31569
    },
    {
      "epoch": 2.5275779376498804,
      "grad_norm": 12.243165016174316,
      "learning_rate": 4e-06,
      "loss": 0.4668,
      "step": 31620
    },
    {
      "epoch": 2.531654676258993,
      "grad_norm": 14.739203453063965,
      "learning_rate": 4e-06,
      "loss": 0.4807,
      "step": 31671
    },
    {
      "epoch": 2.5357314148681054,
      "grad_norm": 8.78465747833252,
      "learning_rate": 4e-06,
      "loss": 0.4651,
      "step": 31722
    },
    {
      "epoch": 2.5398081534772183,
      "grad_norm": 14.73403549194336,
      "learning_rate": 4e-06,
      "loss": 0.4586,
      "step": 31773
    },
    {
      "epoch": 2.543884892086331,
      "grad_norm": 12.012397766113281,
      "learning_rate": 4e-06,
      "loss": 0.4637,
      "step": 31824
    },
    {
      "epoch": 2.547961630695444,
      "grad_norm": 25.679576873779297,
      "learning_rate": 4e-06,
      "loss": 0.4936,
      "step": 31875
    },
    {
      "epoch": 2.5520383693045563,
      "grad_norm": 15.337760925292969,
      "learning_rate": 4e-06,
      "loss": 0.4558,
      "step": 31926
    },
    {
      "epoch": 2.5561151079136692,
      "grad_norm": 13.014193534851074,
      "learning_rate": 4e-06,
      "loss": 0.4597,
      "step": 31977
    },
    {
      "epoch": 2.5601918465227818,
      "grad_norm": 11.357173919677734,
      "learning_rate": 4e-06,
      "loss": 0.4723,
      "step": 32028
    },
    {
      "epoch": 2.5642685851318943,
      "grad_norm": 21.30634117126465,
      "learning_rate": 4e-06,
      "loss": 0.467,
      "step": 32079
    },
    {
      "epoch": 2.568345323741007,
      "grad_norm": 8.622227668762207,
      "learning_rate": 4e-06,
      "loss": 0.4691,
      "step": 32130
    },
    {
      "epoch": 2.57242206235012,
      "grad_norm": 10.560222625732422,
      "learning_rate": 4e-06,
      "loss": 0.4963,
      "step": 32181
    },
    {
      "epoch": 2.5764988009592327,
      "grad_norm": 11.128494262695312,
      "learning_rate": 4e-06,
      "loss": 0.4618,
      "step": 32232
    },
    {
      "epoch": 2.580575539568345,
      "grad_norm": 16.98004150390625,
      "learning_rate": 4e-06,
      "loss": 0.4805,
      "step": 32283
    },
    {
      "epoch": 2.584652278177458,
      "grad_norm": 13.114840507507324,
      "learning_rate": 4e-06,
      "loss": 0.4714,
      "step": 32334
    },
    {
      "epoch": 2.5887290167865706,
      "grad_norm": 10.882728576660156,
      "learning_rate": 4e-06,
      "loss": 0.4749,
      "step": 32385
    },
    {
      "epoch": 2.5928057553956836,
      "grad_norm": 8.781963348388672,
      "learning_rate": 4e-06,
      "loss": 0.451,
      "step": 32436
    },
    {
      "epoch": 2.596882494004796,
      "grad_norm": 13.544221878051758,
      "learning_rate": 4e-06,
      "loss": 0.464,
      "step": 32487
    },
    {
      "epoch": 2.600959232613909,
      "grad_norm": 11.379178047180176,
      "learning_rate": 4e-06,
      "loss": 0.4451,
      "step": 32538
    },
    {
      "epoch": 2.6050359712230216,
      "grad_norm": 10.529903411865234,
      "learning_rate": 4e-06,
      "loss": 0.4608,
      "step": 32589
    },
    {
      "epoch": 2.609112709832134,
      "grad_norm": 8.913954734802246,
      "learning_rate": 4e-06,
      "loss": 0.4931,
      "step": 32640
    },
    {
      "epoch": 2.613189448441247,
      "grad_norm": 10.0164794921875,
      "learning_rate": 4e-06,
      "loss": 0.4734,
      "step": 32691
    },
    {
      "epoch": 2.61726618705036,
      "grad_norm": 15.075758934020996,
      "learning_rate": 4e-06,
      "loss": 0.4538,
      "step": 32742
    },
    {
      "epoch": 2.6213429256594725,
      "grad_norm": 10.992897987365723,
      "learning_rate": 4e-06,
      "loss": 0.4881,
      "step": 32793
    },
    {
      "epoch": 2.625419664268585,
      "grad_norm": 19.719593048095703,
      "learning_rate": 4e-06,
      "loss": 0.4725,
      "step": 32844
    },
    {
      "epoch": 2.629496402877698,
      "grad_norm": 11.458284378051758,
      "learning_rate": 4e-06,
      "loss": 0.4739,
      "step": 32895
    },
    {
      "epoch": 2.6335731414868104,
      "grad_norm": 8.746288299560547,
      "learning_rate": 4e-06,
      "loss": 0.4625,
      "step": 32946
    },
    {
      "epoch": 2.6376498800959234,
      "grad_norm": 14.31275463104248,
      "learning_rate": 4e-06,
      "loss": 0.4723,
      "step": 32997
    },
    {
      "epoch": 2.641726618705036,
      "grad_norm": 15.805724143981934,
      "learning_rate": 4e-06,
      "loss": 0.4801,
      "step": 33048
    },
    {
      "epoch": 2.645803357314149,
      "grad_norm": 8.384048461914062,
      "learning_rate": 4e-06,
      "loss": 0.4849,
      "step": 33099
    },
    {
      "epoch": 2.6498800959232613,
      "grad_norm": 11.978403091430664,
      "learning_rate": 4e-06,
      "loss": 0.4617,
      "step": 33150
    },
    {
      "epoch": 2.653956834532374,
      "grad_norm": 10.9121675491333,
      "learning_rate": 4e-06,
      "loss": 0.4724,
      "step": 33201
    },
    {
      "epoch": 2.658033573141487,
      "grad_norm": 8.296579360961914,
      "learning_rate": 4e-06,
      "loss": 0.4582,
      "step": 33252
    },
    {
      "epoch": 2.6621103117505998,
      "grad_norm": 9.88510799407959,
      "learning_rate": 4e-06,
      "loss": 0.4644,
      "step": 33303
    },
    {
      "epoch": 2.6661870503597123,
      "grad_norm": 9.767138481140137,
      "learning_rate": 4e-06,
      "loss": 0.4607,
      "step": 33354
    },
    {
      "epoch": 2.6702637889688248,
      "grad_norm": 16.159217834472656,
      "learning_rate": 4e-06,
      "loss": 0.4643,
      "step": 33405
    },
    {
      "epoch": 2.6743405275779377,
      "grad_norm": 9.149335861206055,
      "learning_rate": 4e-06,
      "loss": 0.4657,
      "step": 33456
    },
    {
      "epoch": 2.6784172661870502,
      "grad_norm": 11.935235977172852,
      "learning_rate": 4e-06,
      "loss": 0.4945,
      "step": 33507
    },
    {
      "epoch": 2.682494004796163,
      "grad_norm": 10.448596000671387,
      "learning_rate": 4e-06,
      "loss": 0.4824,
      "step": 33558
    },
    {
      "epoch": 2.6865707434052757,
      "grad_norm": 11.992875099182129,
      "learning_rate": 4e-06,
      "loss": 0.4776,
      "step": 33609
    },
    {
      "epoch": 2.6906474820143886,
      "grad_norm": 6.885528087615967,
      "learning_rate": 4e-06,
      "loss": 0.4771,
      "step": 33660
    },
    {
      "epoch": 2.694724220623501,
      "grad_norm": 24.338687896728516,
      "learning_rate": 4e-06,
      "loss": 0.49,
      "step": 33711
    },
    {
      "epoch": 2.6988009592326136,
      "grad_norm": 13.748376846313477,
      "learning_rate": 4e-06,
      "loss": 0.4645,
      "step": 33762
    },
    {
      "epoch": 2.7028776978417266,
      "grad_norm": 10.0679931640625,
      "learning_rate": 4e-06,
      "loss": 0.4571,
      "step": 33813
    },
    {
      "epoch": 2.7069544364508396,
      "grad_norm": 8.902748107910156,
      "learning_rate": 4e-06,
      "loss": 0.4747,
      "step": 33864
    },
    {
      "epoch": 2.711031175059952,
      "grad_norm": 10.768905639648438,
      "learning_rate": 4e-06,
      "loss": 0.456,
      "step": 33915
    },
    {
      "epoch": 2.7151079136690646,
      "grad_norm": 8.285409927368164,
      "learning_rate": 4e-06,
      "loss": 0.476,
      "step": 33966
    },
    {
      "epoch": 2.7191846522781775,
      "grad_norm": 11.123775482177734,
      "learning_rate": 4e-06,
      "loss": 0.4648,
      "step": 34017
    },
    {
      "epoch": 2.72326139088729,
      "grad_norm": 12.420106887817383,
      "learning_rate": 4e-06,
      "loss": 0.4938,
      "step": 34068
    },
    {
      "epoch": 2.727338129496403,
      "grad_norm": 15.784192085266113,
      "learning_rate": 4e-06,
      "loss": 0.4397,
      "step": 34119
    },
    {
      "epoch": 2.7314148681055155,
      "grad_norm": 9.614347457885742,
      "learning_rate": 4e-06,
      "loss": 0.4632,
      "step": 34170
    },
    {
      "epoch": 2.7354916067146284,
      "grad_norm": 12.85510540008545,
      "learning_rate": 4e-06,
      "loss": 0.4835,
      "step": 34221
    },
    {
      "epoch": 2.739568345323741,
      "grad_norm": 10.931224822998047,
      "learning_rate": 4e-06,
      "loss": 0.4602,
      "step": 34272
    },
    {
      "epoch": 2.7436450839328534,
      "grad_norm": 11.496292114257812,
      "learning_rate": 4e-06,
      "loss": 0.4782,
      "step": 34323
    },
    {
      "epoch": 2.7477218225419664,
      "grad_norm": 11.47299575805664,
      "learning_rate": 4e-06,
      "loss": 0.4694,
      "step": 34374
    },
    {
      "epoch": 2.7517985611510793,
      "grad_norm": 13.384010314941406,
      "learning_rate": 4e-06,
      "loss": 0.4873,
      "step": 34425
    },
    {
      "epoch": 2.755875299760192,
      "grad_norm": 12.45289134979248,
      "learning_rate": 4e-06,
      "loss": 0.4543,
      "step": 34476
    },
    {
      "epoch": 2.7599520383693044,
      "grad_norm": 42.37575912475586,
      "learning_rate": 4e-06,
      "loss": 0.4716,
      "step": 34527
    },
    {
      "epoch": 2.7640287769784173,
      "grad_norm": 13.265506744384766,
      "learning_rate": 4e-06,
      "loss": 0.4676,
      "step": 34578
    },
    {
      "epoch": 2.76810551558753,
      "grad_norm": 18.71015739440918,
      "learning_rate": 4e-06,
      "loss": 0.4714,
      "step": 34629
    },
    {
      "epoch": 2.7721822541966428,
      "grad_norm": 12.402403831481934,
      "learning_rate": 4e-06,
      "loss": 0.4719,
      "step": 34680
    },
    {
      "epoch": 2.7762589928057553,
      "grad_norm": 8.471380233764648,
      "learning_rate": 4e-06,
      "loss": 0.4745,
      "step": 34731
    },
    {
      "epoch": 2.7803357314148682,
      "grad_norm": 13.31091022491455,
      "learning_rate": 4e-06,
      "loss": 0.4613,
      "step": 34782
    },
    {
      "epoch": 2.7844124700239807,
      "grad_norm": 9.974373817443848,
      "learning_rate": 4e-06,
      "loss": 0.4559,
      "step": 34833
    },
    {
      "epoch": 2.7884892086330937,
      "grad_norm": 16.678722381591797,
      "learning_rate": 4e-06,
      "loss": 0.4672,
      "step": 34884
    },
    {
      "epoch": 2.792565947242206,
      "grad_norm": 11.586806297302246,
      "learning_rate": 4e-06,
      "loss": 0.4675,
      "step": 34935
    },
    {
      "epoch": 2.796642685851319,
      "grad_norm": 12.085662841796875,
      "learning_rate": 4e-06,
      "loss": 0.4676,
      "step": 34986
    },
    {
      "epoch": 2.8007194244604317,
      "grad_norm": 29.032089233398438,
      "learning_rate": 4e-06,
      "loss": 0.4726,
      "step": 35037
    },
    {
      "epoch": 2.804796163069544,
      "grad_norm": 14.102984428405762,
      "learning_rate": 4e-06,
      "loss": 0.4774,
      "step": 35088
    },
    {
      "epoch": 2.808872901678657,
      "grad_norm": 17.82986831665039,
      "learning_rate": 4e-06,
      "loss": 0.4466,
      "step": 35139
    },
    {
      "epoch": 2.81294964028777,
      "grad_norm": 11.135865211486816,
      "learning_rate": 4e-06,
      "loss": 0.4809,
      "step": 35190
    },
    {
      "epoch": 2.8170263788968826,
      "grad_norm": 15.383672714233398,
      "learning_rate": 4e-06,
      "loss": 0.4831,
      "step": 35241
    },
    {
      "epoch": 2.821103117505995,
      "grad_norm": 10.860206604003906,
      "learning_rate": 4e-06,
      "loss": 0.459,
      "step": 35292
    },
    {
      "epoch": 2.825179856115108,
      "grad_norm": 8.111364364624023,
      "learning_rate": 4e-06,
      "loss": 0.4716,
      "step": 35343
    },
    {
      "epoch": 2.8292565947242205,
      "grad_norm": 12.058816909790039,
      "learning_rate": 4e-06,
      "loss": 0.463,
      "step": 35394
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 10.085551261901855,
      "learning_rate": 4e-06,
      "loss": 0.4741,
      "step": 35445
    },
    {
      "epoch": 2.837410071942446,
      "grad_norm": 11.311388969421387,
      "learning_rate": 4e-06,
      "loss": 0.4733,
      "step": 35496
    },
    {
      "epoch": 2.841486810551559,
      "grad_norm": 16.458454132080078,
      "learning_rate": 4e-06,
      "loss": 0.458,
      "step": 35547
    },
    {
      "epoch": 2.8455635491606714,
      "grad_norm": 11.483838081359863,
      "learning_rate": 4e-06,
      "loss": 0.4559,
      "step": 35598
    },
    {
      "epoch": 2.849640287769784,
      "grad_norm": 10.994821548461914,
      "learning_rate": 4e-06,
      "loss": 0.4674,
      "step": 35649
    },
    {
      "epoch": 2.853717026378897,
      "grad_norm": 10.03485107421875,
      "learning_rate": 4e-06,
      "loss": 0.4698,
      "step": 35700
    },
    {
      "epoch": 2.85779376498801,
      "grad_norm": 13.947110176086426,
      "learning_rate": 4e-06,
      "loss": 0.4538,
      "step": 35751
    },
    {
      "epoch": 2.8618705035971224,
      "grad_norm": 10.199928283691406,
      "learning_rate": 4e-06,
      "loss": 0.4602,
      "step": 35802
    },
    {
      "epoch": 2.865947242206235,
      "grad_norm": 11.407940864562988,
      "learning_rate": 4e-06,
      "loss": 0.4615,
      "step": 35853
    },
    {
      "epoch": 2.870023980815348,
      "grad_norm": 9.224793434143066,
      "learning_rate": 4e-06,
      "loss": 0.4673,
      "step": 35904
    },
    {
      "epoch": 2.8741007194244603,
      "grad_norm": 10.545677185058594,
      "learning_rate": 4e-06,
      "loss": 0.4525,
      "step": 35955
    },
    {
      "epoch": 2.8781774580335733,
      "grad_norm": 7.5771355628967285,
      "learning_rate": 4e-06,
      "loss": 0.4548,
      "step": 36006
    },
    {
      "epoch": 2.882254196642686,
      "grad_norm": 11.455767631530762,
      "learning_rate": 4e-06,
      "loss": 0.4548,
      "step": 36057
    },
    {
      "epoch": 2.8863309352517987,
      "grad_norm": 11.77834415435791,
      "learning_rate": 4e-06,
      "loss": 0.4622,
      "step": 36108
    },
    {
      "epoch": 2.8904076738609112,
      "grad_norm": 9.200343132019043,
      "learning_rate": 4e-06,
      "loss": 0.4773,
      "step": 36159
    },
    {
      "epoch": 2.8944844124700237,
      "grad_norm": 9.010701179504395,
      "learning_rate": 4e-06,
      "loss": 0.4595,
      "step": 36210
    },
    {
      "epoch": 2.8985611510791367,
      "grad_norm": 7.907824993133545,
      "learning_rate": 4e-06,
      "loss": 0.4606,
      "step": 36261
    },
    {
      "epoch": 2.9026378896882497,
      "grad_norm": 9.761000633239746,
      "learning_rate": 4e-06,
      "loss": 0.445,
      "step": 36312
    },
    {
      "epoch": 2.906714628297362,
      "grad_norm": 9.05679988861084,
      "learning_rate": 4e-06,
      "loss": 0.4685,
      "step": 36363
    },
    {
      "epoch": 2.9107913669064747,
      "grad_norm": 9.29472827911377,
      "learning_rate": 4e-06,
      "loss": 0.465,
      "step": 36414
    },
    {
      "epoch": 2.9148681055155876,
      "grad_norm": 13.253036499023438,
      "learning_rate": 4e-06,
      "loss": 0.4726,
      "step": 36465
    },
    {
      "epoch": 2.9189448441247,
      "grad_norm": 12.360503196716309,
      "learning_rate": 4e-06,
      "loss": 0.4672,
      "step": 36516
    },
    {
      "epoch": 2.923021582733813,
      "grad_norm": 11.908672332763672,
      "learning_rate": 4e-06,
      "loss": 0.4589,
      "step": 36567
    },
    {
      "epoch": 2.9270983213429256,
      "grad_norm": 11.733437538146973,
      "learning_rate": 4e-06,
      "loss": 0.4471,
      "step": 36618
    },
    {
      "epoch": 2.9311750599520385,
      "grad_norm": 9.366621971130371,
      "learning_rate": 4e-06,
      "loss": 0.4545,
      "step": 36669
    },
    {
      "epoch": 2.935251798561151,
      "grad_norm": 9.884160041809082,
      "learning_rate": 4e-06,
      "loss": 0.4697,
      "step": 36720
    },
    {
      "epoch": 2.9393285371702635,
      "grad_norm": 18.093477249145508,
      "learning_rate": 4e-06,
      "loss": 0.4598,
      "step": 36771
    },
    {
      "epoch": 2.9434052757793765,
      "grad_norm": 20.85175895690918,
      "learning_rate": 4e-06,
      "loss": 0.492,
      "step": 36822
    },
    {
      "epoch": 2.9474820143884894,
      "grad_norm": 13.004422187805176,
      "learning_rate": 4e-06,
      "loss": 0.4675,
      "step": 36873
    },
    {
      "epoch": 2.951558752997602,
      "grad_norm": 12.768797874450684,
      "learning_rate": 4e-06,
      "loss": 0.4758,
      "step": 36924
    },
    {
      "epoch": 2.9556354916067145,
      "grad_norm": 9.752819061279297,
      "learning_rate": 4e-06,
      "loss": 0.4604,
      "step": 36975
    },
    {
      "epoch": 2.9597122302158274,
      "grad_norm": 8.113765716552734,
      "learning_rate": 4e-06,
      "loss": 0.4638,
      "step": 37026
    },
    {
      "epoch": 2.96378896882494,
      "grad_norm": 8.305475234985352,
      "learning_rate": 4e-06,
      "loss": 0.4431,
      "step": 37077
    },
    {
      "epoch": 2.967865707434053,
      "grad_norm": 10.236854553222656,
      "learning_rate": 4e-06,
      "loss": 0.4739,
      "step": 37128
    },
    {
      "epoch": 2.9719424460431654,
      "grad_norm": 10.106242179870605,
      "learning_rate": 4e-06,
      "loss": 0.4514,
      "step": 37179
    },
    {
      "epoch": 2.9760191846522783,
      "grad_norm": 10.520194053649902,
      "learning_rate": 4e-06,
      "loss": 0.494,
      "step": 37230
    },
    {
      "epoch": 2.980095923261391,
      "grad_norm": 10.557272911071777,
      "learning_rate": 4e-06,
      "loss": 0.4678,
      "step": 37281
    },
    {
      "epoch": 2.9841726618705033,
      "grad_norm": 10.108020782470703,
      "learning_rate": 4e-06,
      "loss": 0.4738,
      "step": 37332
    },
    {
      "epoch": 2.9882494004796163,
      "grad_norm": 8.449090003967285,
      "learning_rate": 4e-06,
      "loss": 0.4535,
      "step": 37383
    },
    {
      "epoch": 2.9923261390887292,
      "grad_norm": 10.329444885253906,
      "learning_rate": 4e-06,
      "loss": 0.4731,
      "step": 37434
    },
    {
      "epoch": 2.9964028776978417,
      "grad_norm": 10.781861305236816,
      "learning_rate": 4e-06,
      "loss": 0.48,
      "step": 37485
    },
    {
      "epoch": 3.0004796163069543,
      "grad_norm": 9.001248359680176,
      "learning_rate": 4e-06,
      "loss": 0.4759,
      "step": 37536
    },
    {
      "epoch": 3.004556354916067,
      "grad_norm": 11.152580261230469,
      "learning_rate": 4e-06,
      "loss": 0.3658,
      "step": 37587
    },
    {
      "epoch": 3.0086330935251797,
      "grad_norm": 12.5055570602417,
      "learning_rate": 4e-06,
      "loss": 0.361,
      "step": 37638
    },
    {
      "epoch": 3.0127098321342927,
      "grad_norm": 20.414630889892578,
      "learning_rate": 4e-06,
      "loss": 0.3526,
      "step": 37689
    },
    {
      "epoch": 3.016786570743405,
      "grad_norm": 8.495919227600098,
      "learning_rate": 4e-06,
      "loss": 0.3578,
      "step": 37740
    },
    {
      "epoch": 3.020863309352518,
      "grad_norm": 9.792919158935547,
      "learning_rate": 4e-06,
      "loss": 0.3512,
      "step": 37791
    },
    {
      "epoch": 3.0249400479616306,
      "grad_norm": 10.014331817626953,
      "learning_rate": 4e-06,
      "loss": 0.3599,
      "step": 37842
    },
    {
      "epoch": 3.0290167865707436,
      "grad_norm": 7.791833877563477,
      "learning_rate": 4e-06,
      "loss": 0.3539,
      "step": 37893
    },
    {
      "epoch": 3.033093525179856,
      "grad_norm": 11.20398235321045,
      "learning_rate": 4e-06,
      "loss": 0.3436,
      "step": 37944
    },
    {
      "epoch": 3.037170263788969,
      "grad_norm": 12.718291282653809,
      "learning_rate": 4e-06,
      "loss": 0.3544,
      "step": 37995
    },
    {
      "epoch": 3.0412470023980815,
      "grad_norm": 8.636126518249512,
      "learning_rate": 4e-06,
      "loss": 0.3393,
      "step": 38046
    },
    {
      "epoch": 3.045323741007194,
      "grad_norm": 15.096343994140625,
      "learning_rate": 4e-06,
      "loss": 0.37,
      "step": 38097
    },
    {
      "epoch": 3.049400479616307,
      "grad_norm": 11.63073444366455,
      "learning_rate": 4e-06,
      "loss": 0.3439,
      "step": 38148
    },
    {
      "epoch": 3.0534772182254195,
      "grad_norm": 11.842741012573242,
      "learning_rate": 4e-06,
      "loss": 0.3363,
      "step": 38199
    },
    {
      "epoch": 3.0575539568345325,
      "grad_norm": 11.024470329284668,
      "learning_rate": 4e-06,
      "loss": 0.3429,
      "step": 38250
    },
    {
      "epoch": 3.061630695443645,
      "grad_norm": 12.170509338378906,
      "learning_rate": 4e-06,
      "loss": 0.3446,
      "step": 38301
    },
    {
      "epoch": 3.065707434052758,
      "grad_norm": 10.891854286193848,
      "learning_rate": 4e-06,
      "loss": 0.3446,
      "step": 38352
    },
    {
      "epoch": 3.0697841726618704,
      "grad_norm": 6.265596866607666,
      "learning_rate": 4e-06,
      "loss": 0.3596,
      "step": 38403
    },
    {
      "epoch": 3.0738609112709834,
      "grad_norm": 10.650002479553223,
      "learning_rate": 4e-06,
      "loss": 0.3596,
      "step": 38454
    },
    {
      "epoch": 3.077937649880096,
      "grad_norm": 9.947229385375977,
      "learning_rate": 4e-06,
      "loss": 0.358,
      "step": 38505
    },
    {
      "epoch": 3.082014388489209,
      "grad_norm": 9.465967178344727,
      "learning_rate": 4e-06,
      "loss": 0.3645,
      "step": 38556
    },
    {
      "epoch": 3.0860911270983213,
      "grad_norm": 11.96566104888916,
      "learning_rate": 4e-06,
      "loss": 0.3621,
      "step": 38607
    },
    {
      "epoch": 3.090167865707434,
      "grad_norm": 12.238584518432617,
      "learning_rate": 4e-06,
      "loss": 0.3696,
      "step": 38658
    },
    {
      "epoch": 3.094244604316547,
      "grad_norm": 10.711029052734375,
      "learning_rate": 4e-06,
      "loss": 0.3451,
      "step": 38709
    },
    {
      "epoch": 3.0983213429256593,
      "grad_norm": 8.026768684387207,
      "learning_rate": 4e-06,
      "loss": 0.3652,
      "step": 38760
    },
    {
      "epoch": 3.1023980815347723,
      "grad_norm": 14.064273834228516,
      "learning_rate": 4e-06,
      "loss": 0.3672,
      "step": 38811
    },
    {
      "epoch": 3.1064748201438848,
      "grad_norm": 10.136927604675293,
      "learning_rate": 4e-06,
      "loss": 0.3472,
      "step": 38862
    },
    {
      "epoch": 3.1105515587529977,
      "grad_norm": 11.572930335998535,
      "learning_rate": 4e-06,
      "loss": 0.345,
      "step": 38913
    },
    {
      "epoch": 3.11462829736211,
      "grad_norm": 11.506898880004883,
      "learning_rate": 4e-06,
      "loss": 0.3715,
      "step": 38964
    },
    {
      "epoch": 3.118705035971223,
      "grad_norm": 9.424798965454102,
      "learning_rate": 4e-06,
      "loss": 0.3572,
      "step": 39015
    },
    {
      "epoch": 3.1227817745803357,
      "grad_norm": 10.848509788513184,
      "learning_rate": 4e-06,
      "loss": 0.3556,
      "step": 39066
    },
    {
      "epoch": 3.1268585131894486,
      "grad_norm": 16.014766693115234,
      "learning_rate": 4e-06,
      "loss": 0.3654,
      "step": 39117
    },
    {
      "epoch": 3.130935251798561,
      "grad_norm": 9.09598159790039,
      "learning_rate": 4e-06,
      "loss": 0.3653,
      "step": 39168
    },
    {
      "epoch": 3.1350119904076736,
      "grad_norm": 10.55789852142334,
      "learning_rate": 4e-06,
      "loss": 0.366,
      "step": 39219
    },
    {
      "epoch": 3.1390887290167866,
      "grad_norm": 10.417668342590332,
      "learning_rate": 4e-06,
      "loss": 0.3825,
      "step": 39270
    },
    {
      "epoch": 3.143165467625899,
      "grad_norm": 11.50376033782959,
      "learning_rate": 4e-06,
      "loss": 0.3555,
      "step": 39321
    },
    {
      "epoch": 3.147242206235012,
      "grad_norm": 11.568777084350586,
      "learning_rate": 4e-06,
      "loss": 0.3564,
      "step": 39372
    },
    {
      "epoch": 3.1513189448441246,
      "grad_norm": 8.952880859375,
      "learning_rate": 4e-06,
      "loss": 0.3678,
      "step": 39423
    },
    {
      "epoch": 3.1553956834532375,
      "grad_norm": 8.611526489257812,
      "learning_rate": 4e-06,
      "loss": 0.3603,
      "step": 39474
    },
    {
      "epoch": 3.15947242206235,
      "grad_norm": 8.78175163269043,
      "learning_rate": 4e-06,
      "loss": 0.3746,
      "step": 39525
    },
    {
      "epoch": 3.163549160671463,
      "grad_norm": 9.43456745147705,
      "learning_rate": 4e-06,
      "loss": 0.3782,
      "step": 39576
    },
    {
      "epoch": 3.1676258992805755,
      "grad_norm": 9.733057022094727,
      "learning_rate": 4e-06,
      "loss": 0.3592,
      "step": 39627
    },
    {
      "epoch": 3.1717026378896884,
      "grad_norm": 9.916589736938477,
      "learning_rate": 4e-06,
      "loss": 0.3457,
      "step": 39678
    },
    {
      "epoch": 3.175779376498801,
      "grad_norm": 10.23129653930664,
      "learning_rate": 4e-06,
      "loss": 0.3657,
      "step": 39729
    },
    {
      "epoch": 3.1798561151079134,
      "grad_norm": 11.001908302307129,
      "learning_rate": 4e-06,
      "loss": 0.3699,
      "step": 39780
    },
    {
      "epoch": 3.1839328537170264,
      "grad_norm": 11.079179763793945,
      "learning_rate": 4e-06,
      "loss": 0.3915,
      "step": 39831
    },
    {
      "epoch": 3.188009592326139,
      "grad_norm": 18.70330238342285,
      "learning_rate": 4e-06,
      "loss": 0.3548,
      "step": 39882
    },
    {
      "epoch": 3.192086330935252,
      "grad_norm": 8.845093727111816,
      "learning_rate": 4e-06,
      "loss": 0.367,
      "step": 39933
    },
    {
      "epoch": 3.1961630695443644,
      "grad_norm": 11.357044219970703,
      "learning_rate": 4e-06,
      "loss": 0.357,
      "step": 39984
    },
    {
      "epoch": 3.2002398081534773,
      "grad_norm": 15.063302040100098,
      "learning_rate": 4e-06,
      "loss": 0.3519,
      "step": 40035
    },
    {
      "epoch": 3.20431654676259,
      "grad_norm": 10.126876831054688,
      "learning_rate": 4e-06,
      "loss": 0.367,
      "step": 40086
    },
    {
      "epoch": 3.2083932853717028,
      "grad_norm": 26.457056045532227,
      "learning_rate": 4e-06,
      "loss": 0.3597,
      "step": 40137
    },
    {
      "epoch": 3.2124700239808153,
      "grad_norm": 11.17713737487793,
      "learning_rate": 4e-06,
      "loss": 0.3728,
      "step": 40188
    },
    {
      "epoch": 3.216546762589928,
      "grad_norm": 8.818745613098145,
      "learning_rate": 4e-06,
      "loss": 0.3574,
      "step": 40239
    },
    {
      "epoch": 3.2206235011990407,
      "grad_norm": 9.311058044433594,
      "learning_rate": 4e-06,
      "loss": 0.3748,
      "step": 40290
    },
    {
      "epoch": 3.2247002398081537,
      "grad_norm": 19.278276443481445,
      "learning_rate": 4e-06,
      "loss": 0.3817,
      "step": 40341
    },
    {
      "epoch": 3.228776978417266,
      "grad_norm": 6.943284511566162,
      "learning_rate": 4e-06,
      "loss": 0.3715,
      "step": 40392
    },
    {
      "epoch": 3.2328537170263787,
      "grad_norm": 13.043787002563477,
      "learning_rate": 4e-06,
      "loss": 0.3813,
      "step": 40443
    },
    {
      "epoch": 3.2369304556354916,
      "grad_norm": 10.44892692565918,
      "learning_rate": 4e-06,
      "loss": 0.3697,
      "step": 40494
    },
    {
      "epoch": 3.241007194244604,
      "grad_norm": 8.84313678741455,
      "learning_rate": 4e-06,
      "loss": 0.3756,
      "step": 40545
    },
    {
      "epoch": 3.245083932853717,
      "grad_norm": 16.814189910888672,
      "learning_rate": 4e-06,
      "loss": 0.3378,
      "step": 40596
    },
    {
      "epoch": 3.2491606714628296,
      "grad_norm": 12.71970272064209,
      "learning_rate": 4e-06,
      "loss": 0.3556,
      "step": 40647
    },
    {
      "epoch": 3.2532374100719426,
      "grad_norm": 9.703192710876465,
      "learning_rate": 4e-06,
      "loss": 0.376,
      "step": 40698
    },
    {
      "epoch": 3.257314148681055,
      "grad_norm": 13.782936096191406,
      "learning_rate": 4e-06,
      "loss": 0.3696,
      "step": 40749
    },
    {
      "epoch": 3.261390887290168,
      "grad_norm": 9.372941017150879,
      "learning_rate": 4e-06,
      "loss": 0.3808,
      "step": 40800
    },
    {
      "epoch": 3.2654676258992805,
      "grad_norm": 19.661420822143555,
      "learning_rate": 4e-06,
      "loss": 0.3556,
      "step": 40851
    },
    {
      "epoch": 3.2695443645083935,
      "grad_norm": 10.229145050048828,
      "learning_rate": 4e-06,
      "loss": 0.3681,
      "step": 40902
    },
    {
      "epoch": 3.273621103117506,
      "grad_norm": 11.280895233154297,
      "learning_rate": 4e-06,
      "loss": 0.3479,
      "step": 40953
    },
    {
      "epoch": 3.277697841726619,
      "grad_norm": 10.243282318115234,
      "learning_rate": 4e-06,
      "loss": 0.3625,
      "step": 41004
    },
    {
      "epoch": 3.2817745803357314,
      "grad_norm": 11.821662902832031,
      "learning_rate": 4e-06,
      "loss": 0.3717,
      "step": 41055
    },
    {
      "epoch": 3.285851318944844,
      "grad_norm": 8.010397911071777,
      "learning_rate": 4e-06,
      "loss": 0.3478,
      "step": 41106
    },
    {
      "epoch": 3.289928057553957,
      "grad_norm": 8.760814666748047,
      "learning_rate": 4e-06,
      "loss": 0.3514,
      "step": 41157
    },
    {
      "epoch": 3.2940047961630694,
      "grad_norm": 10.369292259216309,
      "learning_rate": 4e-06,
      "loss": 0.351,
      "step": 41208
    },
    {
      "epoch": 3.2980815347721824,
      "grad_norm": 10.996461868286133,
      "learning_rate": 4e-06,
      "loss": 0.3839,
      "step": 41259
    },
    {
      "epoch": 3.302158273381295,
      "grad_norm": 8.087442398071289,
      "learning_rate": 4e-06,
      "loss": 0.3635,
      "step": 41310
    },
    {
      "epoch": 3.306235011990408,
      "grad_norm": 12.509740829467773,
      "learning_rate": 4e-06,
      "loss": 0.4206,
      "step": 41361
    },
    {
      "epoch": 3.3103117505995203,
      "grad_norm": 8.956672668457031,
      "learning_rate": 4e-06,
      "loss": 0.3758,
      "step": 41412
    },
    {
      "epoch": 3.3143884892086333,
      "grad_norm": 7.626391410827637,
      "learning_rate": 4e-06,
      "loss": 0.3736,
      "step": 41463
    },
    {
      "epoch": 3.3184652278177458,
      "grad_norm": 10.433640480041504,
      "learning_rate": 4e-06,
      "loss": 0.3929,
      "step": 41514
    },
    {
      "epoch": 3.3225419664268587,
      "grad_norm": 20.410919189453125,
      "learning_rate": 4e-06,
      "loss": 0.3686,
      "step": 41565
    },
    {
      "epoch": 3.3266187050359712,
      "grad_norm": 20.467952728271484,
      "learning_rate": 4e-06,
      "loss": 0.3728,
      "step": 41616
    },
    {
      "epoch": 3.3306954436450837,
      "grad_norm": 10.825440406799316,
      "learning_rate": 4e-06,
      "loss": 0.3817,
      "step": 41667
    },
    {
      "epoch": 3.3347721822541967,
      "grad_norm": 10.786737442016602,
      "learning_rate": 4e-06,
      "loss": 0.3696,
      "step": 41718
    },
    {
      "epoch": 3.338848920863309,
      "grad_norm": 10.2112455368042,
      "learning_rate": 4e-06,
      "loss": 0.3697,
      "step": 41769
    },
    {
      "epoch": 3.342925659472422,
      "grad_norm": 15.0366792678833,
      "learning_rate": 4e-06,
      "loss": 0.3769,
      "step": 41820
    },
    {
      "epoch": 3.3470023980815347,
      "grad_norm": 9.100713729858398,
      "learning_rate": 4e-06,
      "loss": 0.3783,
      "step": 41871
    },
    {
      "epoch": 3.3510791366906476,
      "grad_norm": 10.590435028076172,
      "learning_rate": 4e-06,
      "loss": 0.3692,
      "step": 41922
    },
    {
      "epoch": 3.35515587529976,
      "grad_norm": 8.377703666687012,
      "learning_rate": 4e-06,
      "loss": 0.3618,
      "step": 41973
    },
    {
      "epoch": 3.359232613908873,
      "grad_norm": 14.165430068969727,
      "learning_rate": 4e-06,
      "loss": 0.3742,
      "step": 42024
    },
    {
      "epoch": 3.3633093525179856,
      "grad_norm": 11.476744651794434,
      "learning_rate": 4e-06,
      "loss": 0.3858,
      "step": 42075
    },
    {
      "epoch": 3.3673860911270985,
      "grad_norm": 17.414968490600586,
      "learning_rate": 4e-06,
      "loss": 0.3953,
      "step": 42126
    },
    {
      "epoch": 3.371462829736211,
      "grad_norm": 8.820629119873047,
      "learning_rate": 4e-06,
      "loss": 0.3681,
      "step": 42177
    },
    {
      "epoch": 3.3755395683453235,
      "grad_norm": 8.851969718933105,
      "learning_rate": 4e-06,
      "loss": 0.3799,
      "step": 42228
    },
    {
      "epoch": 3.3796163069544365,
      "grad_norm": 10.173127174377441,
      "learning_rate": 4e-06,
      "loss": 0.3758,
      "step": 42279
    },
    {
      "epoch": 3.383693045563549,
      "grad_norm": 10.76746940612793,
      "learning_rate": 4e-06,
      "loss": 0.3742,
      "step": 42330
    },
    {
      "epoch": 3.387769784172662,
      "grad_norm": 13.430030822753906,
      "learning_rate": 4e-06,
      "loss": 0.3652,
      "step": 42381
    },
    {
      "epoch": 3.3918465227817745,
      "grad_norm": 10.032756805419922,
      "learning_rate": 4e-06,
      "loss": 0.3886,
      "step": 42432
    },
    {
      "epoch": 3.3959232613908874,
      "grad_norm": 11.365612030029297,
      "learning_rate": 4e-06,
      "loss": 0.3756,
      "step": 42483
    },
    {
      "epoch": 3.4,
      "grad_norm": 10.851112365722656,
      "learning_rate": 4e-06,
      "loss": 0.3611,
      "step": 42534
    },
    {
      "epoch": 3.404076738609113,
      "grad_norm": 11.666132926940918,
      "learning_rate": 4e-06,
      "loss": 0.3842,
      "step": 42585
    },
    {
      "epoch": 3.4081534772182254,
      "grad_norm": 9.857427597045898,
      "learning_rate": 4e-06,
      "loss": 0.3829,
      "step": 42636
    },
    {
      "epoch": 3.4122302158273383,
      "grad_norm": 11.020806312561035,
      "learning_rate": 4e-06,
      "loss": 0.4,
      "step": 42687
    },
    {
      "epoch": 3.416306954436451,
      "grad_norm": 16.934202194213867,
      "learning_rate": 4e-06,
      "loss": 0.3722,
      "step": 42738
    },
    {
      "epoch": 3.4203836930455633,
      "grad_norm": 14.051671981811523,
      "learning_rate": 4e-06,
      "loss": 0.372,
      "step": 42789
    },
    {
      "epoch": 3.4244604316546763,
      "grad_norm": 7.930389404296875,
      "learning_rate": 4e-06,
      "loss": 0.3667,
      "step": 42840
    },
    {
      "epoch": 3.428537170263789,
      "grad_norm": 8.652220726013184,
      "learning_rate": 4e-06,
      "loss": 0.3759,
      "step": 42891
    },
    {
      "epoch": 3.4326139088729017,
      "grad_norm": 12.622906684875488,
      "learning_rate": 4e-06,
      "loss": 0.3747,
      "step": 42942
    },
    {
      "epoch": 3.4366906474820142,
      "grad_norm": 13.094279289245605,
      "learning_rate": 4e-06,
      "loss": 0.3625,
      "step": 42993
    },
    {
      "epoch": 3.440767386091127,
      "grad_norm": 7.301567554473877,
      "learning_rate": 4e-06,
      "loss": 0.3593,
      "step": 43044
    },
    {
      "epoch": 3.4448441247002397,
      "grad_norm": 9.250753402709961,
      "learning_rate": 4e-06,
      "loss": 0.3883,
      "step": 43095
    },
    {
      "epoch": 3.4489208633093527,
      "grad_norm": 10.20443344116211,
      "learning_rate": 4e-06,
      "loss": 0.3845,
      "step": 43146
    },
    {
      "epoch": 3.452997601918465,
      "grad_norm": 11.96213150024414,
      "learning_rate": 4e-06,
      "loss": 0.3738,
      "step": 43197
    },
    {
      "epoch": 3.457074340527578,
      "grad_norm": 12.692938804626465,
      "learning_rate": 4e-06,
      "loss": 0.3684,
      "step": 43248
    },
    {
      "epoch": 3.4611510791366906,
      "grad_norm": 9.438404083251953,
      "learning_rate": 4e-06,
      "loss": 0.3924,
      "step": 43299
    },
    {
      "epoch": 3.465227817745803,
      "grad_norm": 10.818068504333496,
      "learning_rate": 4e-06,
      "loss": 0.3637,
      "step": 43350
    },
    {
      "epoch": 3.469304556354916,
      "grad_norm": 10.822028160095215,
      "learning_rate": 4e-06,
      "loss": 0.3766,
      "step": 43401
    },
    {
      "epoch": 3.4733812949640286,
      "grad_norm": 18.10808753967285,
      "learning_rate": 4e-06,
      "loss": 0.379,
      "step": 43452
    },
    {
      "epoch": 3.4774580335731415,
      "grad_norm": 9.91562557220459,
      "learning_rate": 4e-06,
      "loss": 0.3899,
      "step": 43503
    },
    {
      "epoch": 3.481534772182254,
      "grad_norm": 8.62396240234375,
      "learning_rate": 4e-06,
      "loss": 0.3778,
      "step": 43554
    },
    {
      "epoch": 3.485611510791367,
      "grad_norm": 11.576669692993164,
      "learning_rate": 4e-06,
      "loss": 0.3726,
      "step": 43605
    },
    {
      "epoch": 3.4896882494004795,
      "grad_norm": 7.802394390106201,
      "learning_rate": 4e-06,
      "loss": 0.3871,
      "step": 43656
    },
    {
      "epoch": 3.4937649880095925,
      "grad_norm": 13.96810245513916,
      "learning_rate": 4e-06,
      "loss": 0.3746,
      "step": 43707
    },
    {
      "epoch": 3.497841726618705,
      "grad_norm": 9.877827644348145,
      "learning_rate": 4e-06,
      "loss": 0.3786,
      "step": 43758
    },
    {
      "epoch": 3.501918465227818,
      "grad_norm": 16.26590919494629,
      "learning_rate": 4e-06,
      "loss": 0.3708,
      "step": 43809
    },
    {
      "epoch": 3.5059952038369304,
      "grad_norm": 9.22670841217041,
      "learning_rate": 4e-06,
      "loss": 0.3917,
      "step": 43860
    },
    {
      "epoch": 3.510071942446043,
      "grad_norm": 9.227033615112305,
      "learning_rate": 4e-06,
      "loss": 0.3779,
      "step": 43911
    },
    {
      "epoch": 3.514148681055156,
      "grad_norm": 13.168619155883789,
      "learning_rate": 4e-06,
      "loss": 0.3894,
      "step": 43962
    },
    {
      "epoch": 3.518225419664269,
      "grad_norm": 12.087112426757812,
      "learning_rate": 4e-06,
      "loss": 0.3665,
      "step": 44013
    },
    {
      "epoch": 3.5223021582733813,
      "grad_norm": 9.974842071533203,
      "learning_rate": 4e-06,
      "loss": 0.3815,
      "step": 44064
    },
    {
      "epoch": 3.526378896882494,
      "grad_norm": 7.788482666015625,
      "learning_rate": 4e-06,
      "loss": 0.3455,
      "step": 44115
    },
    {
      "epoch": 3.530455635491607,
      "grad_norm": 10.226361274719238,
      "learning_rate": 4e-06,
      "loss": 0.3908,
      "step": 44166
    },
    {
      "epoch": 3.5345323741007193,
      "grad_norm": 10.235329627990723,
      "learning_rate": 4e-06,
      "loss": 0.3669,
      "step": 44217
    },
    {
      "epoch": 3.5386091127098322,
      "grad_norm": 9.605178833007812,
      "learning_rate": 4e-06,
      "loss": 0.3933,
      "step": 44268
    },
    {
      "epoch": 3.5426858513189448,
      "grad_norm": 11.289952278137207,
      "learning_rate": 4e-06,
      "loss": 0.3878,
      "step": 44319
    },
    {
      "epoch": 3.5467625899280577,
      "grad_norm": 13.029062271118164,
      "learning_rate": 4e-06,
      "loss": 0.3834,
      "step": 44370
    },
    {
      "epoch": 3.55083932853717,
      "grad_norm": 10.281686782836914,
      "learning_rate": 4e-06,
      "loss": 0.3763,
      "step": 44421
    },
    {
      "epoch": 3.5549160671462827,
      "grad_norm": 13.518745422363281,
      "learning_rate": 4e-06,
      "loss": 0.367,
      "step": 44472
    },
    {
      "epoch": 3.5589928057553957,
      "grad_norm": 8.118227005004883,
      "learning_rate": 4e-06,
      "loss": 0.3767,
      "step": 44523
    },
    {
      "epoch": 3.5630695443645086,
      "grad_norm": 7.33043098449707,
      "learning_rate": 4e-06,
      "loss": 0.3943,
      "step": 44574
    },
    {
      "epoch": 3.567146282973621,
      "grad_norm": 10.30998420715332,
      "learning_rate": 4e-06,
      "loss": 0.378,
      "step": 44625
    },
    {
      "epoch": 3.5712230215827336,
      "grad_norm": 9.2630615234375,
      "learning_rate": 4e-06,
      "loss": 0.3696,
      "step": 44676
    },
    {
      "epoch": 3.5752997601918466,
      "grad_norm": 11.301423072814941,
      "learning_rate": 4e-06,
      "loss": 0.366,
      "step": 44727
    },
    {
      "epoch": 3.579376498800959,
      "grad_norm": 9.730866432189941,
      "learning_rate": 4e-06,
      "loss": 0.3806,
      "step": 44778
    },
    {
      "epoch": 3.583453237410072,
      "grad_norm": 11.54915714263916,
      "learning_rate": 4e-06,
      "loss": 0.3938,
      "step": 44829
    },
    {
      "epoch": 3.5875299760191846,
      "grad_norm": 15.167139053344727,
      "learning_rate": 4e-06,
      "loss": 0.3744,
      "step": 44880
    },
    {
      "epoch": 3.5916067146282975,
      "grad_norm": 8.089457511901855,
      "learning_rate": 4e-06,
      "loss": 0.3716,
      "step": 44931
    },
    {
      "epoch": 3.59568345323741,
      "grad_norm": 8.826700210571289,
      "learning_rate": 4e-06,
      "loss": 0.3936,
      "step": 44982
    },
    {
      "epoch": 3.5997601918465225,
      "grad_norm": 16.74231719970703,
      "learning_rate": 4e-06,
      "loss": 0.3655,
      "step": 45033
    },
    {
      "epoch": 3.6038369304556355,
      "grad_norm": 11.104880332946777,
      "learning_rate": 4e-06,
      "loss": 0.3733,
      "step": 45084
    },
    {
      "epoch": 3.6079136690647484,
      "grad_norm": 13.828048706054688,
      "learning_rate": 4e-06,
      "loss": 0.3894,
      "step": 45135
    },
    {
      "epoch": 3.611990407673861,
      "grad_norm": 9.32219123840332,
      "learning_rate": 4e-06,
      "loss": 0.3763,
      "step": 45186
    },
    {
      "epoch": 3.6160671462829734,
      "grad_norm": 17.221960067749023,
      "learning_rate": 4e-06,
      "loss": 0.3661,
      "step": 45237
    },
    {
      "epoch": 3.6201438848920864,
      "grad_norm": 9.14663028717041,
      "learning_rate": 4e-06,
      "loss": 0.392,
      "step": 45288
    },
    {
      "epoch": 3.624220623501199,
      "grad_norm": 9.327155113220215,
      "learning_rate": 4e-06,
      "loss": 0.3742,
      "step": 45339
    },
    {
      "epoch": 3.628297362110312,
      "grad_norm": 9.842641830444336,
      "learning_rate": 4e-06,
      "loss": 0.3829,
      "step": 45390
    },
    {
      "epoch": 3.6323741007194243,
      "grad_norm": 19.16097068786621,
      "learning_rate": 4e-06,
      "loss": 0.3706,
      "step": 45441
    },
    {
      "epoch": 3.6364508393285373,
      "grad_norm": 15.143219947814941,
      "learning_rate": 4e-06,
      "loss": 0.3799,
      "step": 45492
    },
    {
      "epoch": 3.64052757793765,
      "grad_norm": 10.469873428344727,
      "learning_rate": 4e-06,
      "loss": 0.3865,
      "step": 45543
    },
    {
      "epoch": 3.6446043165467623,
      "grad_norm": 14.511484146118164,
      "learning_rate": 4e-06,
      "loss": 0.3966,
      "step": 45594
    },
    {
      "epoch": 3.6486810551558753,
      "grad_norm": 9.992953300476074,
      "learning_rate": 4e-06,
      "loss": 0.3647,
      "step": 45645
    },
    {
      "epoch": 3.652757793764988,
      "grad_norm": 13.195287704467773,
      "learning_rate": 4e-06,
      "loss": 0.3862,
      "step": 45696
    },
    {
      "epoch": 3.6568345323741007,
      "grad_norm": 19.901704788208008,
      "learning_rate": 4e-06,
      "loss": 0.3755,
      "step": 45747
    },
    {
      "epoch": 3.6609112709832132,
      "grad_norm": 10.374850273132324,
      "learning_rate": 4e-06,
      "loss": 0.388,
      "step": 45798
    },
    {
      "epoch": 3.664988009592326,
      "grad_norm": 9.129409790039062,
      "learning_rate": 4e-06,
      "loss": 0.3971,
      "step": 45849
    },
    {
      "epoch": 3.6690647482014387,
      "grad_norm": 26.201753616333008,
      "learning_rate": 4e-06,
      "loss": 0.382,
      "step": 45900
    },
    {
      "epoch": 3.6731414868105516,
      "grad_norm": 9.779403686523438,
      "learning_rate": 4e-06,
      "loss": 0.377,
      "step": 45951
    },
    {
      "epoch": 3.677218225419664,
      "grad_norm": 13.456531524658203,
      "learning_rate": 4e-06,
      "loss": 0.3915,
      "step": 46002
    },
    {
      "epoch": 3.681294964028777,
      "grad_norm": 11.109323501586914,
      "learning_rate": 4e-06,
      "loss": 0.3903,
      "step": 46053
    },
    {
      "epoch": 3.6853717026378896,
      "grad_norm": 12.289959907531738,
      "learning_rate": 4e-06,
      "loss": 0.3905,
      "step": 46104
    },
    {
      "epoch": 3.6894484412470026,
      "grad_norm": 19.19773292541504,
      "learning_rate": 4e-06,
      "loss": 0.3862,
      "step": 46155
    },
    {
      "epoch": 3.693525179856115,
      "grad_norm": 9.198209762573242,
      "learning_rate": 4e-06,
      "loss": 0.3839,
      "step": 46206
    },
    {
      "epoch": 3.697601918465228,
      "grad_norm": 11.020429611206055,
      "learning_rate": 4e-06,
      "loss": 0.382,
      "step": 46257
    },
    {
      "epoch": 3.7016786570743405,
      "grad_norm": 7.598019123077393,
      "learning_rate": 4e-06,
      "loss": 0.3734,
      "step": 46308
    },
    {
      "epoch": 3.705755395683453,
      "grad_norm": 10.676024436950684,
      "learning_rate": 4e-06,
      "loss": 0.3831,
      "step": 46359
    },
    {
      "epoch": 3.709832134292566,
      "grad_norm": 11.17956829071045,
      "learning_rate": 4e-06,
      "loss": 0.3801,
      "step": 46410
    },
    {
      "epoch": 3.713908872901679,
      "grad_norm": 8.79090690612793,
      "learning_rate": 4e-06,
      "loss": 0.3604,
      "step": 46461
    },
    {
      "epoch": 3.7179856115107914,
      "grad_norm": 19.603212356567383,
      "learning_rate": 4e-06,
      "loss": 0.3798,
      "step": 46512
    },
    {
      "epoch": 3.722062350119904,
      "grad_norm": 7.681759357452393,
      "learning_rate": 4e-06,
      "loss": 0.3728,
      "step": 46563
    },
    {
      "epoch": 3.726139088729017,
      "grad_norm": 9.889313697814941,
      "learning_rate": 4e-06,
      "loss": 0.369,
      "step": 46614
    },
    {
      "epoch": 3.7302158273381294,
      "grad_norm": 9.259174346923828,
      "learning_rate": 4e-06,
      "loss": 0.3827,
      "step": 46665
    },
    {
      "epoch": 3.7342925659472423,
      "grad_norm": 10.275470733642578,
      "learning_rate": 4e-06,
      "loss": 0.3887,
      "step": 46716
    },
    {
      "epoch": 3.738369304556355,
      "grad_norm": 10.178388595581055,
      "learning_rate": 4e-06,
      "loss": 0.3563,
      "step": 46767
    },
    {
      "epoch": 3.742446043165468,
      "grad_norm": 10.93380355834961,
      "learning_rate": 4e-06,
      "loss": 0.3722,
      "step": 46818
    },
    {
      "epoch": 3.7465227817745803,
      "grad_norm": 8.50243854522705,
      "learning_rate": 4e-06,
      "loss": 0.3881,
      "step": 46869
    },
    {
      "epoch": 3.750599520383693,
      "grad_norm": 10.646613121032715,
      "learning_rate": 4e-06,
      "loss": 0.3688,
      "step": 46920
    },
    {
      "epoch": 3.7546762589928058,
      "grad_norm": 9.172883987426758,
      "learning_rate": 4e-06,
      "loss": 0.3957,
      "step": 46971
    },
    {
      "epoch": 3.7587529976019187,
      "grad_norm": 12.193649291992188,
      "learning_rate": 4e-06,
      "loss": 0.3608,
      "step": 47022
    },
    {
      "epoch": 3.7628297362110312,
      "grad_norm": 10.822504997253418,
      "learning_rate": 4e-06,
      "loss": 0.3955,
      "step": 47073
    },
    {
      "epoch": 3.7669064748201437,
      "grad_norm": 9.399076461791992,
      "learning_rate": 4e-06,
      "loss": 0.3674,
      "step": 47124
    },
    {
      "epoch": 3.7709832134292567,
      "grad_norm": 11.991400718688965,
      "learning_rate": 4e-06,
      "loss": 0.3831,
      "step": 47175
    },
    {
      "epoch": 3.775059952038369,
      "grad_norm": 8.800088882446289,
      "learning_rate": 4e-06,
      "loss": 0.3899,
      "step": 47226
    },
    {
      "epoch": 3.779136690647482,
      "grad_norm": 12.095312118530273,
      "learning_rate": 4e-06,
      "loss": 0.3894,
      "step": 47277
    },
    {
      "epoch": 3.7832134292565947,
      "grad_norm": 8.187620162963867,
      "learning_rate": 4e-06,
      "loss": 0.3885,
      "step": 47328
    },
    {
      "epoch": 3.7872901678657076,
      "grad_norm": 10.372663497924805,
      "learning_rate": 4e-06,
      "loss": 0.3872,
      "step": 47379
    },
    {
      "epoch": 3.79136690647482,
      "grad_norm": 10.6528902053833,
      "learning_rate": 4e-06,
      "loss": 0.3773,
      "step": 47430
    },
    {
      "epoch": 3.7954436450839326,
      "grad_norm": 14.85527229309082,
      "learning_rate": 4e-06,
      "loss": 0.3916,
      "step": 47481
    },
    {
      "epoch": 3.7995203836930456,
      "grad_norm": 11.097454071044922,
      "learning_rate": 4e-06,
      "loss": 0.3785,
      "step": 47532
    },
    {
      "epoch": 3.8,
      "eval_accuracy": 0.6965024690095134,
      "eval_loss": 1.5606005191802979,
      "eval_runtime": 640.6436,
      "eval_samples_per_second": 1.378,
      "eval_steps_per_second": 0.69,
      "step": 47538
    },
    {
      "epoch": 3.8035971223021585,
      "grad_norm": 7.12841796875,
      "learning_rate": 4e-06,
      "loss": 0.3545,
      "step": 47583
    },
    {
      "epoch": 3.807673860911271,
      "grad_norm": 11.305839538574219,
      "learning_rate": 4e-06,
      "loss": 0.3929,
      "step": 47634
    },
    {
      "epoch": 3.8117505995203835,
      "grad_norm": 9.043652534484863,
      "learning_rate": 4e-06,
      "loss": 0.3703,
      "step": 47685
    },
    {
      "epoch": 3.8158273381294965,
      "grad_norm": 12.582476615905762,
      "learning_rate": 4e-06,
      "loss": 0.3726,
      "step": 47736
    },
    {
      "epoch": 3.819904076738609,
      "grad_norm": 11.35181713104248,
      "learning_rate": 4e-06,
      "loss": 0.3665,
      "step": 47787
    },
    {
      "epoch": 3.823980815347722,
      "grad_norm": 9.42553997039795,
      "learning_rate": 4e-06,
      "loss": 0.3833,
      "step": 47838
    },
    {
      "epoch": 3.8280575539568344,
      "grad_norm": 8.2606782913208,
      "learning_rate": 4e-06,
      "loss": 0.3965,
      "step": 47889
    },
    {
      "epoch": 3.8321342925659474,
      "grad_norm": 8.683232307434082,
      "learning_rate": 4e-06,
      "loss": 0.3939,
      "step": 47940
    },
    {
      "epoch": 3.83621103117506,
      "grad_norm": 8.038113594055176,
      "learning_rate": 4e-06,
      "loss": 0.3825,
      "step": 47991
    },
    {
      "epoch": 3.8402877697841724,
      "grad_norm": 10.73523235321045,
      "learning_rate": 4e-06,
      "loss": 0.3638,
      "step": 48042
    },
    {
      "epoch": 3.8443645083932854,
      "grad_norm": 10.222670555114746,
      "learning_rate": 4e-06,
      "loss": 0.3983,
      "step": 48093
    },
    {
      "epoch": 3.8484412470023983,
      "grad_norm": 9.837847709655762,
      "learning_rate": 4e-06,
      "loss": 0.3927,
      "step": 48144
    },
    {
      "epoch": 3.852517985611511,
      "grad_norm": 9.499611854553223,
      "learning_rate": 4e-06,
      "loss": 0.3796,
      "step": 48195
    },
    {
      "epoch": 3.8565947242206233,
      "grad_norm": 11.99538803100586,
      "learning_rate": 4e-06,
      "loss": 0.38,
      "step": 48246
    },
    {
      "epoch": 3.8606714628297363,
      "grad_norm": 11.588541984558105,
      "learning_rate": 4e-06,
      "loss": 0.3785,
      "step": 48297
    },
    {
      "epoch": 3.864748201438849,
      "grad_norm": 10.62131118774414,
      "learning_rate": 4e-06,
      "loss": 0.3762,
      "step": 48348
    },
    {
      "epoch": 3.8688249400479617,
      "grad_norm": 18.6268367767334,
      "learning_rate": 4e-06,
      "loss": 0.371,
      "step": 48399
    },
    {
      "epoch": 3.8729016786570742,
      "grad_norm": 10.290664672851562,
      "learning_rate": 4e-06,
      "loss": 0.3766,
      "step": 48450
    },
    {
      "epoch": 3.876978417266187,
      "grad_norm": 18.378341674804688,
      "learning_rate": 4e-06,
      "loss": 0.3939,
      "step": 48501
    },
    {
      "epoch": 3.8810551558752997,
      "grad_norm": 14.297704696655273,
      "learning_rate": 4e-06,
      "loss": 0.3971,
      "step": 48552
    },
    {
      "epoch": 3.885131894484412,
      "grad_norm": 13.281100273132324,
      "learning_rate": 4e-06,
      "loss": 0.3983,
      "step": 48603
    },
    {
      "epoch": 3.889208633093525,
      "grad_norm": 10.93030834197998,
      "learning_rate": 4e-06,
      "loss": 0.3862,
      "step": 48654
    },
    {
      "epoch": 3.893285371702638,
      "grad_norm": 12.553319931030273,
      "learning_rate": 4e-06,
      "loss": 0.3706,
      "step": 48705
    },
    {
      "epoch": 3.8973621103117506,
      "grad_norm": 9.232672691345215,
      "learning_rate": 4e-06,
      "loss": 0.3934,
      "step": 48756
    },
    {
      "epoch": 3.901438848920863,
      "grad_norm": 16.305965423583984,
      "learning_rate": 4e-06,
      "loss": 0.3805,
      "step": 48807
    },
    {
      "epoch": 3.905515587529976,
      "grad_norm": 10.797216415405273,
      "learning_rate": 4e-06,
      "loss": 0.3916,
      "step": 48858
    },
    {
      "epoch": 3.9095923261390886,
      "grad_norm": 8.941755294799805,
      "learning_rate": 4e-06,
      "loss": 0.3635,
      "step": 48909
    },
    {
      "epoch": 3.9136690647482015,
      "grad_norm": 8.785736083984375,
      "learning_rate": 4e-06,
      "loss": 0.387,
      "step": 48960
    },
    {
      "epoch": 3.917745803357314,
      "grad_norm": 7.656558036804199,
      "learning_rate": 4e-06,
      "loss": 0.3824,
      "step": 49011
    },
    {
      "epoch": 3.921822541966427,
      "grad_norm": 11.116272926330566,
      "learning_rate": 4e-06,
      "loss": 0.3714,
      "step": 49062
    },
    {
      "epoch": 3.9258992805755395,
      "grad_norm": 11.922003746032715,
      "learning_rate": 4e-06,
      "loss": 0.3786,
      "step": 49113
    },
    {
      "epoch": 3.929976019184652,
      "grad_norm": 13.215149879455566,
      "learning_rate": 4e-06,
      "loss": 0.3813,
      "step": 49164
    },
    {
      "epoch": 3.934052757793765,
      "grad_norm": 10.068342208862305,
      "learning_rate": 4e-06,
      "loss": 0.3832,
      "step": 49215
    },
    {
      "epoch": 3.938129496402878,
      "grad_norm": 11.34733772277832,
      "learning_rate": 4e-06,
      "loss": 0.3798,
      "step": 49266
    },
    {
      "epoch": 3.9422062350119904,
      "grad_norm": 12.34439754486084,
      "learning_rate": 4e-06,
      "loss": 0.3874,
      "step": 49317
    },
    {
      "epoch": 3.946282973621103,
      "grad_norm": 14.275751113891602,
      "learning_rate": 4e-06,
      "loss": 0.3766,
      "step": 49368
    },
    {
      "epoch": 3.950359712230216,
      "grad_norm": 13.05200481414795,
      "learning_rate": 4e-06,
      "loss": 0.4125,
      "step": 49419
    },
    {
      "epoch": 3.9544364508393284,
      "grad_norm": 11.001909255981445,
      "learning_rate": 4e-06,
      "loss": 0.3874,
      "step": 49470
    },
    {
      "epoch": 3.9585131894484413,
      "grad_norm": 19.2470645904541,
      "learning_rate": 4e-06,
      "loss": 0.3912,
      "step": 49521
    },
    {
      "epoch": 3.962589928057554,
      "grad_norm": 10.058159828186035,
      "learning_rate": 4e-06,
      "loss": 0.3959,
      "step": 49572
    },
    {
      "epoch": 3.966666666666667,
      "grad_norm": 15.19091510772705,
      "learning_rate": 4e-06,
      "loss": 0.3859,
      "step": 49623
    },
    {
      "epoch": 3.9707434052757793,
      "grad_norm": 10.599493980407715,
      "learning_rate": 4e-06,
      "loss": 0.3835,
      "step": 49674
    },
    {
      "epoch": 3.9748201438848922,
      "grad_norm": 10.723170280456543,
      "learning_rate": 4e-06,
      "loss": 0.4041,
      "step": 49725
    },
    {
      "epoch": 3.9788968824940047,
      "grad_norm": 8.900066375732422,
      "learning_rate": 4e-06,
      "loss": 0.368,
      "step": 49776
    },
    {
      "epoch": 3.9829736211031177,
      "grad_norm": 12.440783500671387,
      "learning_rate": 4e-06,
      "loss": 0.391,
      "step": 49827
    },
    {
      "epoch": 3.98705035971223,
      "grad_norm": 14.259941101074219,
      "learning_rate": 4e-06,
      "loss": 0.3938,
      "step": 49878
    },
    {
      "epoch": 3.9911270983213427,
      "grad_norm": 12.573604583740234,
      "learning_rate": 4e-06,
      "loss": 0.4138,
      "step": 49929
    },
    {
      "epoch": 3.9952038369304557,
      "grad_norm": 8.745530128479004,
      "learning_rate": 4e-06,
      "loss": 0.3792,
      "step": 49980
    },
    {
      "epoch": 3.9992805755395686,
      "grad_norm": 15.322137832641602,
      "learning_rate": 4e-06,
      "loss": 0.3992,
      "step": 50031
    }
  ],
  "logging_steps": 51,
  "max_steps": 50040,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 2000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8.74112569291309e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
