{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "from monai.data.utils import correct_nifti_header_if_necessary\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "import os\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<im_patch>', '<bx_start>', '<bx_end>']\n",
      "[50297, 50296, 50295]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.blocks.patchembedding PatchEmbeddingBlock.__init__:pos_embed: Argument `pos_embed` has been deprecated since version 1.2. It will be removed in version 1.4. please use `proj_type` instead.\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "Some weights of LamedPhiForCausalLM were not initialized from the model checkpoint at /import/c4dm-04/siyoul/Med3DLLM/checkpoint/amosmm_chatgpt_phi2_0210@bs2_acc1_ep16_lr2e5_ws2_fused/checkpoint-132000 and are newly initialized: ['model.seg_projector.0.bias', 'model.seg_projector.0.weight', 'model.seg_projector.2.bias', 'model.seg_projector.2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "lamed_model_path = \"/import/c4dm-04/siyoul/Med3DLLM/checkpoint/amosmm_chatgpt_phi2_0210@bs2_acc1_ep16_lr2e5_ws2_fused/checkpoint-132000\"\n",
    "\n",
    "def load_model(lamed_model_path, enable_lora=False):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        lamed_model_path,\n",
    "        model_max_length=2048,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "        pad_token=\"<unk>\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(tokenizer.additional_special_tokens)\n",
    "    print(tokenizer.additional_special_tokens_ids)\n",
    "    if tokenizer.unk_token is not None and tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.unk_token\n",
    "    \n",
    "    if enable_lora:\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=find_all_linear_names(lamed_model),\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        print(\"Adding LoRA adapters only on LLM.\")\n",
    "        lamed_model = get_peft_model(lamed_model, lora_config)\n",
    "        # lamed_model.print_trainable_parameters()\n",
    "        print(\"Load weights with LoRA\")\n",
    "        state_dict = torch.load(lamed_model_path, map_location=\"cpu\")\n",
    "        lamed_model.load_state_dict(state_dict, strict=True)\n",
    "        print(\"Merge weights with LoRA\")\n",
    "        lamed_model = lamed_model.merge_and_unload()\n",
    "    else:\n",
    "        lamed_model = AutoModelForCausalLM.from_pretrained(\n",
    "        lamed_model_path,\n",
    "        trust_remote_code=True,\n",
    "        )\n",
    "    lamed_model = lamed_model.to(\"cpu\")\n",
    "    lamed_model.eval()\n",
    "    return tokenizer, lamed_model\n",
    "\n",
    "tokenizer, lamed_model = load_model(lamed_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.array CropForeground.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Image shape torch.Size([2, 1, 32, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "image_file_path = \"/import/c4dm-04/siyoul/Med3DLLM/datasets/AMOS-MM/imagesVa/amos_0008.nii.gz\"\n",
    "from src.utils.data_transforms import val_transforms\n",
    "image = [val_transforms(image_file_path), val_transforms(image_file_path)]\n",
    "image = torch.stack(image)\n",
    "print(\"Input Image shape\",image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.array CropForeground.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "from src.utils.linear_3d_transform import Linear3DTransform\n",
    "l3d_t = Linear3DTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Image shape torch.Size([2, 8, 32, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "image = [l3d_t(image_file_path), l3d_t(image_file_path)]\n",
    "image = torch.stack(image)\n",
    "print(\"Input Image shape\",image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Image shape torch.Size([2, 8, 32, 256, 256])\n",
      "Input Image shape torch.Size([16, 1, 32, 256, 256])\n",
      "Image embedding shape torch.Size([16, 256, 2560])\n",
      "Image embedding shape torch.Size([2, 8, 256, 2560])\n"
     ]
    }
   ],
   "source": [
    "# B for batch size, C for channel, D for depth, H for height, W for width\n",
    "B, C, D, H, W = image.shape\n",
    "print(\"Input Image shape\",image.shape)\n",
    "image = image.view(B * C, 1, image.shape[-3], image.shape[-2], image.shape[-1])\n",
    "print(\"Input Image shape\",image.shape)\n",
    "img_emb = lamed_model.encode_images(image)\n",
    "print(\"Image embedding shape\",img_emb.shape)\n",
    "img_emb = img_emb.view(B, C, img_emb.shape[-2], img_emb.shape[-1])\n",
    "print(\"Image embedding shape\",img_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.linear_3d_tokenizer.lin3dt import Linear3DTokenizer\n",
    "embed_size = 2560\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "top_k = 1024\n",
    "l3d_tokenizer = Linear3DTokenizer(\n",
    "    embed_size=embed_size,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    top_k=top_k,\n",
    "    use_multi_scale=True,\n",
    "    num_3d_query_token=256,\n",
    "    hidden_size=2560\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We need to remove 6 to truncate the input but the first sequence has a length 5. \n",
      "We need to remove 6 to truncate the input but the first sequence has a length 5. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text token shape torch.Size([2, 5])\n",
      "Text token shape torch.Size([2, 5, 2560])\n"
     ]
    }
   ],
   "source": [
    "t_token = tokenizer([\"This is a test sentence\",\"This is a test sentence\"], add_special_tokens=False, max_length=-1, truncation=True, padding=\"max_length\", return_tensors=\"pt\", padding_side=\"right\")[\"input_ids\"]\n",
    "print(\"Text token shape\",t_token.shape)\n",
    "t_token = lamed_model.model.embed_tokens(t_token)\n",
    "print(\"Text token shape\",t_token.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape torch.Size([1, 256, 2560])\n"
     ]
    }
   ],
   "source": [
    "num_video_query_token = 8\n",
    "hidden_size = 2560\n",
    "query_tokens = torch.randn(2, 256, 2560)\n",
    "\n",
    "output = l3d_tokenizer(v_token=img_emb, t_token=t_token)\n",
    "l3d_tokenizer\n",
    "print(\"Output shape\",output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metatensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<AliasBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_emb[0] - img_emb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.linear_3d_tokenizer.svr import SpatioTemporalVisualTokenRefinerModel\n",
    "from src.model.linear_3d_tokenizer.tta import TextConditionTokenAggregatorModel\n",
    "embed_size = 2560\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "top_k = 1024\n",
    "svr_model = SpatioTemporalVisualTokenRefinerModel(embed_size=embed_size, num_heads=num_heads, num_layers=num_layers, top_k=top_k, use_multi_scale=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1792, 2560])\n"
     ]
    }
   ],
   "source": [
    "# Example input: (batch_size, num_frames, num_tokens, embed_size)\n",
    "video_data = torch.randn(1, 15, 256, embed_size)  \n",
    "svr_output = svr_model(video_data)\n",
    "print(svr_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SpatioTemporalLayer(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super().__init__()\n",
    "        # Spatial attention: attends to tokens within each frame.\n",
    "        self.spatial_attn = nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n",
    "        # Temporal attention: attends across frames for each token position.\n",
    "        self.temporal_attn = nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n",
    "        self.norm_spatial = nn.LayerNorm(embed_size)\n",
    "        self.norm_temporal = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, T, N, E)\n",
    "        B, T, N, E = x.shape\n",
    "\n",
    "        # --- Spatial Attention ---\n",
    "        # Reshape to combine batch and frame dimensions: (B*T, N, E)\n",
    "        x_spatial = x.view(B * T, N, E)\n",
    "        attn_out, _ = self.spatial_attn(x_spatial, x_spatial, x_spatial)\n",
    "        attn_out = attn_out.view(B, T, N, E)\n",
    "        x = self.norm_spatial(x + attn_out)\n",
    "\n",
    "        # --- Temporal Attention ---\n",
    "        # Permute so that tokens (N) become batch “instances” over T frames:\n",
    "        # (B, N, T, E) then reshape to (B*N, T, E)\n",
    "        x_temporal = x.permute(0, 2, 1, 3).contiguous().view(B * N, T, E)\n",
    "        attn_out, _ = self.temporal_attn(x_temporal, x_temporal, x_temporal)\n",
    "        # Restore shape: (B, N, T, E) then permute back to (B, T, N, E)\n",
    "        attn_out = attn_out.view(B, N, T, E).permute(0, 2, 1, 3)\n",
    "        x = self.norm_temporal(x + attn_out)\n",
    "        return x\n",
    "\n",
    "class SpatioTemporalTokenRefiner(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, num_layers, top_k, use_multi_scale):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [SpatioTemporalLayer(embed_size, num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "        # Linear layer to score the significance of each token.\n",
    "        self.score_fc = nn.Linear(embed_size, 1)\n",
    "        self.top_k = top_k\n",
    "        self.use_multi_scale = use_multi_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, T, N, E)\n",
    "        B, T, N, E = x.shape\n",
    "\n",
    "        # Apply a stack of spatio–temporal attention layers.\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Compute significance scores for each token.\n",
    "        # Resulting shape: (B, T, N)\n",
    "        scores = self.score_fc(x).squeeze(-1)\n",
    "        # Flatten frame and token dimensions: (B, T*N)\n",
    "        scores = scores.view(B, -1)\n",
    "        # Select top_k tokens (indices are flattened over T*N).\n",
    "        _, topk_indices = torch.topk(scores, self.top_k, dim=1)\n",
    "        # Convert flat indices back into frame and token indices.\n",
    "        frame_indices = topk_indices // N  # integer division\n",
    "        token_indices = topk_indices % N\n",
    "\n",
    "        # Gather the top_k tokens for each batch.\n",
    "        batch_idx = torch.arange(B, device=x.device).unsqueeze(1)\n",
    "        selected_tokens = x[batch_idx, frame_indices, token_indices]  # (B, top_k, E)\n",
    "        print(selected_tokens.shape)\n",
    "        # Optionally apply multi–scale pooling over the token sequence.\n",
    "        if self.use_multi_scale:\n",
    "            pooled_tokens = []\n",
    "            # Example scales (kernel sizes) for 1D pooling.\n",
    "            for scale in [1, 2, 4]:\n",
    "                if selected_tokens.size(1) >= scale:\n",
    "                    # Pool along the token (sequence) dimension.\n",
    "                    pooled = F.avg_pool1d(selected_tokens.transpose(1, 2),\n",
    "                                          kernel_size=scale, stride=scale)\n",
    "                    pooled = pooled.transpose(1, 2)  # shape: (B, new_tokens, E)\n",
    "                    pooled_tokens.append(pooled)\n",
    "            # Concatenate pooled outputs along the token dimension.\n",
    "            selected_tokens = torch.cat(pooled_tokens, dim=1)\n",
    "\n",
    "        return selected_tokens  # (B, S, E), with S depending on top_k and pooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024, 2560])\n",
      "torch.Size([2, 1792, 2560])\n"
     ]
    }
   ],
   "source": [
    "svr_model = SpatioTemporalTokenRefiner(\n",
    "    embed_size=embed_size, \n",
    "    num_heads=num_heads, \n",
    "    num_layers=num_layers, \n",
    "    top_k=top_k, \n",
    "    use_multi_scale=True\n",
    "    )\n",
    "\n",
    "# Example input: (batch_size, num_frames, num_tokens, embed_size)\n",
    "video_data = torch.randn(2, 15, 256, embed_size)  \n",
    "svr_output = svr_model(video_data)\n",
    "print(svr_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1792, 2560])\n"
     ]
    }
   ],
   "source": [
    "svr_model = SpatioTemporalTokenRefiner(\n",
    "    embed_size=embed_size, \n",
    "    num_heads=num_heads, \n",
    "    num_layers=num_layers, \n",
    "    top_k=top_k, \n",
    "    use_multi_scale=True\n",
    "    )\n",
    "\n",
    "# Example input: (batch_size, num_frames, num_tokens, embed_size)\n",
    "video_data = torch.randn(1, 15, 256, embed_size)  \n",
    "svr_output = svr_model(video_data)\n",
    "print(svr_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "num_layers = 4\n",
    "tta_model = TextConditionedAggregator(embed_size, num_layers, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 2560])\n"
     ]
    }
   ],
   "source": [
    "query = torch.randn(2, 256, embed_size)\n",
    "# Example input: (batch_size, num_tokens, embed_size)\n",
    "visual_data = torch.randn(2, 1792, embed_size)\n",
    "text_data = torch.randn(2, 600, embed_size)\n",
    "output = tta_model(query, visual_data, text_data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# 2. Text–Conditioned Token Aggregation\n",
    "##########################################\n",
    "class AggregationLayer(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super().__init__()\n",
    "        # Even if the query is a single token, we use self–attention for consistency.\n",
    "        self.self_attn = nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n",
    "        # Cross–attention layers with visual tokens and text tokens.\n",
    "        self.cross_attn_visual = nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n",
    "        self.cross_attn_text = nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, query, visual_tokens, text_tokens):\n",
    "        # query shape: (B, 1, E)\n",
    "        # Self–attention on the query.\n",
    "        self_attn_out, _ = self.self_attn(query, query, query)\n",
    "        query = self.norm(query + self_attn_out)\n",
    "\n",
    "        # Cross–attention with visual tokens.\n",
    "        cross_out, _ = self.cross_attn_visual(query, visual_tokens, visual_tokens)\n",
    "        query = self.norm(query + cross_out)\n",
    "\n",
    "        # Cross–attention with text tokens.\n",
    "        cross_out, _ = self.cross_attn_text(query, text_tokens, text_tokens)\n",
    "        query = self.norm(query + cross_out)\n",
    "        return query  # (B, 1, E)\n",
    "\n",
    "class TextConditionedAggregator(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [AggregationLayer(embed_size, num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "        # Final cross–attention layer to “compress” the visual tokens.\n",
    "        self.final_attn = nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n",
    "        self.norm_final = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, query, visual_tokens, text_tokens):\n",
    "        # query: (B, 1, E); visual_tokens: (B, S, E); text_tokens: (B, L, E)\n",
    "        out = query\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, visual_tokens, text_tokens)\n",
    "        # Final cross–attention with visual tokens.\n",
    "        attn_out, _ = self.final_attn(out, visual_tokens, visual_tokens)\n",
    "        out = self.norm_final(out + attn_out)\n",
    "        # Squeeze the sequence dimension (which is 1) so output shape becomes (B, E)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "##########################################\n",
    "# 3. Combined Linear3DTokenizer Module\n",
    "##########################################\n",
    "class Linear3DTokenizer(nn.Module):\n",
    "    \"\"\"\n",
    "    This module takes the CT image tokens (from a ViT3D model) and compresses\n",
    "    them into a compact visual token. It then aggregates that with text tokens\n",
    "    (and a visual query token) to produce a final token for the LLM.\n",
    "    \n",
    "    Input shapes:\n",
    "      - v_token: (B, T, N, E)  (CT image embedding tokens over frames and patches)\n",
    "      - v_query: (B, 1, E)      (a visual query token)\n",
    "      - t_token: (B, L, E)      (text tokens from a prompt or caption)\n",
    "      \n",
    "    Output shape:\n",
    "      - align_token: (B, E)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, num_heads, num_layers, top_k, use_multi_scale):\n",
    "        super().__init__()\n",
    "        self.visual_token_refiner = SpatioTemporalTokenRefiner(\n",
    "            embed_size, num_heads, num_layers, top_k, use_multi_scale\n",
    "        )\n",
    "        self.text_conditioned_aggregator = TextConditionedAggregator(\n",
    "            embed_size, num_heads, num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, v_query, v_token, t_token):\n",
    "        # First, refine (compress) the visual tokens from the CT image.\n",
    "        # v_token: (B, T, N, E) --> visual_tokens: (B, S, E)\n",
    "        visual_tokens = self.visual_token_refiner(v_token)\n",
    "        # Then aggregate the visual query, refined visual tokens, and text tokens.\n",
    "        # v_query: (B, 1, E), t_token: (B, L, E) --> align_token: (B, E)\n",
    "        align_token = self.text_conditioned_aggregator(v_query, visual_tokens, t_token)\n",
    "        return align_token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "green_score",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
