{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    # Process of elimination: LoRA only targets on LLM backbone\n",
    "    ignore_keywords = ['vision_tower', 'mm_projector', 'embed_tokens', 'lm_head', 'seg_projector', 'seg_module']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in ignore_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            lora_module_names.add(name)\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.blocks.patchembedding PatchEmbeddingBlock.__init__:pos_embed: Argument `pos_embed` has been deprecated since version 1.2. It will be removed in version 1.4. please use `proj_type` instead.\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "Some weights of the model checkpoint at /import/c4dm-04/siyoul/Med3DLLM/checkpoint/amosmm_chatgpt_stage_1/checkpoint-100080 were not used when initializing LamedPhi3ForCausalLM: ['base_model.model.lm_head.weight', 'base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.input_layernorm.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.input_layernorm.weight', 'base_model.model.model.layers.1.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.input_layernorm.weight', 'base_model.model.model.layers.10.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.input_layernorm.weight', 'base_model.model.model.layers.11.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.input_layernorm.weight', 'base_model.model.model.layers.12.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.input_layernorm.weight', 'base_model.model.model.layers.13.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.input_layernorm.weight', 'base_model.model.model.layers.14.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.input_layernorm.weight', 'base_model.model.model.layers.15.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.input_layernorm.weight', 'base_model.model.model.layers.16.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.input_layernorm.weight', 'base_model.model.model.layers.17.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.input_layernorm.weight', 'base_model.model.model.layers.18.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.input_layernorm.weight', 'base_model.model.model.layers.19.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.input_layernorm.weight', 'base_model.model.model.layers.2.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.input_layernorm.weight', 'base_model.model.model.layers.20.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.input_layernorm.weight', 'base_model.model.model.layers.21.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.input_layernorm.weight', 'base_model.model.model.layers.22.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.input_layernorm.weight', 'base_model.model.model.layers.23.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.input_layernorm.weight', 'base_model.model.model.layers.24.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.input_layernorm.weight', 'base_model.model.model.layers.25.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.input_layernorm.weight', 'base_model.model.model.layers.26.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.input_layernorm.weight', 'base_model.model.model.layers.27.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.input_layernorm.weight', 'base_model.model.model.layers.28.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.input_layernorm.weight', 'base_model.model.model.layers.29.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.input_layernorm.weight', 'base_model.model.model.layers.3.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.input_layernorm.weight', 'base_model.model.model.layers.30.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.input_layernorm.weight', 'base_model.model.model.layers.31.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.input_layernorm.weight', 'base_model.model.model.layers.4.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.input_layernorm.weight', 'base_model.model.model.layers.5.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.input_layernorm.weight', 'base_model.model.model.layers.6.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.input_layernorm.weight', 'base_model.model.model.layers.7.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.input_layernorm.weight', 'base_model.model.model.layers.8.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.input_layernorm.weight', 'base_model.model.model.layers.9.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.query_tokens', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.token_selection.score_net.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.token_selection.score_net.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.token_selection.score_net.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.token_selection.score_net.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_t.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_t.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_v.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_v.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_self.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_self.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_t.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_t.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_v.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_v.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_self.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_self.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_t.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_t.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_v.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_v.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_self.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_self.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_t.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_t.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_v.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_v.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_self.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_self.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.mm_projector.projector.0.bias', 'base_model.model.model.mm_projector.projector.0.weight', 'base_model.model.model.mm_projector.projector.2.bias', 'base_model.model.model.mm_projector.projector.2.weight', 'base_model.model.model.norm.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.0.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.0.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.0.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.0.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.0.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.0.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.0.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.0.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.0.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.0.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.1.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.1.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.1.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.1.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.1.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.1.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.1.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.1.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.1.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.1.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.10.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.10.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.10.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.10.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.10.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.10.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.10.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.10.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.10.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.10.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.11.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.11.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.11.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.11.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.11.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.11.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.11.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.11.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.11.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.11.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.2.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.2.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.2.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.2.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.2.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.2.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.2.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.2.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.2.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.2.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.3.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.3.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.3.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.3.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.3.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.3.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.3.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.3.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.3.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.3.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.4.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.4.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.4.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.4.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.4.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.4.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.4.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.4.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.4.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.4.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.5.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.5.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.5.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.5.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.5.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.5.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.5.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.5.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.5.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.5.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.6.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.6.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.6.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.6.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.6.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.6.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.6.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.6.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.6.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.6.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.7.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.7.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.7.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.7.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.7.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.7.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.7.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.7.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.7.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.7.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.8.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.8.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.8.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.8.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.8.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.8.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.8.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.8.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.8.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.8.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.9.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.9.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.9.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.9.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.9.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.9.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.9.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.9.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.9.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.9.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.cls_token', 'base_model.model.model.vision_tower.vision_tower.norm.bias', 'base_model.model.model.vision_tower.vision_tower.norm.weight', 'base_model.model.model.vision_tower.vision_tower.patch_embedding.patch_embeddings.1.bias', 'base_model.model.model.vision_tower.vision_tower.patch_embedding.patch_embeddings.1.weight', 'base_model.model.model.vision_tower.vision_tower.patch_embedding.position_embeddings']\n",
      "- This IS expected if you are initializing LamedPhi3ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LamedPhi3ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LamedPhi3ForCausalLM were not initialized from the model checkpoint at /import/c4dm-04/siyoul/Med3DLLM/checkpoint/amosmm_chatgpt_stage_1/checkpoint-100080 and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.qkv_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.qkv_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.qkv_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.qkv_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.qkv_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.qkv_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.qkv_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.qkv_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.16.mlp.gate_up_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.qkv_proj.weight', 'layers.17.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.17.mlp.gate_up_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.qkv_proj.weight', 'layers.18.input_layernorm.weight', 'layers.18.mlp.down_proj.weight', 'layers.18.mlp.gate_up_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.18.self_attn.o_proj.weight', 'layers.18.self_attn.qkv_proj.weight', 'layers.19.input_layernorm.weight', 'layers.19.mlp.down_proj.weight', 'layers.19.mlp.gate_up_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.19.self_attn.o_proj.weight', 'layers.19.self_attn.qkv_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.qkv_proj.weight', 'layers.20.input_layernorm.weight', 'layers.20.mlp.down_proj.weight', 'layers.20.mlp.gate_up_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.20.self_attn.o_proj.weight', 'layers.20.self_attn.qkv_proj.weight', 'layers.21.input_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.21.mlp.gate_up_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.qkv_proj.weight', 'layers.22.input_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.22.mlp.gate_up_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.22.self_attn.o_proj.weight', 'layers.22.self_attn.qkv_proj.weight', 'layers.23.input_layernorm.weight', 'layers.23.mlp.down_proj.weight', 'layers.23.mlp.gate_up_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.23.self_attn.o_proj.weight', 'layers.23.self_attn.qkv_proj.weight', 'layers.24.input_layernorm.weight', 'layers.24.mlp.down_proj.weight', 'layers.24.mlp.gate_up_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.self_attn.qkv_proj.weight', 'layers.25.input_layernorm.weight', 'layers.25.mlp.down_proj.weight', 'layers.25.mlp.gate_up_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.25.self_attn.o_proj.weight', 'layers.25.self_attn.qkv_proj.weight', 'layers.26.input_layernorm.weight', 'layers.26.mlp.down_proj.weight', 'layers.26.mlp.gate_up_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.26.self_attn.o_proj.weight', 'layers.26.self_attn.qkv_proj.weight', 'layers.27.input_layernorm.weight', 'layers.27.mlp.down_proj.weight', 'layers.27.mlp.gate_up_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.27.self_attn.o_proj.weight', 'layers.27.self_attn.qkv_proj.weight', 'layers.28.input_layernorm.weight', 'layers.28.mlp.down_proj.weight', 'layers.28.mlp.gate_up_proj.weight', 'layers.28.post_attention_layernorm.weight', 'layers.28.self_attn.o_proj.weight', 'layers.28.self_attn.qkv_proj.weight', 'layers.29.input_layernorm.weight', 'layers.29.mlp.down_proj.weight', 'layers.29.mlp.gate_up_proj.weight', 'layers.29.post_attention_layernorm.weight', 'layers.29.self_attn.o_proj.weight', 'layers.29.self_attn.qkv_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.qkv_proj.weight', 'layers.30.input_layernorm.weight', 'layers.30.mlp.down_proj.weight', 'layers.30.mlp.gate_up_proj.weight', 'layers.30.post_attention_layernorm.weight', 'layers.30.self_attn.o_proj.weight', 'layers.30.self_attn.qkv_proj.weight', 'layers.31.input_layernorm.weight', 'layers.31.mlp.down_proj.weight', 'layers.31.mlp.gate_up_proj.weight', 'layers.31.post_attention_layernorm.weight', 'layers.31.self_attn.o_proj.weight', 'layers.31.self_attn.qkv_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.qkv_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.qkv_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.qkv_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.qkv_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.qkv_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.qkv_proj.weight', 'linear3d_tokenizer.query_tokens', 'linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.token_selection.score_net.bias', 'linear3d_tokenizer.svt_module.token_selection.score_net.weight', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.bias', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.weight', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.bias', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.weight', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.bias', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.weight', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.bias', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_t.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_t.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_v.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_v.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.norm_self.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.norm_self.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.self_attention.in_proj_bias', 'linear3d_tokenizer.tta_module.layers_vt.0.self_attention.in_proj_weight', 'linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_t.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_t.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_v.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_v.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.norm_self.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.norm_self.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.self_attention.in_proj_bias', 'linear3d_tokenizer.tta_module.layers_vt.1.self_attention.in_proj_weight', 'linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_t.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_t.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_v.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_v.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.norm_self.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.norm_self.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.self_attention.in_proj_bias', 'linear3d_tokenizer.tta_module.layers_vt.2.self_attention.in_proj_weight', 'linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_t.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_t.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_v.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_v.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.norm_self.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.norm_self.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.self_attention.in_proj_bias', 'linear3d_tokenizer.tta_module.layers_vt.3.self_attention.in_proj_weight', 'linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.weight', 'lm_head.weight', 'mm_projector.projector.0.bias', 'mm_projector.projector.0.weight', 'mm_projector.projector.2.bias', 'mm_projector.projector.2.weight', 'norm.weight', 'vision_tower.vision_tower.blocks.0.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.0.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'vision_tower.vision_tower.blocks.0.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.0.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.0.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.0.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.0.norm1.bias', 'vision_tower.vision_tower.blocks.0.norm1.weight', 'vision_tower.vision_tower.blocks.0.norm2.bias', 'vision_tower.vision_tower.blocks.0.norm2.weight', 'vision_tower.vision_tower.blocks.1.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.1.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'vision_tower.vision_tower.blocks.1.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.1.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.1.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.1.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.1.norm1.bias', 'vision_tower.vision_tower.blocks.1.norm1.weight', 'vision_tower.vision_tower.blocks.1.norm2.bias', 'vision_tower.vision_tower.blocks.1.norm2.weight', 'vision_tower.vision_tower.blocks.10.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.10.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'vision_tower.vision_tower.blocks.10.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.10.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.10.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.10.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.10.norm1.bias', 'vision_tower.vision_tower.blocks.10.norm1.weight', 'vision_tower.vision_tower.blocks.10.norm2.bias', 'vision_tower.vision_tower.blocks.10.norm2.weight', 'vision_tower.vision_tower.blocks.11.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.11.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'vision_tower.vision_tower.blocks.11.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.11.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.11.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.11.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.11.norm1.bias', 'vision_tower.vision_tower.blocks.11.norm1.weight', 'vision_tower.vision_tower.blocks.11.norm2.bias', 'vision_tower.vision_tower.blocks.11.norm2.weight', 'vision_tower.vision_tower.blocks.2.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.2.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'vision_tower.vision_tower.blocks.2.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.2.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.2.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.2.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.2.norm1.bias', 'vision_tower.vision_tower.blocks.2.norm1.weight', 'vision_tower.vision_tower.blocks.2.norm2.bias', 'vision_tower.vision_tower.blocks.2.norm2.weight', 'vision_tower.vision_tower.blocks.3.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.3.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'vision_tower.vision_tower.blocks.3.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.3.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.3.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.3.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.3.norm1.bias', 'vision_tower.vision_tower.blocks.3.norm1.weight', 'vision_tower.vision_tower.blocks.3.norm2.bias', 'vision_tower.vision_tower.blocks.3.norm2.weight', 'vision_tower.vision_tower.blocks.4.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.4.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'vision_tower.vision_tower.blocks.4.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.4.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.4.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.4.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.4.norm1.bias', 'vision_tower.vision_tower.blocks.4.norm1.weight', 'vision_tower.vision_tower.blocks.4.norm2.bias', 'vision_tower.vision_tower.blocks.4.norm2.weight', 'vision_tower.vision_tower.blocks.5.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.5.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'vision_tower.vision_tower.blocks.5.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.5.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.5.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.5.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.5.norm1.bias', 'vision_tower.vision_tower.blocks.5.norm1.weight', 'vision_tower.vision_tower.blocks.5.norm2.bias', 'vision_tower.vision_tower.blocks.5.norm2.weight', 'vision_tower.vision_tower.blocks.6.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.6.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'vision_tower.vision_tower.blocks.6.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.6.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.6.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.6.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.6.norm1.bias', 'vision_tower.vision_tower.blocks.6.norm1.weight', 'vision_tower.vision_tower.blocks.6.norm2.bias', 'vision_tower.vision_tower.blocks.6.norm2.weight', 'vision_tower.vision_tower.blocks.7.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.7.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'vision_tower.vision_tower.blocks.7.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.7.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.7.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.7.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.7.norm1.bias', 'vision_tower.vision_tower.blocks.7.norm1.weight', 'vision_tower.vision_tower.blocks.7.norm2.bias', 'vision_tower.vision_tower.blocks.7.norm2.weight', 'vision_tower.vision_tower.blocks.8.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.8.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'vision_tower.vision_tower.blocks.8.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.8.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.8.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.8.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.8.norm1.bias', 'vision_tower.vision_tower.blocks.8.norm1.weight', 'vision_tower.vision_tower.blocks.8.norm2.bias', 'vision_tower.vision_tower.blocks.8.norm2.weight', 'vision_tower.vision_tower.blocks.9.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.9.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'vision_tower.vision_tower.blocks.9.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.9.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.9.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.9.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.9.norm1.bias', 'vision_tower.vision_tower.blocks.9.norm1.weight', 'vision_tower.vision_tower.blocks.9.norm2.bias', 'vision_tower.vision_tower.blocks.9.norm2.weight', 'vision_tower.vision_tower.cls_token', 'vision_tower.vision_tower.norm.bias', 'vision_tower.vision_tower.norm.weight', 'vision_tower.vision_tower.patch_embedding.patch_embeddings.1.bias', 'vision_tower.vision_tower.patch_embedding.patch_embeddings.1.weight', 'vision_tower.vision_tower.patch_embedding.position_embeddings']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') # 'cpu', 'cuda'\n",
    "dtype = torch.bfloat16 # or bfloat16, float16, float32\n",
    "\n",
    "model_name_or_path = '/import/c4dm-04/siyoul/Med3DLLM/checkpoint/amosmm_chatgpt_stage_1/checkpoint-100080'\n",
    "proj_out_num = 256\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=dtype,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    model_max_length=512,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.blocks.patchembedding PatchEmbeddingBlock.__init__:pos_embed: Argument `pos_embed` has been deprecated since version 1.2. It will be removed in version 1.4. please use `proj_type` instead.\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "Some weights of the model checkpoint at /import/c4dm-04/siyoul/Med3DLLM/checkpoint/amosmm_chatgpt_stage_1/checkpoint-100080 were not used when initializing LamedPhi3ForCausalLM: ['base_model.model.lm_head.weight', 'base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.input_layernorm.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.1.input_layernorm.weight', 'base_model.model.model.layers.1.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.10.input_layernorm.weight', 'base_model.model.model.layers.10.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.11.input_layernorm.weight', 'base_model.model.model.layers.11.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.12.input_layernorm.weight', 'base_model.model.model.layers.12.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.13.input_layernorm.weight', 'base_model.model.model.layers.13.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.14.input_layernorm.weight', 'base_model.model.model.layers.14.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.15.input_layernorm.weight', 'base_model.model.model.layers.15.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.16.input_layernorm.weight', 'base_model.model.model.layers.16.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.17.input_layernorm.weight', 'base_model.model.model.layers.17.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.18.input_layernorm.weight', 'base_model.model.model.layers.18.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.19.input_layernorm.weight', 'base_model.model.model.layers.19.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.2.input_layernorm.weight', 'base_model.model.model.layers.2.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.20.input_layernorm.weight', 'base_model.model.model.layers.20.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.21.input_layernorm.weight', 'base_model.model.model.layers.21.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.22.input_layernorm.weight', 'base_model.model.model.layers.22.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.23.input_layernorm.weight', 'base_model.model.model.layers.23.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.24.input_layernorm.weight', 'base_model.model.model.layers.24.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.24.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.24.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.25.input_layernorm.weight', 'base_model.model.model.layers.25.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.25.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.25.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.26.input_layernorm.weight', 'base_model.model.model.layers.26.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.26.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.26.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.27.input_layernorm.weight', 'base_model.model.model.layers.27.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.27.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.27.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.28.input_layernorm.weight', 'base_model.model.model.layers.28.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.28.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.28.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.29.input_layernorm.weight', 'base_model.model.model.layers.29.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.29.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.29.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.3.input_layernorm.weight', 'base_model.model.model.layers.3.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.30.input_layernorm.weight', 'base_model.model.model.layers.30.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.30.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.30.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.31.input_layernorm.weight', 'base_model.model.model.layers.31.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.31.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.31.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.4.input_layernorm.weight', 'base_model.model.model.layers.4.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.5.input_layernorm.weight', 'base_model.model.model.layers.5.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.6.input_layernorm.weight', 'base_model.model.model.layers.6.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.7.input_layernorm.weight', 'base_model.model.model.layers.7.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.8.input_layernorm.weight', 'base_model.model.model.layers.8.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.layers.9.input_layernorm.weight', 'base_model.model.model.layers.9.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_up_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.gate_up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.query_tokens', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.token_selection.score_net.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.svt_module.token_selection.score_net.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.token_selection.score_net.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.svt_module.token_selection.score_net.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_t.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_t.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_v.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_v.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_self.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_self.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_t.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_t.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_v.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_v.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_self.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_self.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_t.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_t.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_v.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_v.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_self.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_self.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_t.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_t.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_v.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_v.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_self.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_self.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.in_proj_bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.in_proj_weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.lora_B.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.base_layer.bias', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.base_layer.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.lora_A.default.weight', 'base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.lora_B.default.weight', 'base_model.model.model.mm_projector.projector.0.bias', 'base_model.model.model.mm_projector.projector.0.weight', 'base_model.model.model.mm_projector.projector.2.bias', 'base_model.model.model.mm_projector.projector.2.weight', 'base_model.model.model.norm.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.0.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.0.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.0.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.0.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.0.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.0.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.0.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.0.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.0.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.0.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.1.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.1.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.1.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.1.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.1.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.1.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.1.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.1.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.1.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.1.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.10.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.10.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.10.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.10.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.10.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.10.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.10.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.10.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.10.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.10.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.11.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.11.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.11.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.11.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.11.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.11.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.11.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.11.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.11.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.11.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.2.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.2.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.2.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.2.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.2.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.2.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.2.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.2.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.2.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.2.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.3.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.3.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.3.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.3.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.3.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.3.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.3.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.3.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.3.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.3.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.4.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.4.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.4.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.4.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.4.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.4.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.4.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.4.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.4.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.4.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.5.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.5.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.5.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.5.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.5.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.5.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.5.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.5.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.5.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.5.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.6.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.6.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.6.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.6.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.6.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.6.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.6.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.6.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.6.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.6.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.7.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.7.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.7.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.7.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.7.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.7.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.7.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.7.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.7.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.7.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.8.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.8.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.8.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.8.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.8.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.8.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.8.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.8.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.8.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.8.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.9.attn.out_proj.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.9.attn.out_proj.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.9.mlp.linear1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.9.mlp.linear1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.9.mlp.linear2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.9.mlp.linear2.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.9.norm1.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.9.norm1.weight', 'base_model.model.model.vision_tower.vision_tower.blocks.9.norm2.bias', 'base_model.model.model.vision_tower.vision_tower.blocks.9.norm2.weight', 'base_model.model.model.vision_tower.vision_tower.cls_token', 'base_model.model.model.vision_tower.vision_tower.norm.bias', 'base_model.model.model.vision_tower.vision_tower.norm.weight', 'base_model.model.model.vision_tower.vision_tower.patch_embedding.patch_embeddings.1.bias', 'base_model.model.model.vision_tower.vision_tower.patch_embedding.patch_embeddings.1.weight', 'base_model.model.model.vision_tower.vision_tower.patch_embedding.position_embeddings']\n",
      "- This IS expected if you are initializing LamedPhi3ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LamedPhi3ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LamedPhi3ForCausalLM were not initialized from the model checkpoint at /import/c4dm-04/siyoul/Med3DLLM/checkpoint/amosmm_chatgpt_stage_1/checkpoint-100080 and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.qkv_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.qkv_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.qkv_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.qkv_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.qkv_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.qkv_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.qkv_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.qkv_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.16.mlp.gate_up_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.qkv_proj.weight', 'layers.17.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.17.mlp.gate_up_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.qkv_proj.weight', 'layers.18.input_layernorm.weight', 'layers.18.mlp.down_proj.weight', 'layers.18.mlp.gate_up_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.18.self_attn.o_proj.weight', 'layers.18.self_attn.qkv_proj.weight', 'layers.19.input_layernorm.weight', 'layers.19.mlp.down_proj.weight', 'layers.19.mlp.gate_up_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.19.self_attn.o_proj.weight', 'layers.19.self_attn.qkv_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.qkv_proj.weight', 'layers.20.input_layernorm.weight', 'layers.20.mlp.down_proj.weight', 'layers.20.mlp.gate_up_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.20.self_attn.o_proj.weight', 'layers.20.self_attn.qkv_proj.weight', 'layers.21.input_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.21.mlp.gate_up_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.qkv_proj.weight', 'layers.22.input_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.22.mlp.gate_up_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.22.self_attn.o_proj.weight', 'layers.22.self_attn.qkv_proj.weight', 'layers.23.input_layernorm.weight', 'layers.23.mlp.down_proj.weight', 'layers.23.mlp.gate_up_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.23.self_attn.o_proj.weight', 'layers.23.self_attn.qkv_proj.weight', 'layers.24.input_layernorm.weight', 'layers.24.mlp.down_proj.weight', 'layers.24.mlp.gate_up_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.self_attn.qkv_proj.weight', 'layers.25.input_layernorm.weight', 'layers.25.mlp.down_proj.weight', 'layers.25.mlp.gate_up_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.25.self_attn.o_proj.weight', 'layers.25.self_attn.qkv_proj.weight', 'layers.26.input_layernorm.weight', 'layers.26.mlp.down_proj.weight', 'layers.26.mlp.gate_up_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.26.self_attn.o_proj.weight', 'layers.26.self_attn.qkv_proj.weight', 'layers.27.input_layernorm.weight', 'layers.27.mlp.down_proj.weight', 'layers.27.mlp.gate_up_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.27.self_attn.o_proj.weight', 'layers.27.self_attn.qkv_proj.weight', 'layers.28.input_layernorm.weight', 'layers.28.mlp.down_proj.weight', 'layers.28.mlp.gate_up_proj.weight', 'layers.28.post_attention_layernorm.weight', 'layers.28.self_attn.o_proj.weight', 'layers.28.self_attn.qkv_proj.weight', 'layers.29.input_layernorm.weight', 'layers.29.mlp.down_proj.weight', 'layers.29.mlp.gate_up_proj.weight', 'layers.29.post_attention_layernorm.weight', 'layers.29.self_attn.o_proj.weight', 'layers.29.self_attn.qkv_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.qkv_proj.weight', 'layers.30.input_layernorm.weight', 'layers.30.mlp.down_proj.weight', 'layers.30.mlp.gate_up_proj.weight', 'layers.30.post_attention_layernorm.weight', 'layers.30.self_attn.o_proj.weight', 'layers.30.self_attn.qkv_proj.weight', 'layers.31.input_layernorm.weight', 'layers.31.mlp.down_proj.weight', 'layers.31.mlp.gate_up_proj.weight', 'layers.31.post_attention_layernorm.weight', 'layers.31.self_attn.o_proj.weight', 'layers.31.self_attn.qkv_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.qkv_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.qkv_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.qkv_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.qkv_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.qkv_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.qkv_proj.weight', 'linear3d_tokenizer.query_tokens', 'linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.in_proj_bias', 'linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.in_proj_weight', 'linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.bias', 'linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.weight', 'linear3d_tokenizer.svt_module.token_selection.score_net.bias', 'linear3d_tokenizer.svt_module.token_selection.score_net.weight', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.bias', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.weight', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.bias', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.weight', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.bias', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.weight', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.bias', 'linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_t.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_t.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_v.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_v.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.norm_self.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.norm_self.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.self_attention.in_proj_bias', 'linear3d_tokenizer.tta_module.layers_vt.0.self_attention.in_proj_weight', 'linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_t.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_t.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_v.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_v.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.norm_self.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.norm_self.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.self_attention.in_proj_bias', 'linear3d_tokenizer.tta_module.layers_vt.1.self_attention.in_proj_weight', 'linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_t.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_t.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_v.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_v.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.norm_self.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.norm_self.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.self_attention.in_proj_bias', 'linear3d_tokenizer.tta_module.layers_vt.2.self_attention.in_proj_weight', 'linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_t.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_t.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_v.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_v.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.norm_self.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.norm_self.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.self_attention.in_proj_bias', 'linear3d_tokenizer.tta_module.layers_vt.3.self_attention.in_proj_weight', 'linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.weight', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.bias', 'linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.weight', 'lm_head.weight', 'mm_projector.projector.0.bias', 'mm_projector.projector.0.weight', 'mm_projector.projector.2.bias', 'mm_projector.projector.2.weight', 'norm.weight', 'vision_tower.vision_tower.blocks.0.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.0.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.0.attn.qkv.weight', 'vision_tower.vision_tower.blocks.0.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.0.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.0.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.0.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.0.norm1.bias', 'vision_tower.vision_tower.blocks.0.norm1.weight', 'vision_tower.vision_tower.blocks.0.norm2.bias', 'vision_tower.vision_tower.blocks.0.norm2.weight', 'vision_tower.vision_tower.blocks.1.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.1.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.1.attn.qkv.weight', 'vision_tower.vision_tower.blocks.1.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.1.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.1.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.1.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.1.norm1.bias', 'vision_tower.vision_tower.blocks.1.norm1.weight', 'vision_tower.vision_tower.blocks.1.norm2.bias', 'vision_tower.vision_tower.blocks.1.norm2.weight', 'vision_tower.vision_tower.blocks.10.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.10.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.10.attn.qkv.weight', 'vision_tower.vision_tower.blocks.10.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.10.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.10.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.10.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.10.norm1.bias', 'vision_tower.vision_tower.blocks.10.norm1.weight', 'vision_tower.vision_tower.blocks.10.norm2.bias', 'vision_tower.vision_tower.blocks.10.norm2.weight', 'vision_tower.vision_tower.blocks.11.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.11.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.11.attn.qkv.weight', 'vision_tower.vision_tower.blocks.11.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.11.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.11.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.11.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.11.norm1.bias', 'vision_tower.vision_tower.blocks.11.norm1.weight', 'vision_tower.vision_tower.blocks.11.norm2.bias', 'vision_tower.vision_tower.blocks.11.norm2.weight', 'vision_tower.vision_tower.blocks.2.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.2.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.2.attn.qkv.weight', 'vision_tower.vision_tower.blocks.2.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.2.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.2.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.2.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.2.norm1.bias', 'vision_tower.vision_tower.blocks.2.norm1.weight', 'vision_tower.vision_tower.blocks.2.norm2.bias', 'vision_tower.vision_tower.blocks.2.norm2.weight', 'vision_tower.vision_tower.blocks.3.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.3.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.3.attn.qkv.weight', 'vision_tower.vision_tower.blocks.3.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.3.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.3.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.3.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.3.norm1.bias', 'vision_tower.vision_tower.blocks.3.norm1.weight', 'vision_tower.vision_tower.blocks.3.norm2.bias', 'vision_tower.vision_tower.blocks.3.norm2.weight', 'vision_tower.vision_tower.blocks.4.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.4.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.4.attn.qkv.weight', 'vision_tower.vision_tower.blocks.4.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.4.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.4.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.4.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.4.norm1.bias', 'vision_tower.vision_tower.blocks.4.norm1.weight', 'vision_tower.vision_tower.blocks.4.norm2.bias', 'vision_tower.vision_tower.blocks.4.norm2.weight', 'vision_tower.vision_tower.blocks.5.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.5.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.5.attn.qkv.weight', 'vision_tower.vision_tower.blocks.5.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.5.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.5.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.5.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.5.norm1.bias', 'vision_tower.vision_tower.blocks.5.norm1.weight', 'vision_tower.vision_tower.blocks.5.norm2.bias', 'vision_tower.vision_tower.blocks.5.norm2.weight', 'vision_tower.vision_tower.blocks.6.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.6.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.6.attn.qkv.weight', 'vision_tower.vision_tower.blocks.6.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.6.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.6.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.6.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.6.norm1.bias', 'vision_tower.vision_tower.blocks.6.norm1.weight', 'vision_tower.vision_tower.blocks.6.norm2.bias', 'vision_tower.vision_tower.blocks.6.norm2.weight', 'vision_tower.vision_tower.blocks.7.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.7.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.7.attn.qkv.weight', 'vision_tower.vision_tower.blocks.7.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.7.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.7.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.7.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.7.norm1.bias', 'vision_tower.vision_tower.blocks.7.norm1.weight', 'vision_tower.vision_tower.blocks.7.norm2.bias', 'vision_tower.vision_tower.blocks.7.norm2.weight', 'vision_tower.vision_tower.blocks.8.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.8.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.8.attn.qkv.weight', 'vision_tower.vision_tower.blocks.8.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.8.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.8.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.8.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.8.norm1.bias', 'vision_tower.vision_tower.blocks.8.norm1.weight', 'vision_tower.vision_tower.blocks.8.norm2.bias', 'vision_tower.vision_tower.blocks.8.norm2.weight', 'vision_tower.vision_tower.blocks.9.attn.out_proj.bias', 'vision_tower.vision_tower.blocks.9.attn.out_proj.weight', 'vision_tower.vision_tower.blocks.9.attn.qkv.weight', 'vision_tower.vision_tower.blocks.9.mlp.linear1.bias', 'vision_tower.vision_tower.blocks.9.mlp.linear1.weight', 'vision_tower.vision_tower.blocks.9.mlp.linear2.bias', 'vision_tower.vision_tower.blocks.9.mlp.linear2.weight', 'vision_tower.vision_tower.blocks.9.norm1.bias', 'vision_tower.vision_tower.blocks.9.norm1.weight', 'vision_tower.vision_tower.blocks.9.norm2.bias', 'vision_tower.vision_tower.blocks.9.norm2.weight', 'vision_tower.vision_tower.cls_token', 'vision_tower.vision_tower.norm.bias', 'vision_tower.vision_tower.norm.weight', 'vision_tower.vision_tower.patch_embedding.patch_embeddings.1.bias', 'vision_tower.vision_tower.patch_embedding.patch_embeddings.1.weight', 'vision_tower.vision_tower.patch_embedding.position_embeddings']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'find_all_linear_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     14\u001b[0m     model_name_or_path,\n\u001b[1;32m     15\u001b[0m     model_max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lora_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     lora_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     23\u001b[0m         r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     24\u001b[0m         lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m---> 25\u001b[0m         target_modules\u001b[38;5;241m=\u001b[39m\u001b[43mfind_all_linear_names\u001b[49m(base_model),\n\u001b[1;32m     26\u001b[0m         lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[1;32m     27\u001b[0m         bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m         task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding LoRA adapters only on LLM.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m     model \u001b[38;5;241m=\u001b[39m get_peft_model(base_model, lora_config)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'find_all_linear_names' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') # 'cpu', 'cuda'\n",
    "dtype = torch.bfloat16 # or bfloat16, float16, float32\n",
    "\n",
    "model_name_or_path = '/import/c4dm-04/siyoul/Med3DLLM/checkpoint/amosmm_chatgpt_stage_1/checkpoint-100080'\n",
    "lora_model_path = '/import/c4dm-04/siyoul/Med3DLLM/checkpoint/amosmm_chatgpt_stage_1/model_with_lora.bin'\n",
    "state_dict = torch.load(lora_model_path, map_location=\"cpu\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=dtype,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    model_max_length=512,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "if lora_model_path is not None:\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=find_all_linear_names(base_model),\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    print(\"Adding LoRA adapters only on LLM.\")\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    # lamed_model.print_trainable_parameters()\n",
    "    print(\"Load weights with LoRA\")\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    print(\"Merge weights with LoRA\")\n",
    "    model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([32015, 3072])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.0.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.0.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.0.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.1.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.1.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.1.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.2.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.2.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.2.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.3.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.3.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.3.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.4.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.4.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.4.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.5.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.5.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.5.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.6.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.6.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.6.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.7.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.7.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.7.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.8.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.8.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.8.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.9.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.9.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.9.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.10.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.10.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.10.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.11.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.11.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.11.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.12.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.12.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.12.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.13.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.13.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.13.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.14.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.14.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.14.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.15.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.15.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.15.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.16.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.16.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.16.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.17.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.17.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.17.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.18.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.18.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.18.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.19.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.19.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.19.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.20.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.20.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.20.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.21.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.21.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.21.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.22.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.22.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.22.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.23.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.23.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.23.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.24.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.24.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.24.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.25.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.25.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.25.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.26.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.26.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.26.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.27.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.27.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.27.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.28.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.28.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.28.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.29.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.29.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.29.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.30.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.30.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.30.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([3072, 3072])\n",
      "model.layers.31.self_attn.qkv_proj.weight torch.Size([9216, 3072])\n",
      "model.layers.31.mlp.gate_up_proj.weight torch.Size([16384, 3072])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([3072, 8192])\n",
      "model.layers.31.input_layernorm.weight torch.Size([3072])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([3072])\n",
      "model.norm.weight torch.Size([3072])\n",
      "model.vision_tower.vision_tower.cls_token torch.Size([1, 1, 768])\n",
      "model.vision_tower.vision_tower.patch_embedding.position_embeddings torch.Size([1, 2048, 768])\n",
      "model.vision_tower.vision_tower.patch_embedding.patch_embeddings.1.weight torch.Size([768, 1024])\n",
      "model.vision_tower.vision_tower.patch_embedding.patch_embeddings.1.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.0.mlp.linear1.weight torch.Size([3072, 768])\n",
      "model.vision_tower.vision_tower.blocks.0.mlp.linear1.bias torch.Size([3072])\n",
      "model.vision_tower.vision_tower.blocks.0.mlp.linear2.weight torch.Size([768, 3072])\n",
      "model.vision_tower.vision_tower.blocks.0.mlp.linear2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.0.norm1.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.0.norm1.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.0.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.vision_tower.vision_tower.blocks.0.attn.out_proj.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.0.attn.qkv.weight torch.Size([2304, 768])\n",
      "model.vision_tower.vision_tower.blocks.0.norm2.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.0.norm2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.1.mlp.linear1.weight torch.Size([3072, 768])\n",
      "model.vision_tower.vision_tower.blocks.1.mlp.linear1.bias torch.Size([3072])\n",
      "model.vision_tower.vision_tower.blocks.1.mlp.linear2.weight torch.Size([768, 3072])\n",
      "model.vision_tower.vision_tower.blocks.1.mlp.linear2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.1.norm1.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.1.norm1.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.1.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.vision_tower.vision_tower.blocks.1.attn.out_proj.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.1.attn.qkv.weight torch.Size([2304, 768])\n",
      "model.vision_tower.vision_tower.blocks.1.norm2.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.1.norm2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.2.mlp.linear1.weight torch.Size([3072, 768])\n",
      "model.vision_tower.vision_tower.blocks.2.mlp.linear1.bias torch.Size([3072])\n",
      "model.vision_tower.vision_tower.blocks.2.mlp.linear2.weight torch.Size([768, 3072])\n",
      "model.vision_tower.vision_tower.blocks.2.mlp.linear2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.2.norm1.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.2.norm1.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.2.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.vision_tower.vision_tower.blocks.2.attn.out_proj.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.2.attn.qkv.weight torch.Size([2304, 768])\n",
      "model.vision_tower.vision_tower.blocks.2.norm2.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.2.norm2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.3.mlp.linear1.weight torch.Size([3072, 768])\n",
      "model.vision_tower.vision_tower.blocks.3.mlp.linear1.bias torch.Size([3072])\n",
      "model.vision_tower.vision_tower.blocks.3.mlp.linear2.weight torch.Size([768, 3072])\n",
      "model.vision_tower.vision_tower.blocks.3.mlp.linear2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.3.norm1.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.3.norm1.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.3.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.vision_tower.vision_tower.blocks.3.attn.out_proj.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.3.attn.qkv.weight torch.Size([2304, 768])\n",
      "model.vision_tower.vision_tower.blocks.3.norm2.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.3.norm2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.4.mlp.linear1.weight torch.Size([3072, 768])\n",
      "model.vision_tower.vision_tower.blocks.4.mlp.linear1.bias torch.Size([3072])\n",
      "model.vision_tower.vision_tower.blocks.4.mlp.linear2.weight torch.Size([768, 3072])\n",
      "model.vision_tower.vision_tower.blocks.4.mlp.linear2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.4.norm1.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.4.norm1.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.4.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.vision_tower.vision_tower.blocks.4.attn.out_proj.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.4.attn.qkv.weight torch.Size([2304, 768])\n",
      "model.vision_tower.vision_tower.blocks.4.norm2.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.4.norm2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.5.mlp.linear1.weight torch.Size([3072, 768])\n",
      "model.vision_tower.vision_tower.blocks.5.mlp.linear1.bias torch.Size([3072])\n",
      "model.vision_tower.vision_tower.blocks.5.mlp.linear2.weight torch.Size([768, 3072])\n",
      "model.vision_tower.vision_tower.blocks.5.mlp.linear2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.5.norm1.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.5.norm1.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.5.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.vision_tower.vision_tower.blocks.5.attn.out_proj.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.5.attn.qkv.weight torch.Size([2304, 768])\n",
      "model.vision_tower.vision_tower.blocks.5.norm2.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.5.norm2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.6.mlp.linear1.weight torch.Size([3072, 768])\n",
      "model.vision_tower.vision_tower.blocks.6.mlp.linear1.bias torch.Size([3072])\n",
      "model.vision_tower.vision_tower.blocks.6.mlp.linear2.weight torch.Size([768, 3072])\n",
      "model.vision_tower.vision_tower.blocks.6.mlp.linear2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.6.norm1.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.6.norm1.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.6.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.vision_tower.vision_tower.blocks.6.attn.out_proj.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.6.attn.qkv.weight torch.Size([2304, 768])\n",
      "model.vision_tower.vision_tower.blocks.6.norm2.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.6.norm2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.7.mlp.linear1.weight torch.Size([3072, 768])\n",
      "model.vision_tower.vision_tower.blocks.7.mlp.linear1.bias torch.Size([3072])\n",
      "model.vision_tower.vision_tower.blocks.7.mlp.linear2.weight torch.Size([768, 3072])\n",
      "model.vision_tower.vision_tower.blocks.7.mlp.linear2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.7.norm1.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.7.norm1.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.7.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.vision_tower.vision_tower.blocks.7.attn.out_proj.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.7.attn.qkv.weight torch.Size([2304, 768])\n",
      "model.vision_tower.vision_tower.blocks.7.norm2.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.7.norm2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.8.mlp.linear1.weight torch.Size([3072, 768])\n",
      "model.vision_tower.vision_tower.blocks.8.mlp.linear1.bias torch.Size([3072])\n",
      "model.vision_tower.vision_tower.blocks.8.mlp.linear2.weight torch.Size([768, 3072])\n",
      "model.vision_tower.vision_tower.blocks.8.mlp.linear2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.8.norm1.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.8.norm1.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.8.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.vision_tower.vision_tower.blocks.8.attn.out_proj.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.8.attn.qkv.weight torch.Size([2304, 768])\n",
      "model.vision_tower.vision_tower.blocks.8.norm2.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.8.norm2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.9.mlp.linear1.weight torch.Size([3072, 768])\n",
      "model.vision_tower.vision_tower.blocks.9.mlp.linear1.bias torch.Size([3072])\n",
      "model.vision_tower.vision_tower.blocks.9.mlp.linear2.weight torch.Size([768, 3072])\n",
      "model.vision_tower.vision_tower.blocks.9.mlp.linear2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.9.norm1.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.9.norm1.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.9.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.vision_tower.vision_tower.blocks.9.attn.out_proj.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.9.attn.qkv.weight torch.Size([2304, 768])\n",
      "model.vision_tower.vision_tower.blocks.9.norm2.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.9.norm2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.10.mlp.linear1.weight torch.Size([3072, 768])\n",
      "model.vision_tower.vision_tower.blocks.10.mlp.linear1.bias torch.Size([3072])\n",
      "model.vision_tower.vision_tower.blocks.10.mlp.linear2.weight torch.Size([768, 3072])\n",
      "model.vision_tower.vision_tower.blocks.10.mlp.linear2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.10.norm1.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.10.norm1.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.10.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.vision_tower.vision_tower.blocks.10.attn.out_proj.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.10.attn.qkv.weight torch.Size([2304, 768])\n",
      "model.vision_tower.vision_tower.blocks.10.norm2.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.10.norm2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.11.mlp.linear1.weight torch.Size([3072, 768])\n",
      "model.vision_tower.vision_tower.blocks.11.mlp.linear1.bias torch.Size([3072])\n",
      "model.vision_tower.vision_tower.blocks.11.mlp.linear2.weight torch.Size([768, 3072])\n",
      "model.vision_tower.vision_tower.blocks.11.mlp.linear2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.11.norm1.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.11.norm1.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.11.attn.out_proj.weight torch.Size([768, 768])\n",
      "model.vision_tower.vision_tower.blocks.11.attn.out_proj.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.11.attn.qkv.weight torch.Size([2304, 768])\n",
      "model.vision_tower.vision_tower.blocks.11.norm2.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.blocks.11.norm2.bias torch.Size([768])\n",
      "model.vision_tower.vision_tower.norm.weight torch.Size([768])\n",
      "model.vision_tower.vision_tower.norm.bias torch.Size([768])\n",
      "model.mm_projector.projector.0.weight torch.Size([3072, 768])\n",
      "model.mm_projector.projector.0.bias torch.Size([3072])\n",
      "model.mm_projector.projector.2.weight torch.Size([3072, 3072])\n",
      "model.mm_projector.projector.2.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.query_tokens torch.Size([1, 256, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.in_proj_bias torch.Size([9216])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.in_proj_bias torch.Size([9216])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.in_proj_bias torch.Size([9216])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.in_proj_bias torch.Size([9216])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.in_proj_bias torch.Size([9216])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.in_proj_bias torch.Size([9216])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.in_proj_bias torch.Size([9216])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.in_proj_bias torch.Size([9216])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.svt_module.token_selection.score_net.weight torch.Size([1, 3072])\n",
      "model.linear3d_tokenizer.svt_module.token_selection.score_net.bias torch.Size([1])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_v.weight torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_v.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_t.weight torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_t.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.in_proj_bias torch.Size([9216])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.norm_self.weight torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.0.norm_self.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_v.weight torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_v.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_t.weight torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_t.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.in_proj_bias torch.Size([9216])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.norm_self.weight torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.1.norm_self.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_v.weight torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_v.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_t.weight torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_t.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.in_proj_bias torch.Size([9216])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.norm_self.weight torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.2.norm_self.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_v.weight torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_v.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_t.weight torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_t.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.in_proj_bias torch.Size([9216])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.norm_self.weight torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layers_vt.3.norm_self.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.bias torch.Size([3072])\n",
      "model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.weight torch.Size([3072, 3072])\n",
      "model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.bias torch.Size([3072])\n",
      "lm_head.weight torch.Size([32015, 3072])\n",
      "484\n"
     ]
    }
   ],
   "source": [
    "for name, param in base_model.named_parameters():\n",
    "    print(name, param.size())\n",
    "print(len(list(base_model.parameters())))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight torch.Size([32015, 3072])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.0.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.0.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.0.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.0.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.0.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.0.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.0.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.1.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.1.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.1.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.1.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.1.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.1.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.1.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.2.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.2.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.2.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.2.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.2.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.2.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.2.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.3.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.3.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.3.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.3.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.3.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.3.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.3.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.4.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.4.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.4.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.4.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.4.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.4.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.4.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.5.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.5.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.5.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.5.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.5.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.5.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.5.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.6.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.6.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.6.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.6.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.6.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.6.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.6.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.7.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.7.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.7.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.7.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.7.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.7.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.7.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.8.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.8.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.8.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.8.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.8.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.8.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.8.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.9.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.9.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.9.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.9.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.9.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.9.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.9.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.10.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.10.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.10.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.10.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.10.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.10.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.10.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.11.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.11.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.11.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.11.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.11.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.11.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.11.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.12.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.12.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.12.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.12.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.12.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.12.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.12.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.13.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.13.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.13.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.13.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.13.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.13.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.13.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.14.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.14.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.14.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.14.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.14.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.14.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.14.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.15.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.15.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.15.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.15.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.15.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.15.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.15.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.16.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.16.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.16.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.16.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.16.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.16.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.16.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.17.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.17.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.17.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.17.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.17.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.17.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.17.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.18.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.18.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.18.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.18.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.18.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.18.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.18.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.19.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.19.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.19.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.19.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.19.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.19.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.19.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.20.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.20.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.20.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.20.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.20.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.20.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.20.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.21.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.21.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.21.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.21.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.21.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.21.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.21.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.22.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.22.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.22.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.22.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.22.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.22.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.22.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.23.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.23.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.23.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.23.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.23.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.23.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.23.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.24.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.24.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.24.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.24.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.24.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.24.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.24.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.25.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.25.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.25.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.25.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.25.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.25.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.25.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.26.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.26.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.26.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.26.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.26.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.26.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.26.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.27.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.27.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.27.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.27.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.27.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.27.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.27.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.28.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.28.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.28.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.28.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.28.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.28.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.28.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.28.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.29.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.29.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.29.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.29.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.29.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.29.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.29.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.29.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.30.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.30.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.30.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.30.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.30.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.30.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.30.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.30.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.31.self_attn.qkv_proj.base_layer.weight torch.Size([9216, 3072])\n",
      "base_model.model.model.layers.31.self_attn.qkv_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.31.self_attn.qkv_proj.lora_B.default.weight torch.Size([9216, 16])\n",
      "base_model.model.model.layers.31.mlp.gate_up_proj.base_layer.weight torch.Size([16384, 3072])\n",
      "base_model.model.model.layers.31.mlp.gate_up_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.layers.31.mlp.gate_up_proj.lora_B.default.weight torch.Size([16384, 16])\n",
      "base_model.model.model.layers.31.mlp.down_proj.base_layer.weight torch.Size([3072, 8192])\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight torch.Size([16, 8192])\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.layers.31.input_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight torch.Size([3072])\n",
      "base_model.model.model.norm.weight torch.Size([3072])\n",
      "base_model.model.model.vision_tower.vision_tower.cls_token torch.Size([1, 1, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.patch_embedding.position_embeddings torch.Size([1, 2048, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.patch_embedding.patch_embeddings.1.weight torch.Size([768, 1024])\n",
      "base_model.model.model.vision_tower.vision_tower.patch_embedding.patch_embeddings.1.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.0.mlp.linear1.weight torch.Size([3072, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.0.mlp.linear1.bias torch.Size([3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.0.mlp.linear2.weight torch.Size([768, 3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.0.mlp.linear2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.0.norm1.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.0.norm1.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.0.attn.out_proj.weight torch.Size([768, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.0.attn.out_proj.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.0.attn.qkv.weight torch.Size([2304, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.0.norm2.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.0.norm2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.1.mlp.linear1.weight torch.Size([3072, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.1.mlp.linear1.bias torch.Size([3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.1.mlp.linear2.weight torch.Size([768, 3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.1.mlp.linear2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.1.norm1.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.1.norm1.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.1.attn.out_proj.weight torch.Size([768, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.1.attn.out_proj.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.1.attn.qkv.weight torch.Size([2304, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.1.norm2.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.1.norm2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.2.mlp.linear1.weight torch.Size([3072, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.2.mlp.linear1.bias torch.Size([3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.2.mlp.linear2.weight torch.Size([768, 3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.2.mlp.linear2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.2.norm1.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.2.norm1.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.2.attn.out_proj.weight torch.Size([768, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.2.attn.out_proj.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.2.attn.qkv.weight torch.Size([2304, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.2.norm2.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.2.norm2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.3.mlp.linear1.weight torch.Size([3072, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.3.mlp.linear1.bias torch.Size([3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.3.mlp.linear2.weight torch.Size([768, 3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.3.mlp.linear2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.3.norm1.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.3.norm1.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.3.attn.out_proj.weight torch.Size([768, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.3.attn.out_proj.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.3.attn.qkv.weight torch.Size([2304, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.3.norm2.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.3.norm2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.4.mlp.linear1.weight torch.Size([3072, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.4.mlp.linear1.bias torch.Size([3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.4.mlp.linear2.weight torch.Size([768, 3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.4.mlp.linear2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.4.norm1.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.4.norm1.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.4.attn.out_proj.weight torch.Size([768, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.4.attn.out_proj.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.4.attn.qkv.weight torch.Size([2304, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.4.norm2.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.4.norm2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.5.mlp.linear1.weight torch.Size([3072, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.5.mlp.linear1.bias torch.Size([3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.5.mlp.linear2.weight torch.Size([768, 3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.5.mlp.linear2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.5.norm1.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.5.norm1.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.5.attn.out_proj.weight torch.Size([768, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.5.attn.out_proj.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.5.attn.qkv.weight torch.Size([2304, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.5.norm2.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.5.norm2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.6.mlp.linear1.weight torch.Size([3072, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.6.mlp.linear1.bias torch.Size([3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.6.mlp.linear2.weight torch.Size([768, 3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.6.mlp.linear2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.6.norm1.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.6.norm1.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.6.attn.out_proj.weight torch.Size([768, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.6.attn.out_proj.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.6.attn.qkv.weight torch.Size([2304, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.6.norm2.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.6.norm2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.7.mlp.linear1.weight torch.Size([3072, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.7.mlp.linear1.bias torch.Size([3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.7.mlp.linear2.weight torch.Size([768, 3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.7.mlp.linear2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.7.norm1.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.7.norm1.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.7.attn.out_proj.weight torch.Size([768, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.7.attn.out_proj.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.7.attn.qkv.weight torch.Size([2304, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.7.norm2.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.7.norm2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.8.mlp.linear1.weight torch.Size([3072, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.8.mlp.linear1.bias torch.Size([3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.8.mlp.linear2.weight torch.Size([768, 3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.8.mlp.linear2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.8.norm1.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.8.norm1.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.8.attn.out_proj.weight torch.Size([768, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.8.attn.out_proj.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.8.attn.qkv.weight torch.Size([2304, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.8.norm2.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.8.norm2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.9.mlp.linear1.weight torch.Size([3072, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.9.mlp.linear1.bias torch.Size([3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.9.mlp.linear2.weight torch.Size([768, 3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.9.mlp.linear2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.9.norm1.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.9.norm1.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.9.attn.out_proj.weight torch.Size([768, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.9.attn.out_proj.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.9.attn.qkv.weight torch.Size([2304, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.9.norm2.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.9.norm2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.10.mlp.linear1.weight torch.Size([3072, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.10.mlp.linear1.bias torch.Size([3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.10.mlp.linear2.weight torch.Size([768, 3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.10.mlp.linear2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.10.norm1.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.10.norm1.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.10.attn.out_proj.weight torch.Size([768, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.10.attn.out_proj.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.10.attn.qkv.weight torch.Size([2304, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.10.norm2.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.10.norm2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.11.mlp.linear1.weight torch.Size([3072, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.11.mlp.linear1.bias torch.Size([3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.11.mlp.linear2.weight torch.Size([768, 3072])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.11.mlp.linear2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.11.norm1.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.11.norm1.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.11.attn.out_proj.weight torch.Size([768, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.11.attn.out_proj.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.11.attn.qkv.weight torch.Size([2304, 768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.11.norm2.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.blocks.11.norm2.bias torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.norm.weight torch.Size([768])\n",
      "base_model.model.model.vision_tower.vision_tower.norm.bias torch.Size([768])\n",
      "base_model.model.model.mm_projector.projector.0.weight torch.Size([3072, 768])\n",
      "base_model.model.model.mm_projector.projector.0.bias torch.Size([3072])\n",
      "base_model.model.model.mm_projector.projector.2.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.mm_projector.projector.2.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.query_tokens torch.Size([1, 256, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.in_proj_bias torch.Size([9216])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.spatial_attention.out_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.in_proj_bias torch.Size([9216])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.0.temporal_attention.out_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.in_proj_bias torch.Size([9216])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.spatial_attention.out_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.in_proj_bias torch.Size([9216])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.1.temporal_attention.out_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.in_proj_bias torch.Size([9216])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.spatial_attention.out_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.in_proj_bias torch.Size([9216])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.2.temporal_attention.out_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.in_proj_bias torch.Size([9216])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.spatial_attention.out_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.in_proj_bias torch.Size([9216])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.attention_network.layers.3.temporal_attention.out_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.token_selection.score_net.base_layer.weight torch.Size([1, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.token_selection.score_net.base_layer.bias torch.Size([1])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.token_selection.score_net.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.svt_module.token_selection.score_net.lora_B.default.weight torch.Size([1, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wq.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wk.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.wv.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.visual_cross_attention.dense.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wq.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wk.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.wv.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.text_cross_attention.dense.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_v.weight torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_v.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_t.weight torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_cross_t.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.in_proj_bias torch.Size([9216])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.self_attention.out_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_self.weight torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.0.norm_self.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wq.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wk.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.wv.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.visual_cross_attention.dense.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wq.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wk.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.wv.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.text_cross_attention.dense.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_v.weight torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_v.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_t.weight torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_cross_t.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.in_proj_bias torch.Size([9216])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.self_attention.out_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_self.weight torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.1.norm_self.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wq.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wk.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.wv.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.visual_cross_attention.dense.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wq.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wk.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.wv.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.text_cross_attention.dense.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_v.weight torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_v.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_t.weight torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_cross_t.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.in_proj_bias torch.Size([9216])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.self_attention.out_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_self.weight torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.2.norm_self.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wq.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wk.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.wv.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.visual_cross_attention.dense.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wq.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wk.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.wv.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.text_cross_attention.dense.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_v.weight torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_v.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_t.weight torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_cross_t.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.in_proj_weight torch.Size([9216, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.in_proj_bias torch.Size([9216])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.self_attention.out_proj.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_self.weight torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layers_vt.3.norm_self.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wq.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wk.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.wv.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.base_layer.weight torch.Size([3072, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.base_layer.bias torch.Size([3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.lora_A.default.weight torch.Size([16, 3072])\n",
      "base_model.model.model.linear3d_tokenizer.tta_module.layer_linagg.linear_aggregator.dense.lora_B.default.weight torch.Size([3072, 16])\n",
      "base_model.model.lm_head.weight torch.Size([32015, 3072])\n"
     ]
    }
   ],
   "source": [
    "lora_model_path = '/import/c4dm-04/siyoul/Med3DLLM/checkpoint/amosmm_chatgpt_stage_1/model_with_lora.bin'\n",
    "state_dict = torch.load(lora_model_path, map_location=\"cpu\")\n",
    "for k,v in list(state_dict.items()):\n",
    "    print(k, v.size())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding LoRA adapters only on LLM.\n",
      "Load weights with LoRA\n",
      "Merge weights with LoRA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if lora_model_path is not None:\n",
    "        \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=find_all_linear_names(base_model),\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    print(\"Adding LoRA adapters only on LLM.\")\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    # lamed_model.print_trainable_parameters()\n",
    "    print(\"Load weights with LoRA\")\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    print(\"Merge weights with LoRA\")\n",
    "    model = model.merge_and_unload()\n",
    "model = model.to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LamedPhi3ForCausalLM(\n",
       "  (model): LamedPhi3Model(\n",
       "    (embed_tokens): Embedding(32015, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "    (vision_tower): ViT3DTower(\n",
       "      (vision_tower): myViT(\n",
       "        (patch_embedding): PatchEmbeddingBlock(\n",
       "          (patch_embeddings): Sequential(\n",
       "            (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=4, p2=16, p3=16)\n",
       "            (1): Linear(in_features=1024, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0-11): 12 x TransformerBlock(\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): SABlock(\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "              (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)\n",
       "              (out_rearrange): Rearrange('b h l d -> b l (h d)')\n",
       "              (drop_output): Dropout(p=0.0, inplace=False)\n",
       "              (drop_weights): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (mm_projector): SpatialPoolingProjector(\n",
       "      (projector): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (linear3d_tokenizer): Linear3DTokenizer(\n",
       "      (svt_module): SpatioTemporalVisualTokenRefinerModel(\n",
       "        (attention_network): SpatioTemporalSignificanceScoring(\n",
       "          (layers): ModuleList(\n",
       "            (0-3): 4 x SpatioTemporalAttentionLayer(\n",
       "              (spatial_attention): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n",
       "              )\n",
       "              (temporal_attention): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (token_selection): TokenSelection(\n",
       "          (score_net): Linear(in_features=3072, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (tta_module): TextConditionTokenAggregatorModel(\n",
       "        (layers_vt): ModuleList(\n",
       "          (0-3): 4 x TextConditionTokenAttMap(\n",
       "            (visual_cross_attention): MultiHeadCrossAttention(\n",
       "              (wq): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "              (wk): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "              (wv): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "              (dense): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "            )\n",
       "            (text_cross_attention): MultiHeadCrossAttention(\n",
       "              (wq): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "              (wk): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "              (wv): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "              (dense): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "            )\n",
       "            (dropout_cross): Identity()\n",
       "            (norm_cross_v): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm_cross_t): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "            (self_attention): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=3072, out_features=3072, bias=True)\n",
       "            )\n",
       "            (dropout_self): Identity()\n",
       "            (norm_self): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_linagg): LinearAggregation(\n",
       "          (linear_aggregator): MultiHeadCrossAttention(\n",
       "            (wq): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "            (wk): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "            (wv): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "            (dense): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32015, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.array CropForeground.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "from src.utils.linear_3d_transform import Linear3DTransform\n",
    "l3dt = Linear3DTransform(data_type=\"validation\")\n",
    "image_file_path = \"/import/c4dm-04/siyoul/Med3DLLM/datasets/AMOS-MM/imagesVa/amos_0008.nii.gz\"\n",
    "image = l3dt(image_file_path)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'padding_side': 'right'} not recognized.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question Can you provide a diagnosis based on the fingings in chest in this image?.\n",
      "generated_texts The CT report indicates the presence of pericardial and pleural effusions, along with localized incomplete expansion in the lower lobes of both lungs.\n"
     ]
    }
   ],
   "source": [
    "# question = \"Can you provide a caption consists of findings for this medical image?\"\n",
    "question = \"Can you provide a diagnosis based on the fingings in chest in this image?.\"\n",
    "# question = \"What is liver in this image? Please output the box.\"\n",
    "question_ids = tokenizer(\n",
    "            question, add_special_tokens=False, max_length=768, truncation=True, padding=\"max_length\", return_tensors=\"pt\", padding_side=\"right\"\n",
    "        )[\"input_ids\"][0]\n",
    "image_tokens = \"<im_patch>\" * proj_out_num\n",
    "input_txt = image_tokens + question\n",
    "input_id = tokenizer(input_txt, return_tensors=\"pt\")['input_ids'].to(device=device)\n",
    "\n",
    "\n",
    "# generation = model.generate(image_pt, input_id, max_new_tokens=256, do_sample=True, top_p=0.9, temperature=1.0)\n",
    "with torch.cuda.amp.autocast(): \n",
    "    generation = model.generate(image.unsqueeze(0).to(device=device), input_id, question_ids=question_ids.to(device=device), max_new_tokens=768, do_sample=True, top_p=0.9, temperature=1.0)\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(generation, skip_special_tokens=True)\n",
    "\n",
    "print('question', question)\n",
    "print('generated_texts', generated_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.array CropForeground.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "from monai.transforms import (\n",
    "        LoadImage,\n",
    "        Compose,\n",
    "        CropForeground,\n",
    "        ToTensor,\n",
    "        SaveImage,\n",
    "        ScaleIntensityRangePercentiles,\n",
    "        RandRotate90,\n",
    "        RandFlip,\n",
    "        NormalizeIntensity,\n",
    "        RandScaleIntensity,\n",
    "        RandShiftIntensity,\n",
    "        Resize,\n",
    "        Transpose,\n",
    "    )\n",
    "from monai.data.image_reader import NibabelReader\n",
    "\n",
    "transforms = Compose(\n",
    "                [\n",
    "                LoadImage(image_only=True, ensure_channel_first=False, reader=NibabelReader()),\n",
    "                # Transpose(indices=(2, 0, 1)),\n",
    "                ScaleIntensityRangePercentiles(lower=0.5, upper=99.5, b_max=1.0, b_min=0.0, clip=True),\n",
    "                CropForeground(source_key=\"image\"),\n",
    "                #Resize(spatial_size=[32, 256,256],mode='trilinear'),\n",
    "                RandRotate90(prob=0.5, spatial_axes=(1, 2)),\n",
    "                RandFlip(prob=0.10, spatial_axis=0),\n",
    "                RandFlip(prob=0.10, spatial_axis=1),\n",
    "                RandFlip(prob=0.10, spatial_axis=2),\n",
    "                RandScaleIntensity(factors=0.1, prob=0.5),\n",
    "                RandShiftIntensity(offsets=0.1, prob=0.5),\n",
    "                ToTensor(),\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "applying transform <monai.transforms.spatial.array.RandRotate90 object at 0x706948f0ee50>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/transforms/transform.py:141\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# appears where the exception was raised.\u001b[39;00m\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/transforms/transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(data)\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/transforms/spatial/array.py:1244\u001b[0m, in \u001b[0;36mRandRotate90.__call__\u001b[0;34m(self, img, randomize, lazy)\u001b[0m\n\u001b[1;32m   1243\u001b[0m     xform \u001b[38;5;241m=\u001b[39m Rotate90(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rand_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_axes, lazy\u001b[38;5;241m=\u001b[39mlazy_)\n\u001b[0;32m-> 1244\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mxform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/transforms/spatial/array.py:1176\u001b[0m, in \u001b[0;36mRotate90.__call__\u001b[0;34m(self, img, lazy)\u001b[0m\n\u001b[1;32m   1175\u001b[0m lazy_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy \u001b[38;5;28;01mif\u001b[39;00m lazy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lazy\n\u001b[0;32m-> 1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrotate90\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_transform_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/transforms/spatial/functional.py:516\u001b[0m, in \u001b[0;36mrotate90\u001b[0;34m(img, axes, k, lazy, transform_info)\u001b[0m\n\u001b[1;32m    515\u001b[0m     a_0, a_1 \u001b[38;5;241m=\u001b[39m axes[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, axes[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 516\u001b[0m     sp_shape[a_0], sp_shape[a_1] \u001b[38;5;241m=\u001b[39m \u001b[43mori_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma_1\u001b[49m\u001b[43m]\u001b[49m, ori_shape[a_0]\n\u001b[1;32m    517\u001b[0m rank \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpeek_pending_rank() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, MetaTensor) \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m3.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/transforms/compose.py:335\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, input_, start, end, threading, lazy)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, threading\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, lazy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    334\u001b[0m     _lazy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy \u001b[38;5;28;01mif\u001b[39;00m lazy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lazy\n\u001b[0;32m--> 335\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_compose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_lazy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreading\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/transforms/compose.py:111\u001b[0m, in \u001b[0;36mexecute_compose\u001b[0;34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m threading:\n\u001b[1;32m    110\u001b[0m         _transform \u001b[38;5;241m=\u001b[39m deepcopy(_transform) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[38;5;28;01melse\u001b[39;00m _transform\n\u001b[0;32m--> 111\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m data \u001b[38;5;241m=\u001b[39m apply_pending_transforms(data, \u001b[38;5;28;01mNone\u001b[39;00m, overrides, logger_name\u001b[38;5;241m=\u001b[39mlog_stats)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/transforms/transform.py:171\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m         _log_stats(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplying transform \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransform\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: applying transform <monai.transforms.spatial.array.RandRotate90 object at 0x706948f0ee50>"
     ]
    }
   ],
   "source": [
    "image = transforms(image_file_path)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transpose() received an invalid combination of arguments - got (int, int, int), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: transpose() received an invalid combination of arguments - got (int, int, int), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n"
     ]
    }
   ],
   "source": [
    "image = image.transpose(2, 0, 1)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'padding_side': 'right'} not recognized.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# generation = model.generate(image_pt, input_id, max_new_tokens=256, do_sample=True, top_p=0.9, temperature=1.0)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(): \n\u001b[0;32m---> 14\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m generated_texts \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generation, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m, question)\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/M3D-LaMed-Phi-3-4B/modeling_m3d_lamed.py:2039\u001b[0m, in \u001b[0;36mLamedPhi3ForCausalLM.generate\u001b[0;34m(self, images, inputs, seg_enable, **kwargs)\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`inputs_embeds` is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2032\u001b[0m     (\n\u001b[1;32m   2033\u001b[0m         inputs,\n\u001b[1;32m   2034\u001b[0m         position_ids,\n\u001b[1;32m   2035\u001b[0m         attention_mask,\n\u001b[1;32m   2036\u001b[0m         _,\n\u001b[1;32m   2037\u001b[0m         inputs_embeds,\n\u001b[1;32m   2038\u001b[0m         _\n\u001b[0;32m-> 2039\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_inputs_for_multimodal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2043\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2044\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2046\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2047\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2048\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model()\u001b[38;5;241m.\u001b[39membed_tokens(inputs)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/M3D-LaMed-Phi-3-4B/modeling_m3d_lamed.py:1842\u001b[0m, in \u001b[0;36mLamedMetaForCausalLM.prepare_inputs_for_multimodal\u001b[0;34m(self, input_ids, position_ids, attention_mask, past_key_values, labels, images)\u001b[0m\n\u001b[1;32m   1840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m input_ids, position_ids, attention_mask, past_key_values, \u001b[38;5;28;01mNone\u001b[39;00m, labels\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1842\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1843\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model()\u001b[38;5;241m.\u001b[39membed_tokens(input_ids)\n\u001b[1;32m   1844\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m   1845\u001b[0m         (inputs_embeds[:, :\u001b[38;5;241m1\u001b[39m, :], image_features, inputs_embeds[:, (image_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):, :]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/M3D-LaMed-Phi-3-4B/modeling_m3d_lamed.py:1830\u001b[0m, in \u001b[0;36mLamedMetaForCausalLM.encode_images\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_images\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[0;32m-> 1830\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vision_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1831\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model()\u001b[38;5;241m.\u001b[39mmm_projector(image_features)\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_features\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/M3D-LaMed-Phi-3-4B/modeling_m3d_lamed.py:1697\u001b[0m, in \u001b[0;36mViT3DTower.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[0;32m-> 1697\u001b[0m     last_feature, hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_layer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1699\u001b[0m         image_features \u001b[38;5;241m=\u001b[39m last_feature\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/M3D-LaMed-Phi-3-4B/modeling_m3d_lamed.py:1666\u001b[0m, in \u001b[0;36mmyViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m-> 1666\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_token\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1668\u001b[0m         cls_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token\u001b[38;5;241m.\u001b[39mexpand(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/monai/networks/blocks/patchembedding.py:141\u001b[0m, in \u001b[0;36mPatchEmbeddingBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 141\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    143\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/import/dali-share-02/siyoul/miniconda3/envs/green_score/lib/python3.11/site-packages/einops/layers/torch.py:14\u001b[0m, in \u001b[0;36mRearrange.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m     recipe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_multirecipe\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m apply_for_scriptable_torch(recipe, \u001b[38;5;28minput\u001b[39m, reduction_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrearrange\u001b[39m\u001b[38;5;124m\"\u001b[39m, axes_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_axes_lengths)\n",
      "\u001b[0;31mKeyError\u001b[0m: 3"
     ]
    }
   ],
   "source": [
    "# question = \"Can you provide a caption consists of findings for this medical image?\"\n",
    "question = \"Can you provide a diagnosis based on the fingings in chest in this image?.\"\n",
    "# question = \"What is liver in this image? Please output the box.\"\n",
    "question_ids = tokenizer(\n",
    "            question, add_special_tokens=False, max_length=768, truncation=True, padding=\"max_length\", return_tensors=\"pt\", padding_side=\"right\"\n",
    "        )[\"input_ids\"][0]\n",
    "image_tokens = \"<im_patch>\" * proj_out_num\n",
    "input_txt = image_tokens + question\n",
    "input_id = tokenizer(input_txt, return_tensors=\"pt\")['input_ids'].to(device=device)\n",
    "\n",
    "\n",
    "# generation = model.generate(image_pt, input_id, max_new_tokens=256, do_sample=True, top_p=0.9, temperature=1.0)\n",
    "with torch.cuda.amp.autocast(): \n",
    "    generation = model.generate(image[0].to(device=device), input_id, question_ids=question_ids.to(device=device), max_new_tokens=768, do_sample=True, top_p=0.9, temperature=1.0)\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(generation, skip_special_tokens=True)\n",
    "\n",
    "print('question', question)\n",
    "print('generated_texts', generated_texts[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "green_score",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
